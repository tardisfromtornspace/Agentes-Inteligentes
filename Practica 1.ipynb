{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f844acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analizando journal: GET https://www.sciencedirect.com/journal/swarm-and-evolutionary-computation/issues...\n",
      "No tenemos proxy, ten cuidado\n",
      "...GET del journal completado\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en-US\">\n",
      "<head>\n",
      "    <title>Just a moment...</title>\n",
      "    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n",
      "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\">\n",
      "    <meta name=\"robots\" content=\"noindex,nofollow\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width,initial-scale=1\">\n",
      "    <link href=\"/cdn-cgi/styles/challenges.css\" rel=\"stylesheet\">\n",
      "    <meta http-equiv=\"refresh\" content=\"35\">\n",
      "\n",
      "</head>\n",
      "<body class=\"no-js\">\n",
      "    <div class=\"main-wrapper\" role=\"main\">\n",
      "    <div class=\"main-content\">\n",
      "        <noscript>\n",
      "            <div id=\"challenge-error-title\">\n",
      "                <div class=\"h2\">\n",
      "                    <span class=\"icon-wrapper\">\n",
      "                        <div class=\"heading-icon warning-icon\"></div>\n",
      "                    </span>\n",
      "                    <span id=\"challenge-error-text\">\n",
      "                        Enable JavaScript and cookies to continue\n",
      "                    </span>\n",
      "                </div>\n",
      "            </div>\n",
      "        </noscript>\n",
      "        <div id=\"trk_jschal_js\" style=\"display:none;background-image:url('/cdn-cgi/images/trace/jsch/nojs/transparent.gif?ray=7b61f0075b25666f')\"></div>\n",
      "        <form id=\"challenge-form\" action=\"/journal/swarm-and-evolutionary-computation/issues?__cf_chl_f_tk=VqCxIsZQSU3mnOpiAfxnn40Q5V0l9txLCRAzXCb023A-1681202561-0-gaNycGzNCpA\" method=\"POST\" enctype=\"application/x-www-form-urlencoded\">\n",
      "            <input type=\"hidden\" name=\"md\" value=\"9CTJuWf8wksvZANIqjDLVV6UMQrBS_Fc.hR7NOBUvvw-1681202561-0-AZTi7W3P-gxqrA21WykIR1ja5SHC1K6p0rQRsVkOrvmhh51n-XjaBOgyQALctpMTiGc5c-nBXTcnROnYJ99pvyetejFN4JH7H8PEgc2gX5b2Hl8NuSmH4VqCxyVZwMUgoTAxcbOe3eC_nhrI3e-qUw_gob0tMyIILCWx2t-C1uPVmngFpqV5IE0UT2jvzDNZghsQGe_rlsX0BKqurVjlg6YwovbJv_Oi42C_2HY5c1FbaZFZhDEO29BbQ92LMfHZDKJbZbcI9UAEPe6zx4BKw1Wbzy5prZReTTNUfZ0X4yMBY9972GO8rV2JnVub_NqUwbBCsvtocP_fd0TGd5yWGLVAfWbAXoicIdnskOKID9iHS4ACuFD_IyK69EvbNj78uUrLmvQHcPQlauI4iJ0DdO9-jZN13e57a-8mzqkBxdyFHxHYEheCC-mGRRuFtmprr9RLBVLzdO0_BDLLuZASpvO-50cVeQbXxI0xQOs2xkb4nyVVM0-bqVKyLvAOYK-GHR29IjJWEii70h5alnxPhBSio6DETF3ay5VF7dFCCowDsy9kK42gcf-MtZ0-avuSHvvhSnK-eDicJzTxqeoIgh4uPdvugIZck4Peofw81zw-LIctEWdtc_-TPsTzWl1ARIPDsJ1LOcILUxc3-QG1pik_EWWt_Pqe6rxhjYw1BVqsFDJw6BUDj_G9xIqb6tblEE4mZG_t8CuMYYpeuJ31W-6gIKoT-UWf7rKnheK1ir6xGpfIlpK_1EhThQeJ_kYOsdZEJiOCjiHguL20sjQM2OpvltA6uQTavZOmPka3Z4Vnl-6LDOcOV2ysr0T8OcDgEgTBw38cdR2QPzJjKlFObL8WSpfFQVDHOmuvTo0XA6IcbxgkXGK7tKOqICz4ug_NY2exFtFXBbh1-a_SFvNoiL4Rr59J7GZwuCQL0qEPHITbV2w7X7pn-UrxEu42tFsxAZXa3WGfxBJP22EVX1f9piPj2pOk_AaSydnGU5-hFjHqiSE6CcSuhuJyVHWamX0zzngY5XMuqA8ysYAf5VfZPc9H_7gVEX7bGETdERqpWUwjriz-A7Lm96Ug_XgQxLSva0GMasHd5xm9vRKv3DbNlz0-5uQt7gH5qK2SsxDEYPGD9ZvYoBSZ9KOuSiv28V0ymEVVNsUD5wXfSr_8mwH04s2TeesXlRwjTNqLvNY8_uZNgQfu1f18D4yFe_jSP-xzFl8rz9WpaYW3RxdS7hO9_ypuk1L0VX7UVaWCnAkNa2l5Fe6R9T2QYXLwisum1XXpTDuj1rEoAkU1ahGFGUVeEV6hPyRriduCbrVjn32FDK-YJkiOz2OBJA4dgfSOoiJqApgWmSCXKWw2SHoCV9SYTtNWzNTjWURjfWTJJXsuIQ8Gw9_UHOiNObMRYBX69RwRkSJuLH5gRi-Xr6mKpWxpmPvkCDrtt4fAdtzxACxWSnR7wi1KREvn0_3xLHWVSCZ5_rNtrIs89tQoL72zn8Hp1IvAm-LfYGDmYo57zEjJhK7TDVD-dxi7hIzDrAGXLoWUA3Yzz1avyLQ2fQEHKLDLHTcqxunXp2WTa7TQN9XKir-4xHMCsX5lYLGdg4KayS_ARwKicp6_amBQFCV1JOhH6daSGtfExoirJf0Kk9Sw9CbVlTaoPRdRPZ2WpJGwDMAGIUwb646MItlbHGqMb69BVxk9Cr0GqJ930GaxlIJDrWgnBvGaZAnnxs8EBEVGQ8vzJk3xpkPQO2giVV5TkfXNJqDASE4_Z4M38NFLw_co4KmIoUXZElU_1ogs9Z6ErMdL8ZKX9s7Oz5TcM0J33TWX0XeMRpyKUs71_zSAMzLbKCn46vBIT86fHpenCI1cm_XcIx_hzyfXAmD03PCEj6HRHTlfel7KLeqCrI9EXdkMLK0oUq0GfkGCYTDbpRKN5T7hWxCB_vBtLrt6S0VPTK_B2I9Y_gXtQxNhTcE5KqXo4xToe4UCcmxlLHMO9OqOR6nVfrc4FF7773igj3_RCLOAUCSRzpI1EnPwoKQNC_J4HU7lTTCB4cHoEDKKiKQik4K7Omf4Hbyw4EqwMwR-ssU6sonGVYiGkn1rVNytPaNolm_-xBMj9-FHZqfeWCxwqIeuedYFgVCcLpDcO1i2ZzrP9xh5cJjN5PDsm581RRJ_WINT4M0TT0VRt5NZCQQFLPZ60W4UB6Wr45kfK6sQS3PsNVTm9S9sOiwthM1E3hKSyodrfMt3OTQ6lhjEeO6Umc0ItJkLrK1F6uHB-gjhLT3S8mPpis3vppLHARMZPpCkthzIrQWw0z5PaO0ooA7w7tKrSxsl1CdyrrKbQ258FHGn7yZtuxP8pbGgpOAEJCvItNcgSzNLfTFbdPeLokwa4TgnpXLUFQ3wwZDxqC1-bbogne8qd7-Z3q8g84GPSWCdfAAS5hBZjqwEf7eaV6W8HLzjzg0e5cR877k7b4Wh02dElDlcFlKoebiU61bZTasMYV5-d5FOHTlgZZgNsPhV0HtOxduPZcAm7TXPDKak3WQlVf_BOjZ-3iMmeUSVL4JhkT5kGGQFR7aZVvy6O8ojEZndaCyFh5oqDPKudP9DKJY0ecqUy72EshE-ExCXGr0SiZL4mEihP9U7pg2o3SoCLYK267imboZCgbzh6yWgWmtmgyk\">\n",
      "        </form>\n",
      "    </div>\n",
      "</div>\n",
      "<script>\n",
      "    (function(){\n",
      "        window._cf_chl_opt={\n",
      "            cvId: '2',\n",
      "            cZone: 'www.sciencedirect.com',\n",
      "            cType: 'non-interactive',\n",
      "            cNounce: '26854',\n",
      "            cRay: '7b61f0075b25666f',\n",
      "            cHash: '05f3df8b8bbf3da',\n",
      "            cUPMDTk: \"\\/journal\\/swarm-and-evolutionary-computation\\/issues?__cf_chl_tk=VqCxIsZQSU3mnOpiAfxnn40Q5V0l9txLCRAzXCb023A-1681202561-0-gaNycGzNCpA\",\n",
      "            cFPWv: 'b',\n",
      "            cTTimeMs: '1000',\n",
      "            cMTimeMs: '60000',\n",
      "            cTplV: 5,\n",
      "            cTplB: 'cf',\n",
      "            cK: \"\",\n",
      "            cRq: {\n",
      "                ru: 'aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vam91cm5hbC9zd2FybS1hbmQtZXZvbHV0aW9uYXJ5LWNvbXB1dGF0aW9uL2lzc3Vlcw==',\n",
      "                ra: 'TW96aWxsYS81LjAgKFgxMTsgVWJ1bnR1OyBMaW51eCB4ODZfNjQ7IHJ2OjEwNy4wKSBHZWNrby8yMDEwMDEwMSBGaXJlZm94LzEwNy4w',\n",
      "                rm: 'R0VU',\n",
      "                d: 'rbKi49Zmq4wlwvBd0+LH344v7++f2GLjFOUENtmXd9U1ij6Ug9PjQb8kwkvEU7aj5TW6z5f4JrpJbwphHuz9yeqbI8VdFxnCip0Tm4rLh7uNqiHjUzZQGe+8JBDNEnohH8PxOABt305KyEqxatN8STaa8GBFypJ+tXOmo7a/ZJEA98sVmDTojGNH9dFfhtZdmzDcRxqWMCM54rl/tiLiDhVsrkFPBLOD4R9jUcFF9zNwcmFb0TEuwPs+GD0LK5y2WvD0oEm45Im12rPAF75kuFEB3/Z+JkCNtgZ23x92GkQN8onJ2GHemUHZgDEyYB2HAGKmGp3EhsVUd3l5vGPEMGMgTFtpVAkPWpTjelLTbf2A2g5G19YvImCXFp+H9I675lGpIzH4cgMzq611dY6Oh5ZRBEEr2/Ip0gwH7ovxfxb5azfBb5awiyw8QjfwYbNjOfUaw5UaN1Cg8cWtP7Tao+8j09+QoBIUukBlwks6oIPm3jPHPIv5iFcPjtfbS3odxmm1Qexo8quDQrCTqPu2ReOSZPsvvpvOMvOfhQHWSKDLMoP73ojXIU1il4sNed2A8KPhYve8Ip2xRI0f5m4EVA==',\n",
      "                t: 'MTY4MTIwMjU2MS4xODEwMDA=',\n",
      "                m: 'FYhhuCk/nLJFXbbHI0477LKFNhAq6C9IiC/z0TLes2U=',\n",
      "                i1: 'b6fGEdaLpmbs8+L2/ZkjXA==',\n",
      "                i2: 'tVh1MuZFF27mMf/M4fLoMg==',\n",
      "                zh: 'eknJUbsDo2Vxc6GgtD/le0vXqlP53xNp4XjvomKKsGA=',\n",
      "                uh: '7pyfTn6JY3vUj8MX4arZFXp/RxvzByecm13O5Bd0rCg=',\n",
      "                hh: '+7dgh3irsM1xf/It1/rLKw9KLOrDdT/34NcXJ5NU1S0=',\n",
      "            }\n",
      "        };\n",
      "        var trkjs = document.createElement('img');\n",
      "        trkjs.setAttribute('src', '/cdn-cgi/images/trace/jsch/js/transparent.gif?ray=7b61f0075b25666f');\n",
      "        trkjs.setAttribute('alt', '');\n",
      "        trkjs.setAttribute('style', 'display: none');\n",
      "        document.body.appendChild(trkjs);\n",
      "        var cpo = document.createElement('script');\n",
      "        cpo.src = '/cdn-cgi/challenge-platform/h/b/orchestrate/jsch/v1?ray=7b61f0075b25666f';\n",
      "        window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;\n",
      "        window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;\n",
      "        if (window.history && window.history.replaceState) {\n",
      "            var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;\n",
      "            history.replaceState(null, null, \"\\/journal\\/swarm-and-evolutionary-computation\\/issues?__cf_chl_rt_tk=VqCxIsZQSU3mnOpiAfxnn40Q5V0l9txLCRAzXCb023A-1681202561-0-gaNycGzNCpA\" + window._cf_chl_opt.cOgUHash);\n",
      "            cpo.onload = function() {\n",
      "                history.replaceState(null, null, ogU);\n",
      "            };\n",
      "        }\n",
      "        document.getElementsByTagName('head')[0].appendChild(cpo);\n",
      "    }());\n",
      "</script>\n",
      "\n",
      "\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "Extracción de datos del journal: INICIANDO\n",
      "Número de volúmenes en el journal: 0\n",
      "Analizando volúmenes del journal:\n",
      "Número total de artículos de investigación en el journal:0\n",
      "Procediendo a cribar los artículos a número y fecha pedidos\n",
      "Extracción de datos del journal: COMPLETADO\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Agente extractor de datos de artículos\n",
    "\n",
    "Este agente se encarga de la extracción de datos de la revista\n",
    "https://www.sciencedirect.com/journal/swarm-and-evolutionary-computation\n",
    "\n",
    "Para funcionar, el script requiere que estén instaladas las\n",
    "bibliotecas `bs4`, `date` `numpy`, re`, `requests` y `time`, utilizadas para formatear un poco el html, ver las fechas, hacer una función de valor aleatorio, expresiones regulares, captar datos y poder esperar un período de tiempo según lo obtenido por numpy para evitar que nos bloqueen, respectivamente.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Ok primero de todo importamos las bibliotecas que nos interesan\n",
    "from bs4 import BeautifulSoup\n",
    "#from lxml import html\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "#import socks\n",
    "import time\n",
    "from datetime import datetime\n",
    "#from urllib.requests import *\n",
    "\n",
    "def getHumano (url,\n",
    "               proxy=None,\n",
    "               accept='application/json;q=0.9,*/*;q=0.8', \n",
    "               acceptEncoding='gzip, deflate', \n",
    "               acceptLanguage='en-US, en;q=0.9, es;q=0.8', \n",
    "               userAgent='Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:107.0) Gecko/20100101 Firefox/107.0', \n",
    "               referer='https://duckduckgo.com/'):\n",
    "    \"\"\"Función auxiliar empleada para realizar un GET creíble que parezca humano a una url. Se ha dejado con parámetros de Ubuntu por defecto\"\"\"\n",
    "\n",
    "    \n",
    "    proxies = {\n",
    "        'http': 'socks5://127.0.0.1:9050',\n",
    "        'https': 'socks5://127.0.0.1:9050'\n",
    "    }\n",
    "    # Por algún extraño motivo usar esto provoca un error \"Missing dependencies for SOCKS support.\" a pesar de tener instalado pysocks.\n",
    "    response = \"\"\n",
    "    if proxy==None:\n",
    "        response = requests.get(url, headers = {\n",
    "            'Accept': accept,\n",
    "            'Accept-Encoding': acceptEncoding,\n",
    "            'Accept-Language': acceptLanguage,\n",
    "            'User-Agent': userAgent,\n",
    "            'Referer' : referer\n",
    "            })\n",
    "        proxyWork=proxy\n",
    "    else:\n",
    "        try:\n",
    "            response = requests.get(url, headers = {\n",
    "                'Accept': accept,\n",
    "                'Accept-Encoding': acceptEncoding,\n",
    "                'Accept-Language': acceptLanguage,\n",
    "                'User-Agent': userAgent,\n",
    "                'Referer' : referer\n",
    "                },\n",
    "                proxies=proxies\n",
    "            )\n",
    "            print(\"Utilizando proxy Tor\")\n",
    "            proxyWork=proxy\n",
    "        except:\n",
    "            print(\"No tenemos proxy, ten cuidado\")\n",
    "            response = requests.get(url, headers = {\n",
    "                'Accept': accept,\n",
    "                'Accept-Encoding': acceptEncoding,\n",
    "                'Accept-Language': acceptLanguage,\n",
    "                'User-Agent': userAgent,\n",
    "                'Referer' : referer\n",
    "                }#, proxies=proxies\n",
    "            )\n",
    "            proxyWork=None\n",
    "    return response, proxyWork\n",
    "    \n",
    "\n",
    "def extract(n, since=None, urlarticuloBaseEntrada='https://www.sciencedirect.com/', elJournalABuscarEntrada = 'journal/swarm-and-evolutionary-computation', codif='utf-8'):\n",
    "    \"\"\"Extrae la información de los últimos n artículos hasta since\n",
    "  \n",
    "    :param n: El número de artículos de los que extraer datos. Debe\n",
    "        ser un entero mayor que 0.\n",
    "    :param since: La fecha desde cuándo sacar la información. Debe\n",
    "        ser un objeto date. si no se especifica, se presupone la\n",
    "        fecha del día en el que se ejecuta la función\n",
    "    :return: Una lista de tuplas donde cada tupla tendrá la\n",
    "        siguiente forma: (str, str, str, str, List[str], List[str])\n",
    "\n",
    "    *Nombre de la publicación: El nombre de la publicación (no tiene por qué extraerse, ya que para cada grupo sólo se usa una única publicación.\n",
    "    *Título del artículo: Cadena de texto sin caracteres extraños y sin espacios alrededor.\n",
    "    *Fecha de publicación: Cadena de texto en en formato YYYYMMDD. Por ejemplo, el 10 de febrero de 2023 sería 20230210.\n",
    "    *Abstract del artículo: Cadena de texto sin caracteres extraños y sin espacios alrededor.\n",
    "    *Palabras clave: En el caso de que exista, lista de cadenas de texto sin caracteres extraños y sin espacios alrededor. Si no existe, una lista vacía.\n",
    "    \n",
    "    # PASO 0 Se listan los patrones a buscar. Se recomiendan compilar para reducir su peso, sobre todo si se usan mucho.\n",
    "    \n",
    "    # PASO 1: búsqueda de articulos en cada volumen.\n",
    "    \n",
    "        # PASO 1.1 el título de la publicación y todos los volúmenes del journal se extraen de la página principal de issues.\n",
    "        # PASO 1.2 para cada volumen se extraen todos los artículos científicos y se ponen en una variable auxiliar.\n",
    "    \n",
    "    # PASO 2: Búsqueda de info adicional para cada artículo candidato.\n",
    "        # 2.1 Primero visitamos la página específica de cada artículo para sacar su fecha de publicación.\n",
    "        # 2.2 si la fecha dada es mayor que la obtenida, no incluimos el artículo. Acá hemos supuesto que lo que nos piden es que busquemos artículos más viejos que los de la fecha actual.\n",
    "        # 2.3 obtenemos el abstracto (ignormaos las highlights) y las palabras clave.\n",
    "        # 2.4 se crea el objeto unArtículo con los valores extraídos anteriormente.\n",
    "        # 2.5 se hace append a la respuesta.\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    fecha = \"\"\n",
    "    if n <= 0:\n",
    "        print (\"Se ha introducido un número de artículos inválido. Se tomará solo un artículo\")\n",
    "        n = 1\n",
    "    if not since:\n",
    "        \"\"\"\n",
    "        # si no se especifica, se presupone la fecha del día en el que se ejecuta la función.\n",
    "        # NOTA: como el universo es en situaciones normales lineal y esto no es una TARDIS, podemos quitarnos lo de buscar la fecha actual y no utilizar from datetime import date, fechaDesde = date.today()\n",
    "        \"\"\"\n",
    "        fecha = \"00000000\" # esto está acá por si hubiese errores que lo reconozcamos como código de error\n",
    "    else:\n",
    "        # Si la fecha existe pues debería ser Date, la pasamos a nuestro querido formato YYYYMMDD para ser procesada más tarde\n",
    "        fecha = str(since.year).rjust(4, '0')+str(since.month).rjust(2, '0')+str(since.day).rjust(2, '0')\n",
    "        print(\"Fecha encontrada, desde: \"+fecha)\n",
    "            \n",
    "    \n",
    "    \n",
    "    # PETICIONES HTTPS\n",
    "    \"\"\"Utilizamos requests para obtener la información de la página, usando el mayor número de cabeceras para parecer humanos\"\"\"\n",
    "    urlarticuloBase = urlarticuloBaseEntrada                # Nos interesa mucho tener esta separada para el paso 2.1\n",
    "    elJournalABuscar = elJournalABuscarEntrada              # Así queda modular y bonito\n",
    "    issues = \"/issues\"\n",
    "    print(\"Analizando journal: GET \"+urlarticuloBase+elJournalABuscar+issues+\"...\")\n",
    "    response, proxyFunciona = getHumano(urlarticuloBase+elJournalABuscar+issues, \"Tor\")\n",
    "    contenido = response.content\n",
    "    response.encoding = codif                               # Puede sernos útil para cambiarlo más tarde\n",
    "    texto = response.text                                   # TEXTO\n",
    "    cabeceras = response.headers                            # CABECERAS\n",
    "    print(\"...GET del journal completado\")\n",
    "    #print(cabeceras)\n",
    "    #print(codif)\n",
    "    print(texto)\n",
    "    #tree = html.fromstring(contenido)\n",
    "    #print(tree.xpath('//*/text()'))\n",
    "    \n",
    "    # EXTRACCIÓN\n",
    "    print(\"Extracción de datos del journal: INICIANDO\")\n",
    "    \n",
    "    patronVolumenes = re.compile('<a class=\"anchor js-issue-item-link text-m anchor-default\" href=\"([a-zA-Z0-9_ \\/-]*)\"')\n",
    "    patronTituloPublicacion = re.compile('<a class=\"anchor js-title-link [a-zA-Z0-9_ \\/-]*\" href=\"[a-zA-Z0-9_ \\/-]*\" usageZone=\"jrnl_banner\" id=\"journal-title\"><span class=\"anchor-text\">(.*?)<\\/span>')\n",
    "    patronTituloArticulo = re.compile('<dl class=\"js-article article-content\"><dd class=\"article-info text-xs\">.*?<span class=\"js-article-subtype\">Research article<\\/span>.*?<\\/dd><dt><h3 class=\"text-m u-font-serif u-display-inline\"><a(?: id=\"[a-zA-Z0-9_ \\/-]*\"|) class=\"anchor article-content-title[a-zA-Z0-9_ \\/-]*\" href=\"([a-zA-Z0-9_ \\/)(-]*)\" (?:data-aa-name=\"Article title\" |)id=\"([a-zA-Z0-9_ \\/-]*)\" usageZone=\"rslt_list_item\"><span class=\"anchor-text\"><span(?: class=\"js-article-title\"|)>(.*?)<\\/span')\n",
    "    \n",
    "    patronAutores = re.compile('<div class=\"Banner\" id=\"banner\"><div class=\"wrapper truncated\"><div class=\"AuthorGroups text-s\"><div class=\"author-group\" id=\"author-group\"><span class=\"sr-only\">Author links open overlay panel<\\/span>((?:.*?<span class=\"given\\-name\">.*?<\\/span> <span class=\"text surname\">.*?<\\/span>.*?<\\/span>)+)')\n",
    "    patronAutorBueno = re.compile('<span class=\"given\\-name\">(.*?)<\\/span> <span class=\"text surname\">(.*?)<\\/span>.*?<\\/span>')\n",
    "    patronFecha = re.compile('<meta name=\"citation_publication_date\" content=\"([0-9]+)\\/([0-9]+)\\/([0-9]+)\" \\/>')\n",
    "    patronAbstract = re.compile('<div class=\"Abstracts[a-zA-Z0-9_ \\/-]*\" id=\"abstracts\">(.*?<div class=\"abstract author\" id=\"[a-zA-Z0-9_ \\/-]*\"><h2 class=\"section-title[a-zA-Z0-9_ \\/-]*\">Abstract<\\/h2><div id=\"[a-zA-Z0-9_ \\/-]*\"><p id=\"[a-zA-Z0-9_ \\/-]*\">.*?<\\/p>)') # Hemos decidido Tener en cuenta los abstract highlights como parte del abstract y por lo tanto permitir su formato y el caracter utilizado para puntos de una lista de highlights.\n",
    "    patronKeywords = re.compile('(<div class=\"Keywords[a-zA-Z0-9_ \\/-]*\"><div id=\"[a-zA-Z0-9_ \\/-]*\" class=\"keywords-section[a-zA-Z0-9_ \\/-]*\">(?:<h2 class=\"section-title[a-zA-Z0-9_ \\/-]*\">Keywords<\\/h2>)*?.*?<div id=\"[a-zA-Z0-9_ \\/)(-]*\" class=\"keyword\"><span(?: id=\"[a-zA-Z0-9_ \\/-]*\"|)>[a-zA-Z0-9_ \\/)(-]*<\\/span><\\/div><\\/div><\\/div>)')\n",
    "    #patronKeywords = re.compile('(<div class=\"Keywords[a-zA-Z0-9_ \\/-]*\"><div id=\"[a-zA-Z0-9_ \\/-]*\" class=\"keywords-section[a-zA-Z0-9_ \\/-]*\">(?:<h2 class=\"section-title[a-zA-Z0-9_ \\/-]*\">Keywords<\\/h2>)*?.*?(<div id=\"[a-zA-Z0-9_ \\/-]*\" class=\"keyword\"><span(?: id=\"[a-zA-Z0-9_ \\/-]*\"|)>[a-zA-Z0-9_ \\/-]*<\\/span>)*?<\\/div><\\/div><\\/div>)')\n",
    "    patronKeywordsBueno = re.compile('<div id=\"[a-zA-Z0-9_ \\/-]*\" class=\"keyword\"><span(?: id=\"[a-zA-Z0-9_ \\/-]*\"|)>([a-zA-Z0-9_ \\/-]*)<\\/span>')\n",
    "    \n",
    "    # Inicialmente probamos con findall, es mejor usar search pero al menos nos avisa de si algo extraño ocurre\n",
    "    Nombre_de_la_publicación = re.findall(patronTituloPublicacion, texto)\n",
    "    volúmenes = re.findall(patronVolumenes, texto)\n",
    "    if len(Nombre_de_la_publicación) > 1:\n",
    "        print (\"Algo raro ocurre, es posible que nos hayan engañado o hayan actualizado el formato\")\n",
    "    \n",
    "    infoSuperBasic = []\n",
    "    totalArts = 0\n",
    "    print(\"Número de volúmenes en el journal: \"+str(len(volúmenes)))\n",
    "    print(\"Analizando volúmenes del journal:\")\n",
    "    for i in volúmenes:\n",
    "        print(i)\n",
    "        urlTotal = urlarticuloBase + i # url base + url parcial = url total\n",
    "        responseA, proxyFunciona = getHumano(urlTotal, proxyFunciona)\n",
    "        responseA.encoding = codif                               # Puede sernos útil para cambiarlo más tarde\n",
    "        textoA = responseA.text\n",
    "        # Esto me devuelve tres grupos para cada artículo encontrado: la url parcial del artículo, un identificativo especial del html y su título. Esto resulta ventajoso para el futuro\n",
    "        infoSuperBasic1 = re.findall(patronTituloArticulo, textoA)\n",
    "        # Pongo todos los artículos en la lista general\n",
    "        for articulo in infoSuperBasic1:\n",
    "            infoSuperBasic.append(articulo)\n",
    "        totalArts = totalArts + len(infoSuperBasic1)\n",
    "        time.sleep(np.random.uniform(low=0.25, high=0.8))\n",
    "        \n",
    "    print(\"Número total de artículos de investigación en el journal:\"+str(totalArts))\n",
    "    #print(infoSuperBasic)\n",
    "    print(\"Procediendo a cribar los artículos a número y fecha pedidos\")\n",
    "    i = 0 # cont num articulos válidos\n",
    "    artCon = 0 # contador num de articulos visitados\n",
    "    \n",
    "    while artCon < totalArts and i < n:\n",
    "        # Hacemos petición GET de esta nueva url\n",
    "        urlTotal = urlarticuloBase + infoSuperBasic[artCon][0] # url base + url parcial = url total\n",
    "        responseA, proxyFunciona = getHumano(urlTotal, proxyFunciona)\n",
    "        responseA.encoding = codif                               # Puede sernos útil para cambiarlo más tarde\n",
    "        textoA = responseA.text  \n",
    "        \n",
    "        # Dormir aleatoriamente para prevenir que nos identifiquen. Acá para que todos los árboles de decisión duerman\n",
    "        time.sleep(np.random.uniform(low=0.25, high=0.8))\n",
    "        \n",
    "        # Antes de todo cribar solo los research articles, en sí ya se cribaron antes en la expresión regular del título, pero por si acaso. Sabemos que los que no son de investigación tienen un título preciso \n",
    "        if infoSuperBasic[artCon][2] == \"Editorial Board\":\n",
    "            print(\"articulo no de investigación, descartando...\")\n",
    "            artCon = artCon + 1\n",
    "            continue\n",
    "        \n",
    "        # Obtenemos autores. EL primero es para englobar todos y el segundo para cribarlos del primero bien\n",
    "        infoTempAutores = re.findall(patronAutores, textoA)\n",
    "        if len(infoTempAutores) > 0:\n",
    "            infoTempAutoresBuena = re.findall(patronAutorBueno, infoTempAutores[0])\n",
    "        else:    \n",
    "            infoTempAutoresBuena = \"ERROR!\"\n",
    "            print(\"error\")\n",
    "        \n",
    "        # Obtenemos la fecha y la ponemos en el formato YYYYMMDD\n",
    "        infoFecha = re.findall(patronFecha, textoA)\n",
    "        # Como son ingleses usan Año/Día/Mes, así que lo corregimos\n",
    "        fechaFormateada = infoFecha[0][0].rjust(4, '0')+infoFecha[0][2].rjust(2, '0')+infoFecha[0][1].rjust(2, '0')\n",
    "        # Si tenemos una fecha base, comparamos con ella para decidir que hacer\n",
    "        if not since:\n",
    "            print(\"Sin verificación de fecha\")\n",
    "        elif int(fechaFormateada) > int(fecha):\n",
    "            print(\"Articulo \"+str(artCon)+\" de título \"+infoSuperBasic[artCon][2]+\" y fecha \"+fechaFormateada+\" más nuevo de lo pedido, descartando...\")\n",
    "            artCon = artCon + 1\n",
    "            continue\n",
    "        else:\n",
    "            print(\"Artículo de fecha adecuada encontrado\")\n",
    "        \n",
    "        # Obtenemos el abstracto, que en la página de donde lo extraemos es un párrafo a veces precedido de highlights\n",
    "        Abstract_del_artículo = re.findall(patronAbstract, textoA)\n",
    "        \n",
    "        # Obtenemos la sección de keywords para luego afinar en dicha subsección\n",
    "        Palabras_clave_temp = re.findall(patronKeywords, textoA)\n",
    "        if len(Palabras_clave_temp) > 0:\n",
    "            Palabras_clave = re.findall(patronKeywordsBueno, Palabras_clave_temp[0])\n",
    "        else:\n",
    "            Palabras_clave = []\n",
    "        \n",
    "        unArticulo = [\"No encontrado\", \n",
    "                      \"Sin título?\", \n",
    "                      \"Aparentemente fuera del vórtice del Tiempo\", \n",
    "                      \"N/A\", \n",
    "                      \"\", \n",
    "                      [], \n",
    "                      urlTotal]\n",
    "        \n",
    "        if Nombre_de_la_publicación[0]:\n",
    "            unArticulo[0] = Nombre_de_la_publicación[0]\n",
    "        if infoSuperBasic[artCon][2]:\n",
    "            unArticulo[1] = infoSuperBasic[artCon][2]           \n",
    "        if fechaFormateada:\n",
    "            unArticulo[2] = fechaFormateada    \n",
    "        if len(Abstract_del_artículo) >= 1 and Abstract_del_artículo[0]:\n",
    "            # Buscamos algo para limpiarnos el abstracto un poco en caso de caracteres de formato extraño al pasar de html a string\n",
    "            soup = BeautifulSoup(Abstract_del_artículo[0])\n",
    "            unArticulo[3] = soup.get_text()\n",
    "        if Palabras_clave:\n",
    "            unArticulo[4] = Palabras_clave\n",
    "        if infoTempAutoresBuena:\n",
    "            unArticulo[5] = infoTempAutoresBuena\n",
    "        \n",
    "        result.append(unArticulo)\n",
    "        \n",
    "        i = i + 1\n",
    "        artCon = artCon + 1\n",
    "        print(\"next\")\n",
    "        \n",
    "    print(\"Extracción de datos del journal: COMPLETADO\")\n",
    "    return result\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    resultado = extract(n=2000)\n",
    "    f = open(\"palabrasEntradaP1Algigantix.txt\", \"w\")\n",
    "    f.write(str(resultado))\n",
    "    f.close()\n",
    "    #for row in extract(n=20000):\n",
    "    #    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582e315e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
