{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4480ff39",
   "metadata": {},
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Clasificación de textos con CNN y RNN<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Autor: Alberto Díaz Álvarez<br>Última actualización: 2023-03-14</small></i></div>\n",
    "                                                  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2489222",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17030ba",
   "metadata": {},
   "source": [
    "En NLP una tarea muy típica es la de clasificación tde textos. En ella, se clasifica un texto determinado en función de su significado. Suele usarse, por ejemplo, para el problema del análisis de sentimiento.\n",
    "\n",
    "Se trata de un problema de los que se denominan _many-to-one_, es decir, aquel donde el tamaño de secuencia de entrada es $T_X \\ge 1$, pero el tamaño de secuencia de salida es $T_Y = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d1e17",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b9e324",
   "metadata": {},
   "source": [
    "Vamos a hacer un ejercicio de clasificación de comentarios tóxicos a partir del dataset de Kaggle presentado en el ejercicio anterior. Para ello usaremos, en un principio, redes de convolución. Posteriormente veremos cómo el cambio a redes recurrentes es inmediato y no requiere apenas cambio en los modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d401eac",
   "metadata": {},
   "source": [
    "## Imports y configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa528aec",
   "metadata": {},
   "source": [
    "A continuación importaremos las librerías que se usarán a lo largo del notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a8c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import requests\n",
    "from shutil import unpack_archive\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3a5f84",
   "metadata": {},
   "source": [
    "Asímismo, configuramos algunos parámetros para adecuar la presentación gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d522d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (20, 6),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4fec89",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7223928",
   "metadata": {},
   "source": [
    "## Carga de _embedding_ preentrenado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9977840",
   "metadata": {},
   "source": [
    "Descargamos el dataset de [GloVe](https://nlp.stanford.edu/projects/glove/), concretamente el que se encuentra en http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67b5594b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ...OK\n",
      "Unpacking ...OK\n"
     ]
    }
   ],
   "source": [
    "GLOVE_URL = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "GLOVE_FILE = 'tmp/glove.6B.zip'\n",
    "GLOVE_DIR = 'tmp/'\n",
    "\n",
    "if not os.path.isdir(GLOVE_DIR):\n",
    "    os.makedirs(GLOVE_DIR)\n",
    "\n",
    "# Descargamos el dataset comprimido de GloVe (si no lo tenemos ya)\n",
    "if not os.path.exists(GLOVE_FILE):\n",
    "    print('Downloading ...', end='')\n",
    "    with open(GLOVE_FILE, 'wb') as f:\n",
    "        r = requests.get(GLOVE_URL, allow_redirects=True)\n",
    "        f.write(r.content)\n",
    "    print('OK')\n",
    "\n",
    "# Lo descomprimimos en el directorio 'glove'\n",
    "print('Unpacking ...', end='')\n",
    "unpack_archive(GLOVE_FILE, GLOVE_DIR)\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fd945b",
   "metadata": {},
   "source": [
    "Configuramos los parámetros para el sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51a1ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuántas dimensiones tienen nuestros word vectors (50, 100, 200 o 300)\n",
    "EMBEDDING_DIM = 50\n",
    "# El tamaño máximo de nuestro vocabulario (se escogerán las más frecuentes)\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "# El tamaño de la frase más larga con la que alimentar el modelo\n",
    "MAX_SEQUENCE_LENGTH = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdcdc82",
   "metadata": {},
   "source": [
    "Cargamos el _embedding_ de dimensión especificada en la configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98b853ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe 50-d embedding... "
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 2273: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m word2vec \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(GLOVE_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglove.6B.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEMBEDDING_DIM\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124md.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m         values \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m      6\u001b[0m         word2vec[values[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(values[\u001b[38;5;241m1\u001b[39m:], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\program files\\python39\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 2273: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "print(f'Loading GloVe {EMBEDDING_DIM}-d embedding... ', end='')\n",
    "word2vec = {}\n",
    "with open(os.path.join(GLOVE_DIR, f'glove.6B.{EMBEDDING_DIM}d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word2vec[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "print(f'done ({len(word2vec)} word vectors loaded)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933c217",
   "metadata": {},
   "source": [
    "Y ahora cargamos los datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "232f8283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading toxic comments training dataset... done (159571 comments loaded)\n",
      "Biggest comment length:  5000\n",
      "Smallest comment length: 6\n",
      "Avg. comment length:     394.0732213246768\n",
      "Median comment length:   205\n",
      "--------------------\n",
      "Example comment: alignment on this subject and which are contrary to those of DuLithgow\n",
      "Example targets: 0\n"
     ]
    }
   ],
   "source": [
    "print('Loading toxic comments training dataset... ', end='')\n",
    "df = pd.read_csv('Datasets/Toxic/train.csv')\n",
    "sentences = df['comment_text'].fillna('DUMMY_VALUE').values\n",
    "targets = df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values[:,0]\n",
    "print(f'done ({len(sentences)} comments loaded)')\n",
    "print(f'Biggest comment length:  {max(len(s) for s in sentences)}')\n",
    "print(f'Smallest comment length: {min(len(s) for s in sentences)}')\n",
    "print(f'Avg. comment length:     {np.mean([len(s) for s in sentences])}')\n",
    "print(f'Median comment length:   {sorted(len(s) for s in sentences)[len(sentences) // 2]}')\n",
    "print('-' * 20)\n",
    "print(f'Example comment: {sentences[9]}')\n",
    "print(f'Example targets: {targets[9]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0413fefc",
   "metadata": {},
   "source": [
    "Convertimos las frases en enteros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c3221f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biggest index: 9999\n",
      "Unique tokens: 210337\n",
      "--------------------\n",
      "Example comment: alignment on this subject and which are contrary to those of DuLithgow: [15, 13, 242, 4, 53, 19, 1815, 2, 142, 3]\n"
     ]
    }
   ],
   "source": [
    "tokenizer =   tf.keras.preprocessing.text.Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences =   tokenizer.texts_to_sequences(sentences)\n",
    "word_index =  tokenizer.word_index\n",
    "\n",
    "print(f'Biggest index: {max(max(seq) for seq in sequences if len(seq) > 0)}')\n",
    "print(f'Unique tokens: {len(word_index)}')\n",
    "print('-' * 20)\n",
    "print(f'Example comment: {sentences[9]}: {sequences[9]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dac35d",
   "metadata": {},
   "source": [
    "Creamos las secuencias del tamaño especificado, haciendo _padding_ donde corresponda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "738d3835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data tensor shape: (159571, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,  688,   75,    1,  126,  130,\n",
       "        177,   29,  672, 4511, 1116,   86,  331,   51, 2278,   50, 6864,\n",
       "         15,   60, 2756,  148,    7, 2937,   34,  117, 1221, 2825,    4,\n",
       "         45,   59,  244,    1,  365,   31,    1,   38,   27,  143,   73,\n",
       "       3462,   89, 3085, 4583, 2273,  985])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences,\n",
    "    value=0,\n",
    "    maxlen=MAX_SEQUENCE_LENGTH\n",
    ")\n",
    "print(f'Data tensor shape: {data.shape}')\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ece6e",
   "metadata": {},
   "source": [
    "Creamos una capa de embedding a partir de los datos de GloVe. Si resulta que la palabra no está dentro de GloVe, dicha palabra se quedará como un vector de ceros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b548d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding with GloVe vectors... done\n"
     ]
    }
   ],
   "source": [
    "print('Loading embedding with GloVe vectors... ', end='')\n",
    "# Cargamos sólo las palabras elegidas de nuestro conjunto de datos\n",
    "num_words = min(MAX_VOCAB_SIZE, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i < MAX_VOCAB_SIZE:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Creamos la capa de embedding\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "  input_dim=num_words,\n",
    "  output_dim=EMBEDDING_DIM,\n",
    "  weights=[embedding_matrix],\n",
    "  input_length=MAX_SEQUENCE_LENGTH,\n",
    "  trainable=False,\n",
    ")\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6eedd5",
   "metadata": {},
   "source": [
    "## Clasificación usando CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee0758b",
   "metadata": {},
   "source": [
    "Haremos una primera aproximación al problema usando redes convolucionales. En este caso, nuestras frases serán representadas por \"imágenes\" de una única fila, con tantas columnas como a longitud de la secuencia especificada y tantos canales como dimensión tiene cada valabra de la secuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50554551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 50, 50)            500000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 48, 16)            2416      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 768)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 6152      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 508,577\n",
      "Trainable params: 8,577\n",
      "Non-trainable params: 500,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_ = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "x = embedding_layer(input_)\n",
    "x = tf.keras.layers.Conv1D(16, kernel_size=3, activation='relu')(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(8, activation='relu')(x)\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(input_, output)\n",
    "model.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['binary_accuracy'],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4550806",
   "metadata": {},
   "source": [
    "Entrenamos el modelo (10 epochs con separación de conjunto de validación del 10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccda9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(data, targets, epochs=10, validation_split=0.1, batch_size=4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d17311",
   "metadata": {},
   "source": [
    "Veamos qué tal ha ido el entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddec59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training: {history.history[\"loss\"][-1]:.2f}, validation: {history.history[\"val_loss\"][-1]:.2f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['binary_accuracy'], label='Training')\n",
    "plt.plot(history.history['val_binary_accuracy'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(f'Training: {history.history[\"binary_accuracy\"][-1]:.2f}, validation: {history.history[\"val_binary_accuracy\"][-1]:.2f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0460b0f",
   "metadata": {},
   "source": [
    "Vaya, parece que bastante bien. Veamos ahora qué tal lo hace con el conjunto de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434fa2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Datasets/Toxic/test.csv')\n",
    "test_sentences = df['comment_text'].fillna('DUMMY_VALUE').values\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_data = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    test_sequences,\n",
    "    value=0,\n",
    "    padding='post',\n",
    "    maxlen=MAX_SEQUENCE_LENGTH\n",
    ")\n",
    "df = pd.read_csv('Datasets/Toxic/test_labels.csv')\n",
    "test_targets = df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values[:,0]\n",
    "\n",
    "r = model.evaluate(test_data, test_targets)\n",
    "print(f'Results (loss, acc): {r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e3c856",
   "metadata": {},
   "source": [
    "## Clasificación usando RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66be9c25",
   "metadata": {},
   "source": [
    "Ahora haremos el mismo ejercicio, pero usando un modelo basado en redes neuronales recurrentes. Para ello partiremos de un nuevo modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5392767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "x = embedding_layer(input_)\n",
    "x = tf.keras.layers.GRU(128)(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(input_, output)\n",
    "model.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['binary_accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ee23e3",
   "metadata": {},
   "source": [
    "Lo entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bd1f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(data, targets, epochs=10, validation_split=0.1, batch_size=4092)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade383c",
   "metadata": {},
   "source": [
    "Veamos como ha ido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f8c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training: {history.history[\"loss\"][-1]:.2f}, validation: {history.history[\"val_loss\"][-1]:.2f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['binary_accuracy'], label='Training')\n",
    "plt.plot(history.history['val_binary_accuracy'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(f'Training: {history.history[\"binary_accuracy\"][-1]:.2f}, validation: {history.history[\"val_binary_accuracy\"][-1]:.2f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d202e83a",
   "metadata": {},
   "source": [
    "¿Y el desempeño en test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Datasets/Toxic/test.csv')\n",
    "test_sentences = df['comment_text'].fillna('DUMMY_VALUE').values\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_data = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    test_sequences,\n",
    "    value=0,\n",
    "    padding='post',\n",
    "    maxlen=MAX_SEQUENCE_LENGTH\n",
    ")\n",
    "df = pd.read_csv('Datasets/Toxic/test_labels.csv')\n",
    "test_targets = df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values[:,0]\n",
    "\n",
    "r = model.evaluate(test_data, test_targets)\n",
    "print(f'Results (loss, acc): {r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5b8e8e",
   "metadata": {},
   "source": [
    "# Clasificación de textos con RNN bidireccionales\n",
    "\n",
    "Ahora haremos el mismo ejercicio, pero usando un modelo basado en redes neuronales recurrentes bidireccionales. Para ello partiremos de un nuevo modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721dcfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "x = embedding_layer(input_)\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128))(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(input_, output)\n",
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ddef6a",
   "metadata": {},
   "source": [
    "Un modelo bastante más complejo. Vamos a entrenarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9b7814",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(data, targets, epochs=10, validation_split=0.1, batch_size=4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a92f277",
   "metadata": {},
   "source": [
    "Y ahora veamos cómo ha evolucionado el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24067f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training: {history.history[\"loss\"][-1]:.2f}, validation: {history.history[\"val_loss\"][-1]:.2f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(f'Training: {history.history[\"accuracy\"][-1]:.2f}, validation: {history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f495d234",
   "metadata": {},
   "source": [
    "Veamos el desempeño en test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90813965",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Datasets/Toxic/test.csv')\n",
    "test_sentences = df['comment_text'].fillna('DUMMY_VALUE').values\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_data = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    test_sequences,\n",
    "    value=0,\n",
    "    padding='post',\n",
    "    maxlen=MAX_SEQUENCE_LENGTH\n",
    ")\n",
    "df = pd.read_csv('Datasets/Toxic/test_labels.csv')\n",
    "test_targets = df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values[:,0]\n",
    "\n",
    "r = model.evaluate(test_data, test_targets)\n",
    "print(f'Results (loss, acc): {r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43585490",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7defd88",
   "metadata": {},
   "source": [
    "Hemos implementado y comparado tres técnicas de aprendizaje profundo para la tarea de clasificación de textos: redes de convolución, redes recurrentes y redes recurrentes bidireccionales. Las redes recurrentes bidireccionales suelen ser idóneas para estos casos ya que son capaces de entender en contexto de las palabras **dentro** de la frase, tanto por las palabras antecedentes como las consecuentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33005bde",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
