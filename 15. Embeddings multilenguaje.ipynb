{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# _Embeddings_ multilenguaje<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Autor: Alberto Díaz Álvarez<br>Última actualización: 2023-03-14</small></i></div>\n",
    "                                                  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las aplicaciones más prometedoras de los _Embeddings_ es el aprendizaje de transferencia multilingüe.\n",
    "\n",
    "Supongamos que desea entrenar un modelo de NLP en varios idiomas, pero sólo se dispone de datos de entrenamiento en uno de ellos. Recopilar nuevos datos de entrenamiento para cada una de las lenguas destino puede resultar costoso, y traducir todos los textos que se quieren procesar más todavía. Sin embargo, con los _embeddings_ multilingües se puede intentar transferir un modelo de un idioma a otro de forma más eficaz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "\n",
    "Entrenaremos un _embedding_ para que sea capaz de determinar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports y configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación importaremos las librerías que se usarán a lo largo del notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wordfreq import top_n_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asimismo, configuramos algunos parámetros para adecuar la presentación gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (20, 6),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjunto de datos\n",
    "\n",
    "Vamos a partir directamente de dos _embeddings_ extraídos de la biblioteca [Muse](https://github.com/facebookresearch/MUSE) (perteneciente a [Facebook Research](https://research.fb.com/)).\n",
    "\n",
    "Se trata de un respositorio que contiene _embeddings_ entrenados con la Wikipedia para más de 30 idiomas alineados en un único espacio vectorial. Nos vamos a centrar en los idiomas inglés y español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading english...OK\n",
      "Downloading spanish...OK\n"
     ]
    }
   ],
   "source": [
    "WIKI_EN = 'tmp/wiki.en.vec'\n",
    "WIKI_ES = 'tmp/wiki.es.vec'\n",
    "\n",
    "if not os.path.exists('tmp/wiki.en.vec'):\n",
    "    print('Downloading english...', end='')\n",
    "    urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec', WIKI_EN)\n",
    "    print('OK')\n",
    "if not os.path.exists('wiki.es.vec'):\n",
    "    print('Downloading spanish...', end='')\n",
    "    urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.es.vec', WIKI_ES)\n",
    "    print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y cargamos los pesos y las palabras en sus respectivas variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from tmp/wiki.en.vec\n",
      "Loading vectors from tmp/wiki.es.vec\n"
     ]
    }
   ],
   "source": [
    "def load_embedding(path):\n",
    "    print(f'Loading vectors from {path}')\n",
    "    embedding = []\n",
    "    word_index = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            if i > 100000:\n",
    "                break\n",
    "            word, emb = line.rstrip().split(' ', 1)\n",
    "            emb = np.fromstring(emb, sep=' ')\n",
    "            embedding.append(emb)\n",
    "            word_index[word] = len(word_index)\n",
    "\n",
    "    embedding = np.vstack(embedding)\n",
    "    return embedding, word_index\n",
    "\n",
    "en_embedding, en_embedding_word_index = load_embedding(WIKI_EN)\n",
    "es_embedding, es_embedding_word_index = load_embedding(WIKI_ES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos\n",
    "\n",
    "Al igual que antes, entrenaremos un modelo sencillo de análisis de sentimiento con el conjunto de IMDb.\n",
    "\n",
    "Tras descargar los datos, aplicaremos los pasos tradicionales de preprocesamiento. Trabajaremos con un vocabulario de 10.000 palabras, cortaremos los textos largos después de 256 palabras y rellenaremos todos los textos más cortos con padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 10000\n",
    "START_INDEX = 1\n",
    "OOV_INDEX   = 2\n",
    "INDEX_FROM  = 3\n",
    "EMBEDDING_DIM = 300\n",
    "SEQUENCE_LENGTH = 256\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(\n",
    "    num_words=VOCABULARY_SIZE,  # Número de palabras que queremos en nuestro vocabulario,\n",
    "    start_char=START_INDEX,     # Índice para el token de comienzo de secuencia\n",
    "    oov_char=OOV_INDEX,         # Índice para el caracter desconocido (out of vocabulary)\n",
    "    index_from=INDEX_FROM,      # Índice a partir del cual se empiezan a indexar las palabras\n",
    ")\n",
    "\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    x_train,\n",
    "    padding='post',\n",
    "    maxlen=SEQUENCE_LENGTH\n",
    ")\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    x_test,\n",
    "    padding='post',\n",
    "    maxlen=SEQUENCE_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Embeddings_ preentrenados\n",
    "\n",
    "Para poder transferir nuestro modelo entre idiomas, nuestra capa de _embedding_ necesita proyectar las palabras en un espacio vectorial multilingüe.\n",
    "\n",
    "Por tanto, vamos a inicializar nuestra capa de _embedding_ con (un subconjunto de) las incrustaciones de palabras descargadas. En primer lugar, tenemos que averiguar qué palabras representan los índices de los datos IMDB preprocesados y después, tenemos que crear un _embedding_ donde cada fila contenga el vector de palabra indexad0 por su número de fila. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10002, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_embedding_matrix(target_word_index, embedding_word_index, embedding, rows, cols):\n",
    "    embedding_matrix = np.zeros((rows, cols))\n",
    "    for word, index in target_word_index.items():\n",
    "        if index < rows and word in embedding_word_index: \n",
    "            embedding_matrix[index] = embedding[embedding_word_index[word]]\n",
    "    return embedding_matrix\n",
    "\n",
    "en_word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "en_word_index = {word: (index + INDEX_FROM) for word, index in en_word_index.items()}\n",
    "en_word_index['<PAD>']   = 0\n",
    "en_word_index['<START>'] = START_INDEX\n",
    "en_word_index['<UNK>']   = OOV_INDEX\n",
    "\n",
    "en_embedding_matrix = create_embedding_matrix(\n",
    "    target_word_index=en_word_index,\n",
    "    embedding_word_index=en_embedding_word_index,\n",
    "    embedding=en_embedding,\n",
    "    rows=VOCABULARY_SIZE + INDEX_FROM-1,\n",
    "    cols=EMBEDDING_DIM\n",
    ")\n",
    "en_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando y entrenando nuestro modelo\n",
    "\n",
    "Ahora es el momento de crear y entrenar nuestro modelo, que será prácticamente igual que el del ejemplo anterior pero habiendo cambiado el embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 256, 300)          3000600   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 76800)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 614408    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,615,017\n",
      "Trainable params: 614,417\n",
      "Non-trainable params: 3,000,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=VOCABULARY_SIZE + INDEX_FROM - 1,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        input_length=SEQUENCE_LENGTH,\n",
    "        weights=[en_embedding_matrix],\n",
    "        trainable=False\n",
    "    ),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_split=0.1, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y comprobamos qué tal ha evolucionado el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training: {history.history[\"loss\"][-1]:.2f}, validation: {history.history[\"val_loss\"][-1]:.2f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['binary_accuracy'], label='Training')\n",
    "plt.plot(history.history['val_binary_accuracy'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(f'Training: {history.history[\"binary_accuracy\"][-1]:.2f}, validation: {history.history[\"val_binary_accuracy\"][-1]:.2f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver si el modelo hace lo que tiene que hacer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = [\n",
    "    'the movie was amazing'.split(),\n",
    "    'i hated it'.split()\n",
    "]\n",
    "vectors = [\n",
    "    [en_word_index.get(word, OOV_INDEX) for word in comment]\n",
    "    for comment in comments\n",
    "]\n",
    "padded_vectors = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    vectors,\n",
    "    padding='post',\n",
    "    maxlen=SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "model.predict(padded_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transferencia de nuestro modelo\n",
    "\n",
    "Obviamente, el modelo anterior no puede utilizarse para clasificar textos escritos en español. Para transferir nuestro modelo del inglés al español, tenemos que sustituir su embedding en inglés por una capa de embedding en español.\n",
    "\n",
    "De nuevo vamos a trabajar con un vocabulario de 10.000 palabras. Desgraciadamente, no disponemos de un conjunto de textos en español relevantes en los que encajar el vocabulario, por lo que usaremos la librería [wordfreq](https://github.com/LuminosoInsight/wordfreq). Esta contiene información sobre la frecuencia de las palabras en muchas lenguas occidentales. En particular, `top_n_list` nos dará las $n$ palabras más frecuentes de un idioma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_n_list('en', 10))\n",
    "print(top_n_list('es', 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construiremos nuestro vocabulario a partir de las 10.000 palabras españolas más frecuentes y lo utilizaremos para crear un embedding en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_word_index = {word: idx+INDEX_FROM for idx, word in enumerate(top_n_list('es', VOCABULARY_SIZE))}\n",
    "en_word_index['<PAD>']   = 0\n",
    "en_word_index['<START>'] = START_INDEX\n",
    "en_word_index['<UNK>']   = OOV_INDEX\n",
    "\n",
    "es_embedding_matrix = create_embedding_matrix(\n",
    "    target_word_index=es_word_index,\n",
    "    embedding_word_index=es_embedding_word_index,\n",
    "    embedding=es_embedding,\n",
    "    rows=VOCABULARY_SIZE + INDEX_FROM-1,\n",
    "    cols=EMBEDDING_DIM\n",
    ")\n",
    "es_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo siguiente, vamos a reemplazar los pesos del embedding original (el inglés) por los pesos del nuevo embedding (el español). Para ello hay que usar el método `set_weights([weight_matrix])` del objeto de la clase `Embedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([es_embedding_matrix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y como por arte de magie (magia rara) el modelo que acabamos de entrenar con textos en inglés ahora puede tomar textos en castellano como entrada y clasificarlos correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = [\n",
    "    'la película fue muy buena'.split(),\n",
    "    'no me ha gustado nada'.split()\n",
    "]\n",
    "vectors        =  [\n",
    "    [es_word_index.get(word, OOV_INDEX) for word in comment]\n",
    "    for comment in comments\n",
    "]\n",
    "padded_vectors = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    vectors,\n",
    "    padding='post',\n",
    "    maxlen=SEQUENCE_LENGTH\n",
    ")\n",
    "model.predict(padded_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "Los _embeddings_ multilingües nos permiten transferir un modelo de un idioma a otro. Es muy útil cuando se necesita aplicar un mismo modelo a varios idiomas y únicamente se disponen de datos en uno de ellos.\n",
    "\n",
    "Por supuesto, esto no está exento de problemas. Cuanto más diferentes son dos idiomas, peor se comportará (lo acabamos de ver con español e inglés). Los idiomas no sólo son palabras, sino también expresiones, órdenes de palabras, etc. Sin embargo, cuando dos idiomas son parecidos en términos lingüísticos (e.g. español y asturiano), esta solución es razonablemente buena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
