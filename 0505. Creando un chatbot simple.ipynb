{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4480ff39",
   "metadata": {},
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Creando un chatbot simple<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Autor: Alberto Díaz Álvarez<br>Última actualización: 2023-03-28</small></i></div>\n",
    "                                                  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2489222",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17030ba",
   "metadata": {},
   "source": [
    "Los _chatbots_ son sistemas informáticos diseñados para simular una conversación humana y responder preguntas de manera automatizada. Estos sistemas han ganado popularidad en los últimos años debido a su capacidad para proporcionar asistencia al cliente las 24 horas del día, los 7 días de la semana, y su potencial para mejorar la eficiencia de los negocios al automatizar tareas rutinarias.\n",
    "\n",
    "Los chatbots se han vuelto especialmente útiles en la industria del servicio al cliente, donde pueden ayudar a las empresas a reducir los tiempos de espera y mejorar la satisfacción del cliente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d1e17",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b9e324",
   "metadata": {},
   "source": [
    "El objetivo de este proyecto es diseñar y desarrollar un chatbot simple siguiedo la arquitectura _seq2seq_. Este modelo, también llamado codificador-decodificador, utiliza la (corta) memoria a largo plazo que mantienen unidades como LSTM o GRU para generar una secuencia de salida a partir de una secuencia de entrada tras haber sido entrenado con un corpus suficientemente grande de secuencias origen y sus correspondientes secuencias de destino."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d401eac",
   "metadata": {},
   "source": [
    "## Imports y configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa528aec",
   "metadata": {},
   "source": [
    "A continuación importaremos las librerías que se usarán a lo largo del notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a8c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4fec89",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea42ab1",
   "metadata": {},
   "source": [
    "## Constantes previas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5588872c",
   "metadata": {},
   "source": [
    "Vamos a comenzar definiendo una serie de constantes que usaremos a lo largo del _notebook_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbd4a4d",
   "metadata": {},
   "source": [
    "Comenzamos por las etiquetas referentes a comienzo y fin de frase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c73011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_SOS = '#sos#'\n",
    "TOKEN_EOS = '#eos#'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8817fd4",
   "metadata": {},
   "source": [
    "Definimos también el número de características de salida del embedding, el máximo de longitud de secuencia y el tamaño de nuestro vocabulario de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "985abe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "MAX_SEQUENCE_LEN = 20\n",
    "VOCABULARY_SIZE = 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83617f72",
   "metadata": {},
   "source": [
    "Por último, indicamos las \"unidades\" (neuronas) de salida de nuestra unidad recurrente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab803b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNITS = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7223928",
   "metadata": {},
   "source": [
    "## Carga y preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d3a3bc",
   "metadata": {},
   "source": [
    "Vamos a descargar una serie de pares de pregunta-respuesta para que nuestro _chatbot_ aprenda de ellas. Vamos a hacerlo en español, por cambiar un poco.\n",
    "\n",
    "Para ello descargaremos el repositorio [ChatterBot Language Training Corpus](https://github.com/gunthercox/chatterbot-corpus) de GitHub, el cual contiene varios _corpus_ en varios idiomas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07075092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading corpus ... OK\n",
      "Unpacking corpus ... OK\n"
     ]
    }
   ],
   "source": [
    "CORPUS_URL = 'https://github.com/gunthercox/chatterbot-corpus/archive/master.zip'\n",
    "CORPUS_FILE = 'tmp/master.zip'\n",
    "\n",
    "# Creamos el directorio temporal si no existiese ya\n",
    "TMP_DIR = 'tmp/'\n",
    "if not os.path.isdir(TMP_DIR):\n",
    "    os.makedirs(TMP_DIR)\n",
    "\n",
    "# Descargamos todo el repositorio\n",
    "if not os.path.exists(CORPUS_FILE):\n",
    "    print('Downloading corpus ... ', end='')\n",
    "    with open(CORPUS_FILE, 'wb') as f:\n",
    "        r = requests.get(CORPUS_URL, allow_redirects=True)\n",
    "        f.write(r.content)\n",
    "    print('OK')\n",
    "\n",
    "# Lo descomprimimos en el directorio temporal\n",
    "print('Unpacking corpus ... ', end='')\n",
    "shutil.unpack_archive(CORPUS_FILE, TMP_DIR)\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbc803a",
   "metadata": {},
   "source": [
    "Con el dataset descargado, recogemos todas las conversaciones de la sección en español. La verdad es que las conversaciones no son muy para tirar cohetes, pero bueno, a ver qué sale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce93f965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ... 50 conversations loaded\n"
     ]
    }
   ],
   "source": [
    "print('Loading ... ', end='')\n",
    "shutil.unpack_archive(CORPUS_FILE, TMP_DIR)\n",
    "chats = []\n",
    "for path in glob.glob(f'{TMP_DIR}chatterbot-corpus-master/chatterbot_corpus/data/spanish/*.yml'):\n",
    "    with open(path) as f:\n",
    "        chats.extend(c for c in yaml.safe_load(f).get('conversaciones', []))\n",
    "print(f'{len(chats)} conversations loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed780916",
   "metadata": {},
   "source": [
    "Bien, ya tenemos unas cuantas conversaciones que se componen de \"preguntas y \"respuestas\" (bueno, no tienen por qué ser técnicamente preguntas y respuestas, pero bueno, el toma y daca de una conversación).\n",
    "\n",
    "Ahora lo separaremos en tres conjuntos:\n",
    "\n",
    "- `questions`: El de preguntas, sin nada especial.\n",
    "- `answers`: El de respuestas, con el que alimentaremos la parte decodificadora de nuestra red, y que por tanto requerirá dos etiquetas al principio y al final para que denoten el comienzo y el fin de una frase\n",
    "- `answers_no_start`: El de respuestas con el que contrastaremos la respuesta dada por el decodificador, y que por tanto sólo requerirá una etiqueta al final para que denote el fin de una frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6e15691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing conversations ... 212 answers and questions loaded\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "answers_no_start = []\n",
    "\n",
    "print('Processing conversations ... ', end='')\n",
    "for chat in chats:\n",
    "    for q, a in zip(chat[:-1], chat[1:]):\n",
    "        questions.append(q)\n",
    "        answers.append(f'{TOKEN_SOS} {a} {TOKEN_EOS}')\n",
    "        answers_no_start.append(f'{a} {TOKEN_EOS}')\n",
    "print(f'{len(questions)} answers and questions loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11437a5",
   "metadata": {},
   "source": [
    "No es mucho, pero bueno, es algo por lo que comenzar.\n",
    "\n",
    "Una vez tenemos nuestras frases, vamos a crear el componente que traducirá estas fsentencias a secuencias de tamaño fijo de enteros. Para esto usaremos una capa denominada `TextVectorization`, la cual hace la transformación automática (según la hayamos configurado, claro) de textos a secuencias de enteros, y que es directamente encajable en el _pipeline_ de un modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9415c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la capa de vectorización (frase -> secuencia de enteros)\n",
    "def standardize(inputs):\n",
    "    inputs = tf.strings.regex_replace(inputs, r'[!\"$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']', \"\")\n",
    "    return tf.strings.lower(inputs)\n",
    "\n",
    "vectorization_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCABULARY_SIZE,\n",
    "    output_sequence_length=MAX_SEQUENCE_LEN,\n",
    "    standardize=standardize,\n",
    "    name='vectorization',\n",
    ")\n",
    "\n",
    "# Lo inicializamos con el vocabulario de nuestro dataset. Aunque aquí\n",
    "# no es necesario, lo cargamos en batch para ilustrar cómo funciona.\n",
    "# Esto nos permitiría trabajar con datasets muy grandes.\n",
    "text_dataset = tf.data.Dataset.from_tensor_slices(questions + answers)\n",
    "vectorization_layer.adapt(text_dataset.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cead435",
   "metadata": {},
   "source": [
    "Y vamos a aprovechar que hemos creado nuestra capa de vectorización para transformar a vectores el conjunto de salida con el que compararemos la salida de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a920a449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9 197   4 ...   0   0   0]\n",
      " [  4  23  38 ...   0   0   0]\n",
      " [  8 304 150 ...   0   0   0]\n",
      " ...\n",
      " [ 15 396 100 ...   0   0   0]\n",
      " [ 17 395   4 ...   0   0   0]\n",
      " [ 17 194  45 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "answers_no_start = vectorization_layer(answers_no_start).numpy()\n",
    "print(answers_no_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5159eccc",
   "metadata": {},
   "source": [
    "Por último, al igual que hemos creado la capa de vectorización, crearemos también la capa de _embedding_ que usaremos tanto en la parte codificadora como en la decodificadora de nuestro modelo seq2seq. Igual que la estamos creando, podemos hacer uso de un embedding ya existente como GLoVe (y lo mismo esto no lo leéis y lo he cambiado para tirar de una implementación de GLoVe en español)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc58914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    VOCABULARY_SIZE + 1, # El 0 está vetado, por eso es +1\n",
    "    EMBEDDING_DIM,\n",
    "    mask_zero=True,      # 0 es padding (no se debe usar en el vocabulario)\n",
    "    name='embedding',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42148f2f",
   "metadata": {},
   "source": [
    "## Creación del modelo\n",
    "\n",
    "Ya hemos visto en teoría que el esquema _seq2seq_ es un problema tipo _many-to-many_. De alguna manera tenemos que conseguir que nuestro modelo aprenda las relaciones existentes entre las palabras de nuestro texto junto con una codificación que tenga en cuenta estas relaciones y el contexto de las preguntas y respuestas.\n",
    "\n",
    "Lo que haremos será entrenar simultáneamente dos capas GRU. Concretamente entrenaremos la primera para las preguntas y luego utilizaremos sus pesos como estado inicial para entrenar la segunda para las respuestas.\n",
    "\n",
    "La primera capa GRU será el **codificador**, esto es, el procesamiento de la entrada para devolver una codificación de esta en su estado interno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bf9e5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_gru = tf.keras.layers.GRU(UNITS, return_state=True, name='Encoder-GRU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d345c1c6",
   "metadata": {},
   "source": [
    "La entrada a esta capa será una secuencia ya vectorizada y convertida a vectores de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d49dfd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string, name='Question')\n",
    "encoder_inputs_word_vectors = embedding_layer(vectorization_layer(encoder_inputs))\n",
    "\n",
    "encoder_output, encoder_state = encoder_gru(encoder_inputs_word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3526f073",
   "metadata": {},
   "source": [
    "La segunda capa GRU será la correspondiente al **decodificador**. Tomará el estado como contexto para predecir las siguientes palabras de la secuencia objetivo. Devolverá tantas salidas como elementos tenga la secuencia de entrada, básicamente para poder entrenala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2493ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_gru = tf.keras.layers.GRU(UNITS, return_state=True, return_sequences=True, name='Decoder-GRU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ecc4b2",
   "metadata": {},
   "source": [
    "El decodificador se conectará también a una secuencia de entrada (para hacer _teacher enforcing_) ya vectorizada y pasada a vectores de palabras, y a una capa densa de salida que hará la clasificación a la palabra más probable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a5c7ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string, name='Answer')\n",
    "decoder_inputs_word_vectors = embedding_layer(vectorization_layer(decoder_inputs))\n",
    "\n",
    "decoder_output, _ = decoder_gru(decoder_inputs_word_vectors, initial_state=[encoder_state])\n",
    "decoder_dense = tf.keras.layers.Dense(VOCABULARY_SIZE, activation='softmax', name='Decoder-classifier')\n",
    "decoder_output = decoder_dense(decoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e1fdd",
   "metadata": {},
   "source": [
    "Hay que prestar especial atención a que la capa `Decoder-GRU` recibe como estado inicial el estado de la capa `Encoder-GRU`.\n",
    "\n",
    "Ahora ya podemos crear el modelo especificando entradas y salidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "996f0920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Answer (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " Question (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " vectorization (TextVectorizati  (None, 20)          0           ['Question[0][0]',               \n",
      " on)                                                              'Answer[0][0]']                 \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 20, 50)       125050      ['vectorization[0][0]',          \n",
      "                                                                  'vectorization[1][0]']          \n",
      "                                                                                                  \n",
      " Encoder-GRU (GRU)              [(None, 256),        236544      ['embedding[0][0]']              \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " Decoder-GRU (GRU)              [(None, 20, 256),    236544      ['embedding[1][0]',              \n",
      "                                 (None, 256)]                     'Encoder-GRU[0][1]']            \n",
      "                                                                                                  \n",
      " Decoder-classifier (Dense)     (None, 20, 2500)     642500      ['Decoder-GRU[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,240,638\n",
      "Trainable params: 1,240,638\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], [decoder_output])\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='sparse_categorical_crossentropy')\n",
    "model.summary()\n",
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7257c3aa",
   "metadata": {},
   "source": [
    "Dado que la salida de nuestro modelo (la salida del _decoder_) es un _softmax_ de 2500 palabras, usamos `sparse_categorical_crossentropy` para utilizar el índice de la palabra en lugar de pasar a una codificación _one-hot_ todas las labels.\n",
    "\n",
    "Ahora vamos a entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "742b35e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 6s 61ms/step - loss: 7.8057\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 7.7134\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 6.6458\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 5.5695\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 5.2433\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 5.1109\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 5.0304\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 4.9735\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 4.9138\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 1s 80ms/step - loss: 4.8814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15e04c29970>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([np.array(questions), np.array(answers)], np.array(answers_no_start), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02392841",
   "metadata": {},
   "source": [
    "Una vez entrenado el modelo general, extraeremos las capas entrenadas a dos componentes. Primero, el **_encoder_**, el más sencillo. Su entrada será una pregunta/frase y su salida será el estado interno de la neurona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "203912ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pydot\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\andro\\appdata\\roaming\\python\\python39\\site-packages (from pydot) (2.4.7)\n",
      "Installing collected packages: pydot\n",
      "Successfully installed pydot-1.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cf79465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting graphviz\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "     ---------------------------------------- 0.0/47.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 47.0/47.0 kB 2.5 MB/s eta 0:00:00\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fca5c892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "encoder_model = tf.keras.models.Model(encoder_inputs, encoder_state)\n",
    "tf.keras.utils.plot_model(encoder_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b159fd7d",
   "metadata": {},
   "source": [
    "Segundo, el **_decoder_**, que tomará dos entradas: el estado del _encoder_ (el espacio latente de la pregunta) y la palabra anterior (o el token _start of sequence_ si no la hay), con la que generaremos la nueva palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39ea21ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "decoder_input_state = tf.keras.layers.Input(shape=(UNITS,), name='Input-state')\n",
    "decoder_output, decoder_state = decoder_gru(decoder_inputs_word_vectors, initial_state=[decoder_input_state])\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "decoder_model = tf.keras.models.Model(\n",
    "    [decoder_inputs, decoder_input_state],\n",
    "    [decoder_output, decoder_state],\n",
    ")\n",
    "tf.keras.utils.plot_model(decoder_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42499821",
   "metadata": {},
   "source": [
    "Ahora ya podemos hablar con el chatbot. El proceso es el siguiente:\n",
    "\n",
    "1. Tomamos una pregunta como entrada y predecimos los valores de estado utilizando el _encoder_.\n",
    "1. Establecemos los valores de estado del _encoder_ en el _decoder_.\n",
    "1. Establecemos el valor `TOKEN_SOS` en la entrada del decoder y obtenemos el valor de salida $O$.\n",
    "1. Mientras $O$ no sea el token `TOKEN_EOS` o lleguemos a la longitud máxima de secuencia\n",
    "    1. Añadimos el valor de salida $O$ a nuestra secuencia resultado\n",
    "    2. Reemplazamos la entrada del decoder por $O$ y obtenemos el nuevo $O$ de la salida del decoder\n",
    "\n",
    "Lo implementaremos en una clase que denominaremos FedericoTalker porque sí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ae9ad2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedericoTalker:\n",
    "    def __init__(self, encoder, decoder):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def tell(self, message, max_len=None):\n",
    "        # Codificamos la pregunta\n",
    "        state = encoder_model.predict([message], verbose=0)\n",
    "        # Vamos generando palabras hasta que terminemos la frase\n",
    "        response = [TOKEN_SOS]\n",
    "        while response[-1] != TOKEN_EOS and len(response) < (max_len or np.inf):\n",
    "            output, state = decoder_model.predict([np.array([[response[-1]]]), state], verbose=0)\n",
    "            token = np.argmax(output[0,-1])\n",
    "            response.append(vectorization_layer.get_vocabulary()[token])\n",
    "        # Devolvemos la frase sin los tokens de comienzo y fin de frase\n",
    "        return ' '.join(response[1:-1])\n",
    "\n",
    "federico_talker = FedericoTalker(encoder_model, decoder_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc69b4",
   "metadata": {},
   "source": [
    "Ahora podemos crear un nuevo _chatbot_ con nuestros _encoder_ y _decoder_ y charlar con él."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f233ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no  no'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "federico_talker.tell('¿Perdona?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "568499b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfederico_talker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtell\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMe apetece saberlo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 12\u001b[0m, in \u001b[0;36mFedericoTalker.tell\u001b[1;34m(self, message, max_len)\u001b[0m\n\u001b[0;32m     10\u001b[0m response \u001b[38;5;241m=\u001b[39m [TOKEN_SOS]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m response[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m TOKEN_EOS \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response) \u001b[38;5;241m<\u001b[39m (max_len \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39minf):\n\u001b[1;32m---> 12\u001b[0m     output, state \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     token \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(output[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     14\u001b[0m     response\u001b[38;5;241m.\u001b[39mappend(vectorization_layer\u001b[38;5;241m.\u001b[39mget_vocabulary()[token])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py:2317\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2308\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m   2309\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2310\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2311\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2314\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   2315\u001b[0m         )\n\u001b[1;32m-> 2317\u001b[0m data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2320\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2321\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2325\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2328\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   2331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py:1579\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_coordinator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1579\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py:1259\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[0;32m   1258\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1261\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1274\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py:347\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flat_dataset\n\u001b[0;32m    345\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m indices_dataset\u001b[38;5;241m.\u001b[39mflat_map(slice_batch_indices)\n\u001b[1;32m--> 347\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshuffle_batch\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py:388\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrab_batch\u001b[39m(i, data):\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m    385\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m d: tf\u001b[38;5;241m.\u001b[39mgather(d, i, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), data\n\u001b[0;32m    386\u001b[0m     )\n\u001b[1;32m--> 388\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrab_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAUTOTUNE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;66;03m# Default optimizations are disabled to avoid the overhead of\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;66;03m# (unnecessary) input pipeline graph serialization and deserialization\u001b[39;00m\n\u001b[0;32m    392\u001b[0m options \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mOptions()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2296\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2294\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m MapDataset(\u001b[38;5;28mself\u001b[39m, map_func, preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m   2295\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2296\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallelMapDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2297\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2298\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2299\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2300\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2301\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2302\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:5555\u001b[0m, in \u001b[0;36mParallelMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m   5552\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_parallel_calls \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\n\u001b[0;32m   5553\u001b[0m     num_parallel_calls, dtype\u001b[38;5;241m=\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mint64, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_parallel_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5554\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m-> 5555\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39mparallel_map_dataset_v2(\n\u001b[0;32m   5556\u001b[0m     input_dataset\u001b[38;5;241m.\u001b[39m_variant_tensor,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   5557\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs,\n\u001b[0;32m   5558\u001b[0m     f\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[0;32m   5559\u001b[0m     num_parallel_calls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_parallel_calls,\n\u001b[0;32m   5560\u001b[0m     deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deterministic,\n\u001b[0;32m   5561\u001b[0m     use_inter_op_parallelism\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism,\n\u001b[0;32m   5562\u001b[0m     preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality,\n\u001b[0;32m   5563\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_args)\n\u001b[0;32m   5564\u001b[0m \u001b[38;5;28msuper\u001b[39m(ParallelMapDataset, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(input_dataset, variant_tensor)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:6184\u001b[0m, in \u001b[0;36mparallel_map_dataset_v2\u001b[1;34m(input_dataset, other_arguments, num_parallel_calls, f, output_types, output_shapes, use_inter_op_parallelism, deterministic, preserve_cardinality, metadata, name)\u001b[0m\n\u001b[0;32m   6182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   6183\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 6184\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   6185\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mParallelMapDatasetV2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_arguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6186\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6187\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_shapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_inter_op_parallelism\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6188\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_inter_op_parallelism\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeterministic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6189\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreserve_cardinality\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   6191\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "federico_talker.tell('Me apetece saberlo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "946e6ec8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfederico_talker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtell\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCreo que con tan poca conversación es difícil que me contestes a algo con sentido\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 12\u001b[0m, in \u001b[0;36mFedericoTalker.tell\u001b[1;34m(self, message, max_len)\u001b[0m\n\u001b[0;32m     10\u001b[0m response \u001b[38;5;241m=\u001b[39m [TOKEN_SOS]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m response[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m TOKEN_EOS \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response) \u001b[38;5;241m<\u001b[39m (max_len \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39minf):\n\u001b[1;32m---> 12\u001b[0m     output, state \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     token \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(output[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     14\u001b[0m     response\u001b[38;5;241m.\u001b[39mappend(vectorization_layer\u001b[38;5;241m.\u001b[39mget_vocabulary()[token])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py:2346\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2344\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_begin()\n\u001b[0;32m   2345\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2346\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():  \u001b[38;5;66;03m# Single epoch.\u001b[39;00m\n\u001b[0;32m   2347\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m   2348\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py:1304\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[0;32m   1303\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[1;32m-> 1304\u001b[0m     data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1305\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epochs):\n\u001b[0;32m   1306\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:499\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    501\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    502\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:703\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    699\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    701\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    702\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 703\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:742\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    739\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m    740\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[0;32m    741\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[1;32m--> 742\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3439\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3438\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3439\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3440\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3442\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "federico_talker.tell('Creo que con tan poca conversación es difícil que me contestes a algo con sentido')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43585490",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7defd88",
   "metadata": {},
   "source": [
    "La implementación de un chatbot basado en _seq2seq_ en español es una tarea algo compleja, pero mola ver cómo es capaz de responder (aunque sea de manera rudimentaria) a las preguntas que le hacemos.\n",
    "\n",
    "Este tipo de modelo de redes neuronales recurrentes es capaz de generar respuestas relativamente coherentes y contextualizadas a partir de las preguntas que se le plantean. La clave para el éxito de la implementación radica en una adecuada selección y preparación de los datos de entrenamiento, así como en una arquitectura de red neuronal bien diseñada y ajustada. Además, el uso de técnicas como el modelo de atención puede mejorar significativamente la calidad de las respuestas generadas por el chatbot.\n",
    "\n",
    "En general, la implementación de un chatbot basado en _seq2seq_ en español es una muy buena forma de experimentar con técnicas avanzadas de procesamiento del lenguaje natural y de construir un modelo capaz de comunicarse de manera efectiva con los usuarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33005bde",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
