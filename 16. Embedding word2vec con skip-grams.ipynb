{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb3e4566",
   "metadata": {},
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Word embedding con Word2Vec<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Autor: Alberto Díaz Álvarez<br>Última actualización: 2023-03-14</small></i></div>\n",
    "                                                  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0625fc1d",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbddc17",
   "metadata": {},
   "source": [
    "Comenzamos con los más importante de todo: _Word embedding techniques_ es una forma de decir _representar palabras de forma numérica_ pero con más gancho. Y una vez dicho esto, vamos a programar un proceso de aprendizaje de _embeddings_ a partir de corpus de texto. Nos centraremos en una técnica denominada _Word2Vec_, aunque ya hemos visto que hay más.\n",
    "\n",
    "_Word2vec_ se basa en una red neuronal que genera la matriz usando un entrenamiento supervisado sobre un problema de clasificación. El artículo donde se presenta el método es [Efficient Estimation of Word Representations in Vector Space (Mikolov et al.,2013)](https://arxiv.org/pdf/1301.3781.pdf) y es un método que se usa con bastante éxito a la hora de medir **similitud sintáctica y semántica de palabras**.\n",
    "\n",
    "El artículo explora dos modelos diferentes: _Bag-of-Words_ y _Skip-gram_. El más usado es este último, y será el que veamos en este ejercicio.\n",
    "\n",
    "La idea del _Skip-gram_ es la siguiente: dada una palabra (que denominaremos _palabra de contexto_), queremos entrenar un modelo de tal manera que sea capaz de predecir una palabra perteneciente a una ventana de tamaño $N$. Por ejemplo, suponiendo una ventana de tamaño $N = 3$ y dada la siguiente frase:\n",
    "\n",
    "> Todos <span style=\"color:red\">esos momentos se</span> **perderán** <span style=\"color:red\">en el tiempo</span> como lágrimas en la lluvia_\n",
    "\n",
    "La _palabra de contexto_ sería **perderán**, y entrenaríamos al modelo para predecir una de las palabras existentes dentro de la ventana especificada, es decir, una de entre `['esos', 'momentos', 'se', 'en', 'el', 'tiempo']`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de84c899",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d1ecaa",
   "metadata": {},
   "source": [
    "En este _notebook_ crearemos un _embedding_ a partir de la técnica _skip-gram_ de _Word2Vec_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f806243c",
   "metadata": {},
   "source": [
    "## Imports y configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01d185e",
   "metadata": {},
   "source": [
    "A continuación importaremos las bibliotecas que usaremos a lo largo del _notebook_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee694de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7286046",
   "metadata": {},
   "source": [
    "Asímismo, configuramos algunos parámetros para adecuar la presentación gráfica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd7ae796",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (16, 9),'figure.dpi': 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694a766e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f708e1e",
   "metadata": {},
   "source": [
    "## Construcción del corpus\n",
    "\n",
    "Hemos descargado un [corpus de textos](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data) sobre comentarios tóxicos (que usaremos más adelante en otros notebooks). El primer paso será leerlo y separarlo en frases. Este paso generalmente requiere bastante tiempo porque las fuentes originales de datos no vienen completamente limpias.\n",
    "\n",
    "Veamos primero el corpus del fichero con el conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49843b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Datasets/Toxic/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c87ad27",
   "metadata": {},
   "source": [
    "Para nuestro cometido (crear un _embedding_), nos da igual el output del modelo o el id de los ejemplos; Nosotros queremos los textos, así que vamos a extraerlos, eliminando los blancos que puedan tener al principio y al final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0953c621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Explanation\\nWhy the edits made under my usern...\n",
       "1    D'aww! He matches this background colour I'm s...\n",
       "2    Hey man, I'm really not trying to edit war. It...\n",
       "3    \"\\nMore\\nI can't make any real suggestions on ...\n",
       "4    You, sir, are my hero. Any chance you remember...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = df['comment_text'].str.strip()\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88339f8",
   "metadata": {},
   "source": [
    "La variable `corpus` apunta a una serie con todas las frases de nuestro conjunto. Vamos a tokenizar cada uno de los comentarios, convirtiéndolos en una lista de palabras. Para ello usaremos la función `tokenizer` incluida en keras, aunque es importante entender que este paso no es trivial y seguramente requiera de mucho preproceso para tener un conjunto de datos de calidad (e.g. lematización, $n$-gramas, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd22c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a66f01",
   "metadata": {},
   "source": [
    "En este momento, nuestro _tokenizer_ ha procesado todos los comentarios y ha extraído todas las palabras, asignándoles un identifficador a cada una. Las almacenaremos en dos diccionarios para poder convertirlas en enteros (para identificar la palabra) y en palabras (una vez tengamos el entero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10a49b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2id: {'the': 1, 'to': 2, 'of': 3, 'and': 4} ...\n",
      "id2word: {1: 'the', 2: 'to', 3: 'of', 4: 'and'} ...\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "index_word = {index: word for word, index in word_index.items()}\n",
    "\n",
    "print(f'word2id: {dict(list(word_index.items())[0:4])} ...')\n",
    "print(f'id2word: {dict(list(index_word.items())[0:4])} ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c711e2d2",
   "metadata": {},
   "source": [
    "Por último, cada uno de los comentarios del corpus será transformado en una lista de enteros donde cada token del mismo se reemplazará por el entero que representa. Obtendremos también el tamaño de nuestro vocabulario a partir del número de palabras identificadas.\n",
    "\n",
    "Para convertir una cadena de texto en una secuencia de palabras podemos usar la función de Keras `tf.keras.preprocessing.text.text_to_word_sequence(text)`. A partir de ahí sacar el índice de cada palabra es trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1dd139b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus sentences: 159571 sentences\n",
      "Vocabulary Size: 210338 words\n",
      "Sentence example: [10960, 15, 13, 242, 4, 53, 19, 1815, 2, 142, 3, 65704]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    [word_index[w] for w in tf.keras.preprocessing.text.text_to_word_sequence(text)]\n",
    "    for text in corpus\n",
    "]\n",
    "vocab_size = len(word_index) + 1\n",
    "\n",
    "print(f'Corpus sentences: {len(sentences)} sentences')\n",
    "print(f'Vocabulary Size: {vocab_size} words')\n",
    "print(f'Sentence example: {sentences[9]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1952a1",
   "metadata": {},
   "source": [
    "## Generador de skip-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af53c104",
   "metadata": {},
   "source": [
    "Ahora generaremos los _skip-grams_. La idea es, de todas las frases del corpus (cada `sentence` de `sentences`) y dada una ventana de acción, sacar su contexto (las palabras alrededor) para determinar para cada par de palabras si son o no contextuales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0f1746c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (6610128, 3)\n",
      "[[    31    365      1]\n",
      " [  2756 158196      0]\n",
      " [    51   4511      1]\n",
      " ...\n",
      " [   102   1391      1]\n",
      " [    97 176286      0]\n",
      " [     9    217      1]]\n"
     ]
    }
   ],
   "source": [
    "WINDOW_SIZE = 5\n",
    "\n",
    "dataset = None\n",
    "for sentence in sentences[:5000]:\n",
    "    s = tf.keras.preprocessing.sequence.skipgrams(\n",
    "        sentence,\n",
    "        vocabulary_size=vocab_size,\n",
    "        window_size=WINDOW_SIZE,\n",
    "    )\n",
    "\n",
    "    if s[0] and s[1]:\n",
    "        X = np.array(s[0])\n",
    "        Y = np.array(s[1]).reshape(-1, 1)\n",
    "        subset = np.hstack((X, Y))\n",
    "\n",
    "        dataset = subset if dataset is None else np.vstack((dataset, subset))\n",
    "\n",
    "print(f'Dataset shape: {dataset.shape}')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ada0c3",
   "metadata": {},
   "source": [
    "Se puede ver que se ha generado, para cada par de palabras, si son (1) o no (0) contextuales. Esto es porque la función `skipgrams` transforma una secuencia de palabras (en realidad de enteros) en tuplas de la forma:\n",
    "\n",
    "- (palabra, palabra en el contexto), label 1 (positivo, son contextuales).\n",
    "- (palabra, palabra aleatoria del vocabulario), label 0 (no son contextuales)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72a01dc",
   "metadata": {},
   "source": [
    "## Creación y entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fcfeb6",
   "metadata": {},
   "source": [
    "Ya tenemos un dataset con inputs y sus respectivos outputs. Ahora el objetivo es entrenar un modelo que es capaz de determinar si dos palabras pertencen al mismo contexto.\n",
    "\n",
    "Para ello crearemos una capa de embedding que transformara las palabras en su vector de características. Las palabras serán aquellas que son o no contextuales, y se determinará lo cerca (más contextuales) o lejos (menos contextuales) que están, usando una medida de distancia (producto escalar).\n",
    "\n",
    "Por último, la salida de la red será una única neurona que se activará o no si son contextuales.\n",
    "\n",
    "Esta arquitectura forzará a que las palabras más contextuales estén más cerca, y por tanto que sus vectores de características sean más similares. Se espera así que la matriz de pesos de la capa de embedding converja a una representación de las características de las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a44df22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 1, 5)         1051690     ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 5, 1)         0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 5, 1)         0           ['embedding[1][0]']              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 5, 1)         0           ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 5, 1)         0           ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 1, 1)         0           ['dropout[0][0]',                \n",
      "                                                                  'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 1)            0           ['dot[0][0]']                    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            2           ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,051,692\n",
      "Trainable params: 1,051,692\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 5\n",
    "\n",
    "input_target = tf.keras.layers.Input((1,))\n",
    "input_context = tf.keras.layers.Input((1,))\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    input_length=1,\n",
    "    name='embedding',\n",
    ")\n",
    "\n",
    "target_embedding = embedding_layer(input_target)\n",
    "target_embedding = tf.keras.layers.Reshape((EMBEDDING_DIM, 1))(target_embedding)\n",
    "target_embedding = tf.keras.layers.Dropout(0.5)(target_embedding)\n",
    "\n",
    "context_embedding = embedding_layer(input_context)\n",
    "context_embedding = tf.keras.layers.Reshape((EMBEDDING_DIM, 1))(context_embedding)\n",
    "context_embedding = tf.keras.layers.Dropout(0.5)(context_embedding)\n",
    "\n",
    "hidden_layer = tf.keras.layers.Dot(axes=1, normalize=True)([target_embedding, context_embedding])\n",
    "hidden_layer = tf.keras.layers.Reshape((1,))(hidden_layer)\n",
    "\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_target, input_context], outputs=output)\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(clipnorm=0.0001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcbde71",
   "metadata": {},
   "source": [
    "Ya sólo nos queda entrenar el modelo. Para ello, lo entrenaremos con cada uno de los _skip-grams_ generados anteriormente. Usaremos una separación de validación del 10% y entrenaremos durante 10 epochs.\n",
    "\n",
    "**Este paso es muy costoso**, y se puede tirar bastantes minutos (horas) así que, o tenemos una máquina potente, o mejor lo dejamos aquí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf21a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([dataset[:, 0], dataset[:, 1]], dataset[:, 2], epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74812104",
   "metadata": {},
   "source": [
    "Veamos la evolución del entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961fc58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Training')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9742d5ba",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e7f8c8",
   "metadata": {},
   "source": [
    "Una vez entrenado el modelo, ya tenemos una matriz con los pesos de las características para cada palabra. Para ver una representación podemos cogerlos directamente e imprimirlos en un dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7527d055",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights = embedding_layer.get_weights()[0][1:]\n",
    "\n",
    "df = pd.DataFrame(weights, index=index_word.values())\n",
    "df.head(10).style.background_gradient(cmap ='hot').format('{:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf039e5",
   "metadata": {},
   "source": [
    "Vamos a hacer una búsqueda con las palabras más parecidas a una dada usando, por ejemplo, la distancia euclídea de sus vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10654a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLOSEST_WORDS = 10\n",
    "WORD = 'man'\n",
    "\n",
    "v1 = weights[word_index[WORD] - 1]\n",
    "words = sorted(\n",
    "    [word for word in word_index.keys()],\n",
    "    key=lambda w: np.linalg.norm(v1 - weights[word_index[w]-1])\n",
    ")\n",
    "df.loc[words[:NUM_CLOSEST_WORDS + 1], :].style.background_gradient(cmap ='hot').format('{:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad020787",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691f3670",
   "metadata": {},
   "source": [
    "En conclusión, hemos implementado un embedding utilizando la técnica _skip-grams_ de _word2vec_ y hemos demostrado su eficacia para representar palabras de manera más significativa en un espacio vectorial. Esta técnica es capaz de capturar la semántica de las palabras, representándolas en un espacio vectorial de dimensión menor a la que ocuparía con una representación _one-hot_.\n",
    "\n",
    "Ojo, también hemos comprobado que **es una técnica muy costosa**, y que por tanto no tiene mucho sentido (en general) ya que disponemos de muchos _embeddings_ ya preparados para descargar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc70b0a3",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
