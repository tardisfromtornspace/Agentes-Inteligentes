{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4480ff39",
   "metadata": {},
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Creando un chatbot simple<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Autor: Alberto Díaz Álvarez<br>Última actualización: 2023-03-28</small></i></div>\n",
    "                                                  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2489222",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17030ba",
   "metadata": {},
   "source": [
    "Los _chatbots_ son sistemas informáticos diseñados para simular una conversación humana y responder preguntas de manera automatizada. Estos sistemas han ganado popularidad en los últimos años debido a su capacidad para proporcionar asistencia al cliente las 24 horas del día, los 7 días de la semana, y su potencial para mejorar la eficiencia de los negocios al automatizar tareas rutinarias.\n",
    "\n",
    "Los chatbots se han vuelto especialmente útiles en la industria del servicio al cliente, donde pueden ayudar a las empresas a reducir los tiempos de espera y mejorar la satisfacción del cliente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d1e17",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b9e324",
   "metadata": {},
   "source": [
    "El objetivo de este proyecto es diseñar y desarrollar un chatbot simple siguiedo la arquitectura seq2seq. Este modelo, también llamado codificador-decodificador, utiliza la (corta) memoria a largo plazo que mantienen unidades como LSTM o GRU para generar una secuencia de salida a partir de una secuencia de entrada tras haber sido entrenado con un corpus suficientemente grande de secuencias origen y sus correspondientes secuencias de destino."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d401eac",
   "metadata": {},
   "source": [
    "## Imports y configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa528aec",
   "metadata": {},
   "source": [
    "A continuación importaremos las librerías que se usarán a lo largo del notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4fec89",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea42ab1",
   "metadata": {},
   "source": [
    "## Constantes previas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5588872c",
   "metadata": {},
   "source": [
    "Vamos a comenzar definiendo una serie de constantes que usaremos a lo largo del _notebook_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbd4a4d",
   "metadata": {},
   "source": [
    "Comenzamos por las etiquetas referentes a comienzo y fin de frase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c73011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_SOS = '#sos#'\n",
    "TOKEN_EOS = '#eos#'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8817fd4",
   "metadata": {},
   "source": [
    "Definimos también el número de características de salida del embedding, el máximo de longitud de secuencia y el tamaño de nuestro vocabulario de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985abe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "MAX_SEQUENCE_LEN = 20\n",
    "VOCABULARY_SIZE = 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83617f72",
   "metadata": {},
   "source": [
    "Por último, indicamos las \"unidades\" (neuronas) de salida de nuestra unidad recurrente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab803b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNITS = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7223928",
   "metadata": {},
   "source": [
    "## Carga y preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d3a3bc",
   "metadata": {},
   "source": [
    "Vamos a descargar una serie de pares de pregunta-respuesta para que nuestro _chatbot_ aprenda de ellas. Vamos a hacerlo en español, por cambiar un poco.\n",
    "\n",
    "Para ello descargaremos el repositorio [ChatterBot Language Training Corpus](https://github.com/gunthercox/chatterbot-corpus) de GitHub, el cual contiene varios _corpus_ en varios idiomas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07075092",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_URL = 'https://github.com/gunthercox/chatterbot-corpus/archive/master.zip'\n",
    "CORPUS_FILE = 'tmp/master.zip'\n",
    "\n",
    "# Creamos el directorio temporal si no existiese ya\n",
    "TMP_DIR = 'tmp/'\n",
    "if not os.path.isdir(TMP_DIR):\n",
    "    os.makedirs(TMP_DIR)\n",
    "\n",
    "# Descargamos todo el repositorio\n",
    "if not os.path.exists(CORPUS_FILE):\n",
    "    print('Downloading corpus ... ', end='')\n",
    "    with open(CORPUS_FILE, 'wb') as f:\n",
    "        r = requests.get(CORPUS_URL, allow_redirects=True)\n",
    "        f.write(r.content)\n",
    "    print('OK')\n",
    "\n",
    "# Lo descomprimimos en el directorio temporal\n",
    "print('Unpacking corpus ... ', end='')\n",
    "shutil.unpack_archive(CORPUS_FILE, TMP_DIR)\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbc803a",
   "metadata": {},
   "source": [
    "Con el dataset descargado, recogemos todas las conversaciones de la sección en español. La verdad es que las conversaciones no son muy para tirar cohetes, pero bueno, a ver qué sale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce93f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading ... ', end='')\n",
    "shutil.unpack_archive(CORPUS_FILE, TMP_DIR)\n",
    "chats = []\n",
    "for path in glob.glob(f'{TMP_DIR}chatterbot-corpus-master/chatterbot_corpus/data/spanish/*.yml'):\n",
    "    with open(path) as f:\n",
    "        chats.extend(c for c in yaml.safe_load(f).get('conversaciones', []))\n",
    "print(f'{len(chats)} conversations loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed780916",
   "metadata": {},
   "source": [
    "Bien, ya tenemos unas cuantas conversaciones que se componen de \"preguntas y \"respuestas\" (bueno, no tienen por qué ser técnicamente preguntas y respuestas, pero bueno, el toma y daca de una conversación).\n",
    "\n",
    "Ahora lo separaremos en tres conjuntos:\n",
    "\n",
    "- `questions`: El de preguntas, sin nada especial.\n",
    "- `answers`: El de respuestas, con el que alimentaremos la parte decodificadora de nuestra red, y que por tanto requerirá dos etiquetas al principio y al final para que denoten el comienzo y el fin de una frase\n",
    "- `answers_no_start`: El de respuestas con el que contrastaremos la respuesta dada por el decodificador, y que por tanto sólo requerirá una etiqueta al final para que denote el fin de una frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e15691",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "answers_no_start = []\n",
    "\n",
    "print('Processing conversations ... ', end='')\n",
    "for chat in chats:\n",
    "    for q, a in zip(chat[:-1], chat[1:]):\n",
    "        questions.append(q)\n",
    "        answers.append(f'{TOKEN_SOS} {a} {TOKEN_EOS}')\n",
    "        answers_no_start.append(f'{a} {TOKEN_EOS}')\n",
    "print(f'{len(questions)} answers and questions loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11437a5",
   "metadata": {},
   "source": [
    "No es mucho, pero bueno, es algo por lo que comenzar.\n",
    "\n",
    "Una vez tenemos nuestras frases, vamos a crear el componente que traducirá estas fsentencias a secuencias de tamaño fijo de enteros. Para esto usaremos una capa denominada `TextVectorization`, la cual hace la transformación automática (según la hayamos configurado, claro) de textos a secuencias de enteros, y que es directamente encajable en el _pipeline_ de un modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9415c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la capa de vectorización (frase -> secuencia de enteros)\n",
    "def standardize(inputs):\n",
    "    inputs = tf.strings.regex_replace(inputs, r'[!\"$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']', \"\")\n",
    "    return tf.strings.lower(inputs)\n",
    "\n",
    "vectorization_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCABULARY_SIZE,\n",
    "    output_sequence_length=MAX_SEQUENCE_LEN,\n",
    "    standardize=standardize,\n",
    "    name='vectorization',\n",
    ")\n",
    "\n",
    "# Lo inicializamos con el vocabulario de nuestro dataset. Aunque aquí\n",
    "# no es necesario, lo cargamos en batch para ilustrar cómo funciona.\n",
    "# Esto nos permitiría trabajar con datasets muy grandes.\n",
    "text_dataset = tf.data.Dataset.from_tensor_slices(questions + answers)\n",
    "vectorization_layer.adapt(text_dataset.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cead435",
   "metadata": {},
   "source": [
    "Y vamos a aprovechar que hemos creado nuestra capa de vectorización para transformar a vectores el conjunto de salida con el que compararemos la salida de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a920a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_no_start = vectorization_layer(answers_no_start).numpy()\n",
    "print(answers_no_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5159eccc",
   "metadata": {},
   "source": [
    "Por último, al igual que hemos creado la capa de vectorización, crearemos también la capa de _embedding_ que usaremos tanto en la parte codificadora como en la decodificadora de nuestro modelo seq2seq. Igual que la estamos creando, podemos hacer uso de un embedding ya existente como GLoVe (y lo mismo esto no lo leéis y lo he cambiado para tirar de una implementación de GLoVe en español)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc58914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    VOCABULARY_SIZE + 1, # El 0 está vetado, por eso es +1\n",
    "    EMBEDDING_DIM,\n",
    "    mask_zero=True,      # 0 es padding (no se debe usar en el vocabulario)\n",
    "    name='embedding',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42148f2f",
   "metadata": {},
   "source": [
    "## Creación del modelo\n",
    "\n",
    "Ya hemos visto en teoría que el esquema _seq2seq_ es un problema tipo _many-to-many_. De alguna manera tenemos que conseguir que nuestro modelo aprenda las relaciones existentes entre las palabras de nuestro texto junto con una codificación que tenga en cuenta estas relaciones y el contexto de las preguntas y respuestas.\n",
    "\n",
    "Lo que haremos será entrenar simultáneamente dos capas GRU. Concretamente entrenaremos la primera para las preguntas y luego utilizaremos sus pesos como estado inicial para entrenar la segunda para las respuestas.\n",
    "\n",
    "La primera capa GRU será el **codificador**, esto es, el procesamiento de la entrada para devolver una codificación de esta en su estado interno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf9e5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_gru = tf.keras.layers.GRU(UNITS, return_state=True, name='Encoder-GRU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d345c1c6",
   "metadata": {},
   "source": [
    "La entrada a esta capa será una secuencia ya vectorizada y convertida a vectores de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49dfd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string, name='Question')\n",
    "encoder_inputs_word_vectors = embedding_layer(vectorization_layer(encoder_inputs))\n",
    "\n",
    "encoder_output, encoder_state = encoder_gru(encoder_inputs_word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3526f073",
   "metadata": {},
   "source": [
    "La segunda capa GRU será la correspondiente al **decodificador**. Tomará el estado como contexto para predecir las siguientes palabras de la secuencia objetivo. Devolverá tantas salidas como elementos tenga la secuencia de entrada, básicamente para poder entrenala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2493ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_gru = tf.keras.layers.GRU(UNITS, return_state=True, return_sequences=True, name='Decoder-GRU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ecc4b2",
   "metadata": {},
   "source": [
    "El decodificador se conectará también a una secuencia de entrada (para hacer _teacher enforcing_) ya vectorizada y pasada a vectores de palabras, y a una capa densa de salida que hará la clasificación a la palabra más probable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5c7ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string, name='Answer')\n",
    "decoder_inputs_word_vectors = embedding_layer(vectorization_layer(decoder_inputs))\n",
    "\n",
    "decoder_output, _ = decoder_gru(decoder_inputs_word_vectors, initial_state=[encoder_state])\n",
    "decoder_dense = tf.keras.layers.Dense(VOCABULARY_SIZE, activation='softmax', name='Decoder-classifier')\n",
    "decoder_output = decoder_dense(decoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e1fdd",
   "metadata": {},
   "source": [
    "Hay que prestar especial atención a que la capa `Decoder-GRU` recibe como estado inicial el estado de la capa `Encoder-GRU`.\n",
    "\n",
    "Ahora ya podemos crear el modelo especificando entradas y salidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996f0920",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], [decoder_output])\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='sparse_categorical_crossentropy')\n",
    "model.summary()\n",
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7257c3aa",
   "metadata": {},
   "source": [
    "Dado que la salida de nuestro modelo (la salida del _decoder_) es un _softmax_ de 2500 palabras, usamos `sparse_categorical_crossentropy` para utilizar el índice de la palabra en lugar de pasar a una codificación _one-hot_ todas las labels.\n",
    "\n",
    "Ahora vamos a entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b35e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([np.array(questions), np.array(answers)], np.array(answers_no_start), epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02392841",
   "metadata": {},
   "source": [
    "Una vez entrenado el modelo general, extraeremos las capas entrenadas a dos componentes. Primero, el **_encoder_**, el más sencillo. Su entrada será una pregunta/frase y su salida será el estado interno de la neurona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca5c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = tf.keras.models.Model(encoder_inputs, encoder_state)\n",
    "tf.keras.utils.plot_model(encoder_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b159fd7d",
   "metadata": {},
   "source": [
    "Segundo, el **_decoder_**, que tomará dos entradas: el estado del _encoder_ (el espacio latente de la pregunta) y la palabra anterior (o el token _start of sequence_ si no la hay), con la que generaremos la nueva palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ea21ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_state = tf.keras.layers.Input(shape=(UNITS,), name='Input-state')\n",
    "decoder_output, decoder_state = decoder_gru(decoder_inputs_word_vectors, initial_state=[decoder_input_state])\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "decoder_model = tf.keras.models.Model(\n",
    "    [decoder_inputs, decoder_input_state],\n",
    "    [decoder_output, decoder_state],\n",
    ")\n",
    "tf.keras.utils.plot_model(decoder_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42499821",
   "metadata": {},
   "source": [
    "Ahora ya podemos hablar con el chatbot. El proceso es el siguiente:\n",
    "\n",
    "1. Tomamos una pregunta como entrada y predecimos los valores de estado utilizando el _encoder_.\n",
    "1. Establecemos los valores de estado del _encoder_ en el _decoder_.\n",
    "1. Establecemos el valor `TOKEN_SOS` en la entrada del decoder y obtenemos el valor de salida $O$.\n",
    "1. Mientras $O$ no sea el token `TOKEN_EOS` o lleguemos a la longitud máxima de secuencia\n",
    "    1. Añadimos el valor de salida $O$ a nuestra secuencia resultado\n",
    "    2. Reemplazamos la entrada del decoder por $O$ y obtenemos el nuevo $O$ de la salida del decoder\n",
    "\n",
    "Lo implementaremos en una clase que denominaremos FedericoTalker porque sí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae9ad2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedericoTalker:\n",
    "    def __init__(self, encoder, decoder):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def tell(self, message, max_len=None):\n",
    "        # Codificamos la pregunta\n",
    "        state = encoder_model.predict([message], verbose=0)\n",
    "        # Vamos generando palabras hasta que terminemos la frase\n",
    "        response = [TOKEN_SOS]\n",
    "        while response[-1] != TOKEN_EOS and len(response) < (max_len or np.inf):\n",
    "            output, state = decoder_model.predict([np.array([[response[-1]]]), state], verbose=0)\n",
    "            token = np.argmax(output[0,-1])\n",
    "            response.append(vectorization_layer.get_vocabulary()[token])\n",
    "        # Devolvemos la frase sin los tokens de comienzo y fin de frase\n",
    "        return ' '.join(response[1:-1])\n",
    "\n",
    "federico_talker = FedericoTalker(encoder_model, decoder_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc69b4",
   "metadata": {},
   "source": [
    "Ahora podemos crear un nuevo _chatbot_ con nuestros _encoder_ y _decoder_ y charlar con él."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f233ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "federico_talker.tell('Hola, ¿cómo te encuentras hoy?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568499b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "federico_talker.tell('Me apetece saberlo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946e6ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "federico_talker.tell('Creo que con tan poca conversación es difícil que me contestes a algo con sentido')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43585490",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7defd88",
   "metadata": {},
   "source": [
    "La implementación de un chatbot basado en _seq2seq_ en español es una tarea algo compleja, pero mola ver cómo es capaz de responder (aunque sea de manera rudimentaria) a las preguntas que le hacemos.\n",
    "\n",
    "Este tipo de modelo de redes neuronales recurrentes es capaz de generar respuestas relativamente coherentes y contextualizadas a partir de las preguntas que se le plantean. La clave para el éxito de la implementación radica en una adecuada selección y preparación de los datos de entrenamiento, así como en una arquitectura de red neuronal bien diseñada y ajustada. Además, el uso de técnicas como el modelo de atención puede mejorar significativamente la calidad de las respuestas generadas por el chatbot.\n",
    "\n",
    "En general, la implementación de un chatbot basado en _seq2seq_ en español es una muy buena forma de experimentar con técnicas avanzadas de procesamiento del lenguaje natural y de construir un modelo capaz de comunicarse de manera efectiva con los usuarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33005bde",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
