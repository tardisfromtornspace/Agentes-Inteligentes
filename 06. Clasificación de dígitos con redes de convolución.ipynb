{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "010176d1",
   "metadata": {},
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Clasificación de dígitos con redes de convolución<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Autor: Alberto Díaz Álvarez<br>Última actualización: 2023-03-05</small></i></div>\n",
    "                                                  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554fb61b",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "En este _notebook_ vamos a crear un clasificador para el problema del `mnist` usando para ello un modelo de red neuronal convolucional (CNN).\n",
    "\n",
    "Una CNN es una arquitectura un tanto diferente de red neuronal que, aunque mantiene su comportamiento _feed-forward_, aprovecha la estructura de los datos de entrada (normalmente imágenes, pero generalizando cualquier conjunto de datos donde los valores relacionados tienden a estar cerca) para reducir el tamaño del modelo y a la vez ofrecer un mejor desempeño."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f243ba3",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "\n",
    "El objetivo será la implementación de la red neuronal convolucional, la cual compararemos con el MLP que implementamos anteriormente para comparar ambos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67582fcc",
   "metadata": {},
   "source": [
    "## Bibliotecas y configuración\n",
    "\n",
    "A continuación importaremos las bibliotecas que se utilizarán a lo largo del _notebook_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58374f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e97df",
   "metadata": {},
   "source": [
    "Configuraremos también algunos parámetros para adecuar la presentación gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aca72f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (16, 9),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06541e93",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aec0b7",
   "metadata": {},
   "source": [
    "## Preparación de los datos.\n",
    "\n",
    "Vamos a ver un poco más en detalle los datos con los que vamos a trabajar. El conjunto `mnist` es un conjunto de imágenes pequeñas ($28 \\times 28$ píxeles) con dígitos del 0 al 9 manuscritos.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
    "\n",
    "Ya hemos visto que `keras` nos ofrece un módulo para trabajar con este _dataset_ directamente sin preocuparnos. Vamos a cargarlo y a verlo un poco más en detalle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88d8acf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (60000, 28, 28) input, (60000,) output\n",
      "Test shape:     (10000, 28, 28) input, (10000,) output\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(f'Training shape: {x_train.shape} input, {y_train.shape} output')\n",
    "print(f'Test shape:     {x_test.shape} input, {y_test.shape} output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7362f3",
   "metadata": {},
   "source": [
    "Vemos que el conjunto de datos se divide en un conjunto de datos de entrenamiento de 60000 ejemplos y un conjunto de test de 10000 ejemplos. El primero es con el que entrenaremos nuestro modelo mientras que el segundo servirá para evaluar el desempeño del modelo con datos que no ha visto nunca."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b552fecd",
   "metadata": {},
   "source": [
    "Veamos la forma que tienen los datos de entrada de un ejemplo en concreto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c8315fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   1  26 111 195 230\n",
      "   30   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  28 107 195 254 254 254 244\n",
      "   20   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  46 167 248 254 222 146 150 254 174\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  65 223 246 254 153  61  10   0  48 254 129\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  85 175 164  80   2   0   0   0  48 254 120\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 182 254  16\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 207 254  16\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 207 202   3\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  28 248 170   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 107 254  61   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 166 252  30   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 191 206   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 191 206   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  14 246 186   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  91 254  77   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 175 254  48   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 175 240  27   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 215 222   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 115 255 152   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 134 255  68   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "INDEX = 42\n",
    "\n",
    "print(x_train[INDEX])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a449a7",
   "metadata": {},
   "source": [
    "Cada uno de los ejemplos es una matrix de $28 \\times 28$ valores numéricos que van del 0 al 255. Esto es porque sus valores son bytes que representan la intensidad del blanco, del 0 (totalmente negro) al 255 (totalmente blanco).\n",
    "\n",
    "Vamos a representar a este número con la biblioteca matplotlib en un mapa de calor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e3f8131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1024x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[INDEX], cmap='hot');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ba6af6",
   "metadata": {},
   "source": [
    "Cuando trabajamos con redes neuronales, lo más común es trabajar con entradas que están normalizadas al intervalo $[0, 1]$. Vamos a aprovechar la potencia de `numpy` y el _bradcasting_ (así se le llama a aplicar una función a todos los elementos de un vector o matriz) para normalizar los valores. Si el valor máximo es 255, el proceso es tan sencillo como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29ca0d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760629b9",
   "metadata": {},
   "source": [
    "Acabamos de aplicar una división a ambos conjuntos de datos de entrada. Ahora, si mostramos el contenido de nuestro número (bueno, una porción) veremos que está normalizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11176cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.00392157 0.10196078 0.43529412 0.76470588 0.90196078\n",
      "  0.11764706 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.10980392\n",
      "  0.41960784 0.76470588 0.99607843 0.99607843 0.99607843 0.95686275\n",
      "  0.07843137 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.18039216 0.65490196 0.97254902\n",
      "  0.99607843 0.87058824 0.57254902 0.58823529 0.99607843 0.68235294\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[INDEX][5:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5a7d2f",
   "metadata": {},
   "source": [
    "Pero la relación entre los valores no habrá cambiado, así que si vbemos el mapa de calor veremos que las diferencias relativas entre valores se han mantenido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b413411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1024x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[INDEX], cmap='hot');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e1bca",
   "metadata": {},
   "source": [
    "Por cierto, hemos elegido un número bastante curioso ya que es complicado decidir si se trata de un 1 o un 7 (al menos a mí). Veamos Su valor de salida y continuemos desde ahí:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "059f9f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(y_train[INDEX])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1008de9",
   "metadata": {},
   "source": [
    "Ahora lo que vamos a tratar es un problema relacionado con la **clasificación multiclase**. En el momento que trabajamos con dos clases, tener una neurona de salida que nos discrimina entre los dos tiene sentido. Supongamos que estamos con un problema para identificar si una foto se corresponde o no a un gato. Si usamos una neurona de salida con activación sigmoidal, cuanto más se acerque al 0 o al 1, más se alejará del 1 o del 0, o dicho de otro modo, cuando más seguros estemos de que es un gato, más seguros estaremos de que no es un no-gato.\n",
    "\n",
    "Bueno, supongamos que queremos indicar si en una foto hay un gato, un perro o un oso. Con una neurona ¿qué hacemos? ¿Asignamos intervalos? Y recordemos que las redes funcionan con un proceso denominado descenso del gradiente que trabaja con pequeños incrementos de error. Al tener un rango de valores reales del 0 al 1, tenemos un orden entre estos valores. ¿Cómo colocamos los intervalos? ¿Es oso entre medias? ¿Es el oso una mezcla entre perro y gato? ¿Como vamos del perro al gato si el descenso del gradiente nos va a alejar del oso porque el error va a ser muy alto?\n",
    "\n",
    "Bueno, la solución es usar tantas neuronas como clases que queremos clasificar. Cada neurona se corresponderá con un es/no es algo y al final escogeremos la neurona que mayor activación tenga. Y para ello, entre las utilidades de Keras existe una herramienta que convierte los valores numéricos a categóricos, realizando una codificación one_hot. Funcionará ordenando los valores y convirtiendo cada uno de los diferentes valores a una posición del vector de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90129a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print(y_train[INDEX])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc333efb",
   "metadata": {},
   "source": [
    "Esta codificación nos convierte el valor numérico en un array ordenado donde se activa la neurona que se corresponde con la respuesta. De esta manera ya tenemos un valor de salida que podremos comparar con la salida de nuestra red, que será de 10 neuronas (10 números a clasificar. Y ahora sí, ya tenemos nuestros datos preparados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a36bf0",
   "metadata": {},
   "source": [
    "## Creación del modelo\n",
    "\n",
    "Vamos a crear una CNN sencilla, sin varias ramas independientes ni elementos raros, así que podemos usar el API secuencial sin problemas. Si nos acordamos, el modelo secuencial se crea como una lista de capas que se conectan una detrás de otra, pero hay otra forma que es llamando al método `add` de los objetos de la clase `Sequential`.\n",
    "\n",
    "Comenzaremos creando el modelo de la clase `Sequential`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c47797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb5f805",
   "metadata": {},
   "source": [
    "Ahora añadiremos una capa oculta. Si nos acordamos de teoría, las CNN se componen de dos partes, la parte de extracción de características, compuesta de capas de convolución, y las capas de inferencia, compuestas de capas densas. Comenzaremos con una capa de convolución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bedfcd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.add(tf.keras.layers.Conv2D(\n",
    "    input_shape=(28, 28, 1),\n",
    "    kernel_size=(3, 3),\n",
    "    filters=8,\n",
    "    activation='relu',\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb1fb40",
   "metadata": {},
   "source": [
    "Acabamos de crear una capa compuesta de 8 convoluciones de tamaño $3 \\times 3 \\times 1$ que navegará por toda la imagen extrayendo características.\n",
    "\n",
    "Otra capa que se suele combinar con las de convolución son las de muestreo parcial, _subsampling_ o _pooling_. Si recordamos, se trataba de filtros que reducían la imágen sacando los valores más destacables de cada área que cubre el filtro. En este caso, usaremos una capa `MaxPooling2D` que escogerá el valor más alto del área que cubre, con un _stride_ del tamaño del filtro para que no ahya solapamiento. Esto reducirá bastante el tamaño de la salida de la capa anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e48663de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.add(tf.keras.layers.MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    strides=(2, 2)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81aa407",
   "metadata": {},
   "source": [
    "De momento vamos a mantener una única capa de convolución y una de _pooling_, vamos a saltar a la parte de inferencia.\n",
    "\n",
    "Ahora bien, para saltar a la parte de inferencia (que no deja de ser un perceptrón multicapa) necesitamos que todos los datos de entrada estén en una dimensión. Por ello, antes de entrar en la capa de inferencia, vamos a \"aplanar\" la salida de la convolución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "546fee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151168d0",
   "metadata": {},
   "source": [
    "No necesitamos parámetros, lo único que hace es poner la matriz multidimensional en dos dimensiones, el número de ejemplos y todas las características extraídas para dicho ejemplo.\n",
    "\n",
    "Ahora vamos a añadir las capas densas que queramos. Vamos a añadir una oculta primero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c20342f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.add(tf.keras.layers.Dense(4, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3c6691",
   "metadata": {},
   "source": [
    "Y la de salida. Recordemos, la salida serán **10 neuronas**, una por cada posible clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6016acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495d6af7",
   "metadata": {},
   "source": [
    "La razon por la que añadimos una función de activación de tipo _softmax_ es debido a que:\n",
    "\n",
    "1. La clase que hay que seleccionar es exclusiva, es decir, un ejemplo no puede pertenecer a varias clases a la vez.\n",
    "2. _Softmax_ es derivable (de hecho, es la versión derivable de la función _argmax_), por lo que es posible aplicar el algoritmo del descenso del gradiente para entrenar la red.\n",
    "\n",
    "Y con esto ya tenemos definido nuestro modelo. Veamos su estructura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab74944c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_10 (Conv2D)          (None, 26, 26, 8)         80        \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 13, 13, 8)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 1352)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 4)                 5412      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                50        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,542\n",
      "Trainable params: 5,542\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dce57d",
   "metadata": {},
   "source": [
    "Vamos a compararlo con un perceptrón multicapa de un par de capas ocultas con aproximadamente el mismo número de parámetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d2d2b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_3 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 7)                 5495      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 8)                 64        \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 10)                90        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,649\n",
      "Trainable params: 5,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_mlp = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(7, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "model_mlp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5a4def",
   "metadata": {},
   "source": [
    "Ahora veamos qué tal se comportan en un par de entrenamientos. Para ello los entrenaremos con los mismos algoritmos y el mismo número de epochs. Empezamos con la CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99aa2d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1688/1688 [==============================] - 6s 3ms/step - loss: 1.4735 - categorical_accuracy: 0.4524 - val_loss: 0.9194 - val_categorical_accuracy: 0.6973\n",
      "Epoch 2/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.8692 - categorical_accuracy: 0.7215 - val_loss: 0.6505 - val_categorical_accuracy: 0.7980\n",
      "Epoch 3/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.6435 - categorical_accuracy: 0.8045 - val_loss: 0.5229 - val_categorical_accuracy: 0.8390\n",
      "Epoch 4/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.5241 - categorical_accuracy: 0.8447 - val_loss: 0.4562 - val_categorical_accuracy: 0.8608\n",
      "Epoch 5/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4417 - categorical_accuracy: 0.8714 - val_loss: 0.3852 - val_categorical_accuracy: 0.8845\n",
      "Epoch 6/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3874 - categorical_accuracy: 0.8868 - val_loss: 0.3440 - val_categorical_accuracy: 0.8983\n",
      "Epoch 7/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3515 - categorical_accuracy: 0.8975 - val_loss: 0.3012 - val_categorical_accuracy: 0.9148\n",
      "Epoch 8/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3208 - categorical_accuracy: 0.9059 - val_loss: 0.2738 - val_categorical_accuracy: 0.9247\n",
      "Epoch 9/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2989 - categorical_accuracy: 0.9121 - val_loss: 0.2491 - val_categorical_accuracy: 0.9327\n",
      "Epoch 10/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2783 - categorical_accuracy: 0.9184 - val_loss: 0.2366 - val_categorical_accuracy: 0.9352\n",
      "Epoch 11/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2623 - categorical_accuracy: 0.9234 - val_loss: 0.2534 - val_categorical_accuracy: 0.9273\n",
      "Epoch 12/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2481 - categorical_accuracy: 0.9276 - val_loss: 0.2091 - val_categorical_accuracy: 0.9427\n",
      "Epoch 13/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2360 - categorical_accuracy: 0.9310 - val_loss: 0.2084 - val_categorical_accuracy: 0.9418\n",
      "Epoch 14/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2253 - categorical_accuracy: 0.9345 - val_loss: 0.2008 - val_categorical_accuracy: 0.9472\n",
      "Epoch 15/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2162 - categorical_accuracy: 0.9361 - val_loss: 0.1887 - val_categorical_accuracy: 0.9518\n",
      "Epoch 16/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2069 - categorical_accuracy: 0.9402 - val_loss: 0.1759 - val_categorical_accuracy: 0.9525\n",
      "Epoch 17/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1996 - categorical_accuracy: 0.9416 - val_loss: 0.1761 - val_categorical_accuracy: 0.9538\n",
      "Epoch 18/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1924 - categorical_accuracy: 0.9430 - val_loss: 0.1671 - val_categorical_accuracy: 0.9565\n",
      "Epoch 19/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1858 - categorical_accuracy: 0.9450 - val_loss: 0.1645 - val_categorical_accuracy: 0.9567\n",
      "Epoch 20/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1796 - categorical_accuracy: 0.9469 - val_loss: 0.2043 - val_categorical_accuracy: 0.9420\n",
      "Epoch 21/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1749 - categorical_accuracy: 0.9479 - val_loss: 0.1567 - val_categorical_accuracy: 0.9568\n",
      "Epoch 22/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1700 - categorical_accuracy: 0.9494 - val_loss: 0.1537 - val_categorical_accuracy: 0.9590\n",
      "Epoch 23/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1660 - categorical_accuracy: 0.9508 - val_loss: 0.1600 - val_categorical_accuracy: 0.9577\n",
      "Epoch 24/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1613 - categorical_accuracy: 0.9527 - val_loss: 0.1479 - val_categorical_accuracy: 0.9603\n",
      "Epoch 25/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1576 - categorical_accuracy: 0.9533 - val_loss: 0.1500 - val_categorical_accuracy: 0.9600\n",
      "Epoch 26/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1548 - categorical_accuracy: 0.9536 - val_loss: 0.1437 - val_categorical_accuracy: 0.9610\n",
      "Epoch 27/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1506 - categorical_accuracy: 0.9549 - val_loss: 0.1436 - val_categorical_accuracy: 0.9620\n",
      "Epoch 28/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1482 - categorical_accuracy: 0.9568 - val_loss: 0.1386 - val_categorical_accuracy: 0.9638\n",
      "Epoch 29/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1456 - categorical_accuracy: 0.9564 - val_loss: 0.1397 - val_categorical_accuracy: 0.9628\n",
      "Epoch 30/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1426 - categorical_accuracy: 0.9573 - val_loss: 0.1480 - val_categorical_accuracy: 0.9602\n",
      "Epoch 31/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1406 - categorical_accuracy: 0.9592 - val_loss: 0.1433 - val_categorical_accuracy: 0.9613\n",
      "Epoch 32/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1381 - categorical_accuracy: 0.9592 - val_loss: 0.1374 - val_categorical_accuracy: 0.9655\n",
      "Epoch 33/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1361 - categorical_accuracy: 0.9592 - val_loss: 0.1400 - val_categorical_accuracy: 0.9655\n",
      "Epoch 34/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1339 - categorical_accuracy: 0.9603 - val_loss: 0.1350 - val_categorical_accuracy: 0.9642\n",
      "Epoch 35/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1317 - categorical_accuracy: 0.9614 - val_loss: 0.1340 - val_categorical_accuracy: 0.9637\n",
      "Epoch 36/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1308 - categorical_accuracy: 0.9610 - val_loss: 0.1401 - val_categorical_accuracy: 0.9632\n",
      "Epoch 37/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1288 - categorical_accuracy: 0.9621 - val_loss: 0.1310 - val_categorical_accuracy: 0.9648\n",
      "Epoch 38/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1278 - categorical_accuracy: 0.9628 - val_loss: 0.1366 - val_categorical_accuracy: 0.9625\n",
      "Epoch 39/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1258 - categorical_accuracy: 0.9633 - val_loss: 0.1397 - val_categorical_accuracy: 0.9628\n",
      "Epoch 40/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1250 - categorical_accuracy: 0.9638 - val_loss: 0.1299 - val_categorical_accuracy: 0.9645\n",
      "Epoch 41/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1237 - categorical_accuracy: 0.9638 - val_loss: 0.1297 - val_categorical_accuracy: 0.9642\n",
      "Epoch 42/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1225 - categorical_accuracy: 0.9633 - val_loss: 0.1345 - val_categorical_accuracy: 0.9637\n",
      "Epoch 43/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1206 - categorical_accuracy: 0.9644 - val_loss: 0.1443 - val_categorical_accuracy: 0.9613\n",
      "Epoch 44/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1196 - categorical_accuracy: 0.9646 - val_loss: 0.1254 - val_categorical_accuracy: 0.9653\n",
      "Epoch 45/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1184 - categorical_accuracy: 0.9651 - val_loss: 0.1237 - val_categorical_accuracy: 0.9665\n",
      "Epoch 46/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1177 - categorical_accuracy: 0.9656 - val_loss: 0.1433 - val_categorical_accuracy: 0.9615\n",
      "Epoch 47/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1163 - categorical_accuracy: 0.9661 - val_loss: 0.1251 - val_categorical_accuracy: 0.9662\n",
      "Epoch 48/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1158 - categorical_accuracy: 0.9660 - val_loss: 0.1422 - val_categorical_accuracy: 0.9613\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1152 - categorical_accuracy: 0.9663 - val_loss: 0.1309 - val_categorical_accuracy: 0.9647\n",
      "Epoch 50/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1144 - categorical_accuracy: 0.9663 - val_loss: 0.1235 - val_categorical_accuracy: 0.9647\n"
     ]
    }
   ],
   "source": [
    "model_cnn.compile(\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.SGD(),\n",
    "    metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    ")\n",
    "history_cnn = model_cnn.fit(x_train, y_train, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5c873a",
   "metadata": {},
   "source": [
    "Veamos la evolución de su error y su exactitud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history_cnn.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07184619",
   "metadata": {},
   "source": [
    "Ahora seguimos con el MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e6e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp.compile(\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.SGD(),\n",
    "    metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    ")\n",
    "history_mlp = model_mlp.fit(x_train, y_train, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d001e5",
   "metadata": {},
   "source": [
    "Y veamos también la evolución de su error y su exactitud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90813e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history_mlp.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b5baea",
   "metadata": {},
   "source": [
    "Parece que, aunque el MLP funciona bastante bien, la red no es capaz de llegar a tanta precisión como la CNN. Tiene sentido, ya que muchos de los parámetros se ocupan de relacionar píxeles que probablemente tengan muy poco que ver. Veamos qué tal se comportan con el conjunto de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6fee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_cnn, acc_cnn = model_cnn.evaluate(x_test, y_test)\n",
    "loss_mlp, acc_mlp = model_mlp.evaluate(x_test, y_test)\n",
    "\n",
    "print(f'Results for CNN: Loss={loss_cnn}, acc={acc_cnn}')\n",
    "print(f'Results for MLP: Loss={loss_mlp}, acc={acc_mlp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c2a3d7",
   "metadata": {},
   "source": [
    "Además hemos hecho una pequeña trampa. Nuestra red de convolución sólo tiene una capa, y esto provoca que el resultado sea muy grande. Este, al conectarlo con la capa densa hace que el número de parámetros crezca dramáticamente.\n",
    "\n",
    "Vamos a tratar de solucionar esto y a comparar con un nuevo perceptrón."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f3cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # Capa convolucional\n",
    "    tf.keras.layers.Conv2D(kernel_size=(3,3), filters=8, activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2, 2)),\n",
    "    # Capa convolucional\n",
    "    tf.keras.layers.Conv2D(kernel_size=(3,3), filters=8, activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2, 2)),\n",
    "    # Aplanamos características\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # Inferencia\n",
    "    tf.keras.layers.Dense(8, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.SGD(),\n",
    "    metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb3d79b",
   "metadata": {},
   "source": [
    "Casi la mitad de parámetros. Veamos su desempeño:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f803a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e897570",
   "metadata": {},
   "source": [
    "Gráficamente, la evolución queda como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6be095",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b18a03f",
   "metadata": {},
   "source": [
    "Parece un entrenamiento más estable y con mejores valores. Comparemos ahora los resultados de los tres modelos con el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d920bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f'Results for CNN 2.0: Loss={loss}, acc={acc}')\n",
    "print(f'Results for CNN:     Loss={loss_cnn}, acc={acc_mlp}')\n",
    "print(f'Results for MLP:     Loss={loss_mlp}, acc={acc_cnn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4379fe8a",
   "metadata": {},
   "source": [
    "El modelo generaliza mejor que el resto y además ocupa casi la mitad. Parece que la arquitectura de CNN se comporta bastante mejor para este tipo de problemas.\n",
    "\n",
    "Sin embargo, tiene un problema que es probable que hayamos identificado: La velocidad de entrenamiento. Después de todo, las operaciones de convolución son más lentas que un simple producto de matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5fe593",
   "metadata": {},
   "source": [
    "## Modelos LeNet y AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d93676",
   "metadata": {},
   "source": [
    "El modelo de red convolucional LeNet es una de las primeras redes neuronales que usaron opreaciones de convolución. Fue propuesta por Yann LeCun [1] en 1989 para el problema del reconocimiento (i.e. clasificación) de números manuscritos\n",
    "\n",
    "LeNet es el modelo en el que se inspira AlexNet [2], el modelo de que ganó la competición ImageNet en 2012 y que se considera como el origen de la vorágine del deep learning que nos ha traído hasta el momento actual.\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"Images/lenet-vs-alexnet.svg\" alt=\"Lenet vs AlexNet\" style=\"width:100%\" />\n",
    "        <figcaption align = \"center\"><strong>Figura 1</strong>. Diferencias entre arquitecturas LeNet y AlexNet. Autor: <a href=\"http://commons.wikimedia.org/wiki/User:Cmglee\" title=\"User:Cmglee\">Cmglee</a>, <a href=\"https://creativecommons.org/licenses/by-sa/4.0\" title=\"Creative Commons Attribution-Share Alike 4.0\">CC BY-SA 4.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=104937230\">Link</a></figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "Fijémonos en el año del primero. Usaron los algoritmos de retropropagación recién propuestos (1986, aunque es cierto que la técnica tiene predecesores) para el entrenamiento de los filtros de convolución. Por ello, sus autores se consideran pioneros en la clasificación de imágenes.\n",
    "\n",
    "Crearemos ahora estas arquitecturas para ver cómo se comportan con este conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c62f93",
   "metadata": {},
   "source": [
    "### Arquitectura LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263a2fb8",
   "metadata": {},
   "source": [
    "Existen muchas variantes diferentes. Nosotros implementaremos la mostrada en la Figura 1, pero no es la original (partiendo de que esta además toma como entrada imágenes de $32 \\times 32$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeb42ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lenet = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, 5, padding='same', activation='sigmoid', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.AveragePooling2D(2),\n",
    "    tf.keras.layers.Conv2D(16, 5, activation='sigmoid'),\n",
    "    tf.keras.layers.AveragePooling2D(2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(120, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(84, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "lenet.compile(\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.SGD(),\n",
    "    metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    ")\n",
    "lenet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1174b387",
   "metadata": {},
   "source": [
    "Esta arquitectura tiene bastantes más parámetros que las que hemos estado tratando hasta ahora. Veamos cómo progresa el entrenamiento durante 50 epochs como en los anteriores ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f861e9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_history = lenet.fit(x_train, y_train, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b409a9",
   "metadata": {},
   "source": [
    "Gráficamente, la evolución queda como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c9b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lenet_history.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30690699",
   "metadata": {},
   "source": [
    "### Arquitectura AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cddef3",
   "metadata": {},
   "source": [
    "Como hemos visto, la arquitectura AlexNet es una ampliación de la arquitectura LeNet, diseñada para la competición ImageNet que consistía en la clasificación de imágenes de $224 \\times 224 \\times 3$ en una entre mil clases.\n",
    "\n",
    "A diferencia que con LeNet, aquí implementaremos una versión un tanto diferente a la de la Figura 2:\n",
    "\n",
    "1. a última capa no será de 1000, sino de 10, ya que las clases en las que clasificar son 10 dígitos\n",
    "2. Reduciremos las dimensiones de los filtros de convolución y _pooling_ ya que la imagen de entrada no es lo suficientemente grande para soportarlos (se reducen por debajo de cero píxeles)\n",
    "3. También eliminaremos los _strides_ por el mismo motivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9443a6f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 26, 26, 96)        960       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 13, 13, 96)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 13, 13, 256)       221440    \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 6, 6, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 6, 6, 384)         885120    \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 6, 6, 384)         1327488   \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 6, 6, 256)         884992    \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 2, 2, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4096)              4198400   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                40970     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,340,682\n",
      "Trainable params: 24,340,682\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "alexnet = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(96, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(2),\n",
    "    tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2),\n",
    "    tf.keras.layers.Conv2D(384, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Conv2D(384, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(3),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4096, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(4096, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "alexnet.compile(\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.SGD(),\n",
    "    metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    ")\n",
    "alexnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5135145",
   "metadata": {},
   "source": [
    "Esta arquitectura tiene bastantes más parámetros que las que hemos estado tratando hasta ahora. Además incluye dos capas de _dropout_, en las que entraremos en detalle más adelante. Por ahora están ahí y sirven para reducir el overfitting durante el entrenamiento.\n",
    "\n",
    "Veamos cómo progresa el entrenamiento durante 50 epochs como en los anteriores ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157a4137",
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet_history = lenet.fit(x_train, y_train, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8f862d",
   "metadata": {},
   "source": [
    "Gráficamente, la evolución queda como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81414ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(alexnet_history.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3611aa0a",
   "metadata": {},
   "source": [
    "Veamos una comparativa entre todos los modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fb114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_loss, lenet_acc = lenet.evaluate(x_test, y_test)\n",
    "alexnet_loss, alexnet_acc = alexnet.evaluate(x_test, y_test)\n",
    "\n",
    "print(f'Results for AlexNet: Loss={alexnet_loss}, acc={alexnet_acc}')\n",
    "print(f'Results for LeNet:   Loss={lenet_loss}, acc={lenet_acc}')\n",
    "print(f'Results for CNN 2.0: Loss={loss}, acc={acc}')\n",
    "print(f'Results for CNN:     Loss={loss_cnn}, acc={acc_mlp}')\n",
    "print(f'Results for MLP:     Loss={loss_mlp}, acc={acc_cnn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcbf5bb",
   "metadata": {},
   "source": [
    "Quizá en este ejemplo no es apreciable la potencia de estos modelos respecto a los anteriores, pero lo cierto es que los superan su desempeño en varios órdenes de magnitud en problemas más complejos... y también en tiempo de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05096b5",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd65fcd4",
   "metadata": {},
   "source": [
    "Las redes convolucionales son una arquitectura de red neuronal que aprovecha las características de la entrada para aprender las relaciones existentes. Es equivalente a aproximar un problema de forma más inteligente que la fuerza bruta.\n",
    "\n",
    "Su principal desventaja es la velocidad de entrenamiento, pero superan con creces la capacidad de resolución de problemas cuando tratamos con elementos como imágenes.\n",
    "\n",
    "Más adelante veremos otros tipos de redes que solucionan ciertos problemas o que permiten que éstas sean todavía más grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582efec2",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
