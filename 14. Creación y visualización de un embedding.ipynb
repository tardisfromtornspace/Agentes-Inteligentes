{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SN5USFEIIK3"
   },
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Creación y visualización de un _embedding_<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Autor: Alberto Díaz Álvarez<br>Última actualización: 2023-03-14</small></i></div>\n",
    "                                                  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los _embeddings_ son una técnica utilizada para representar datos de manera más compacta y significativa. En particular, se utilizan para representar vectores de alta dimensionalidad (por ejemplo, representaciones de palabras o imágenes) en un espacio de menor dimensión (generalmente, de unas pocas decenas o cientos de dimensiones).\n",
    "\n",
    "Son muy útiles en aplicaciones de aprendizaje automático que involucran el procesamiento del lenguaje natural. Por ejemplo, se pueden utilizar para representar palabras en un espacio vectorial, de tal manera que palabras que tienen un significado similar se representen cerca una de la otra. Esto es útil en tareas como la traducción automática, donde se busca encontrar la palabra en el idioma de destino que tiene el significado más cercano a la palabra en el idioma de origen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6mJg1g3apaz"
   },
   "source": [
    "En este _notebook_ vamos a crear un Embedding y a proyectarlo en un espacio bidimensional para comprobar cómo pueden llegar a relacionarse palabras en un espacio vectorial en un contexto (problema) determinado.\n",
    "\n",
    "Entrenaremos este _embedding_ para una tarea de clasificación típicamente usada para empezar a aprender NLP, el del análisis de sentimiento de comentarios de IMDb. Sin embargo, nos quedaremos en el punto donde se entrena el _Embedding_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports y configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación importaremos las bibliotecas que usaremos a lo largo del _notebook_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asímismo, configuramos algunos parámetros para adecuar la presentación gráfica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RutaI-Tpev3T"
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (16, 9),'figure.dpi': 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBFctV8-JZOc"
   },
   "source": [
    "## Obteniendo el conjunto de datos de [IMDb](https://www.imdb.com/)\n",
    "\n",
    "Vamos a usar el _dataset_ de IMDb como fuente de datos. Afortunadamente, disponemos de él en el módulo `datasets` de Keras, así que podemos cargarlo de la misma manera que lo hemos hecho con el conjunto de datos `mnist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "aPO4_UmfF0KH",
    "outputId": "4c3c690f-0d09-4ea3-85e2-98507fb7ae5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los comentarios descargados son, en realidad, índices a palabras. El diccionario de `palabra->índice` se obtiene con la función `get_word_index`. Obtendremos el diccionario y haremos el diccionario inverso para poder hacer la decodificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1641221/1641221 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"the as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room titillate it so heart shows to years of every never going villaronga help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but pratfalls to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other tricky in of seen over landed for anyone of gilmore's br show's to whether from than out themselves history he name half some br of 'n odd was two most of mean for 1 any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "index_word = {index: word for (word, index) in word_index.items()}\n",
    "\n",
    "' '.join(index_word.get(i, '<UNK>') for i in x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No tiene mucho sentido, ¿verdad? Eso es porque este diccionario de vocabulario es un poco trampa, ya que no se corresponde exactamente con los datos del dataset:\n",
    "\n",
    "- El primer índice de todos es el 1, no el 0. El 0 no se usa.\n",
    "- Los índices están desplazados 3 enteros debido a que eso se refiere a palabras reservadas (el índice 1 a comienzo de frase, el 2 a ítem desconocido y el 3 a no usado\n",
    "\n",
    "Por tanto, vamos a rellenar correr tres posiciones todos los índices y a rellenar los valores por defecto. El 0 lo usaremos para indicar un padding (para rellenar las sentencias que no tengan la longitud fija requerida por los modelos. Por último, volveremos a generar el diccionario inverso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<SOS> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = {word: (index + 3) for word, index in word_index.items()} \n",
    "word_index['<PAD>'] = 0  # Para padding\n",
    "word_index['<SOS>'] = 1  # Para comienzo de sentencia\n",
    "word_index['<UNK>'] = 2  # Para valores desconocidos\n",
    "word_index['<UNU>'] = 3  # Para valores que no se usan\n",
    "\n",
    "index_word = {index: word for (word, index) in word_index.items()}\n",
    "\n",
    "' '.join(index_word.get(i, '<UNK>') for i in x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHa6cq0-Ym0g"
   },
   "source": [
    "Vamos a tratar el primer problema. Para obtener una entrada de longitud fija, podemos simplemente truncar las reseñas a un número fijo de palabras, digamos 256. Para las reseñas que tengan más de 256 palabras, mantendremos sólo las primeras 256 palabras. En el caso de las reseñas más cortas, rellenaremos los huecos de palabras no utilizados con el valor que le hemos asignado al padding (el 0). Con keras, esto es fácil de hacer usando la función `pad_sequences`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,    14,    22,    16,    43,   530,   973,  1622,  1385,\n",
       "          65,   458,  4468,    66,  3941,     4,   173,    36,   256,\n",
       "           5,    25,   100,    43,   838,   112,    50,   670, 22665,\n",
       "           9,    35,   480,   284,     5,   150,     4,   172,   112,\n",
       "         167, 21631,   336,   385,    39,     4,   172,  4536,  1111,\n",
       "          17,   546,    38,    13,   447,     4,   192,    50,    16,\n",
       "           6,   147,  2025,    19,    14,    22,     4,  1920,  4613,\n",
       "         469,     4,    22,    71,    87,    12,    16,    43,   530,\n",
       "          38,    76,    15,    13,  1247,     4,    22,    17,   515,\n",
       "          17,    12,    16,   626,    18, 19193,     5,    62,   386,\n",
       "          12,     8,   316,     8,   106,     5,     4,  2223,  5244,\n",
       "          16,   480,    66,  3785,    33,     4,   130,    12,    16,\n",
       "          38,   619,     5,    25,   124,    51,    36,   135,    48,\n",
       "          25,  1415,    33,     6,    22,    12,   215,    28,    77,\n",
       "          52,     5,    14,   407,    16,    82, 10311,     8,     4,\n",
       "         107,   117,  5952,    15,   256,     4, 31050,     7,  3766,\n",
       "           5,   723,    36,    71,    43,   530,   476,    26,   400,\n",
       "         317,    46,     7,     4, 12118,  1029,    13,   104,    88,\n",
       "           4,   381,    15,   297,    98,    32,  2071,    56,    26,\n",
       "         141,     6,   194,  7486,    18,     4,   226,    22,    21,\n",
       "         134,   476,    26,   480,     5,   144,    30,  5535,    18,\n",
       "          51,    36,    28,   224,    92,    25,   104,     4,   226,\n",
       "          65,    16,    38,  1334,    88,    12,    16,   283,     5,\n",
       "          16,  4472,   113,   103,    32,    15,    16,  5345,    19,\n",
       "         178,    32,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    x_train,\n",
    "    value=word_index['<PAD>'],\n",
    "    padding='post',\n",
    "    maxlen=256\n",
    ")\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    x_test,\n",
    "    value=word_index['<PAD>'],\n",
    "    padding='post',\n",
    "    maxlen=256\n",
    ")\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqBazMiVQkj1"
   },
   "source": [
    "## 2. Uso de la capa de Embedding\n",
    "\n",
    "Keras facilita **mucho** el trabajo con _Embeddings_ gracias a la capa `Embedding`, que puede ser entendida como una capa densa que mapea desde índices enteros (los índices de las palabras específicas) a vectores de palabras (su _embeddings_ correspondientes).\n",
    "\n",
    "Los `Embeddings` se crean con dos parámetros; el primero (`input_dim`), el número de palabras, indica a cuantas palabras hará caso nuestro embedding. Todas aquellas que no se encuentren en el embedding devolverán una representación para un token \"desconocido\". Cuanto mayor sea este parámetro, más palabras se podrán representar pero más ocupará en memoria y, sobre todo, más parámetros habrá que entrenar.\n",
    "\n",
    "El segundo parámetro (`output_dim`) es la dimensión. Este parámetro indica el número de cualidades que se guardarán de cada palabra del embedding. Este es el parámetro con el que más se juega cuando se está creando un embedding desde 0 para resolver un problema, de la misma manera que se experimenta con el número de neuronas en una capa `Dense`.\n",
    "\n",
    "Por ejemplo, creemos un _embedding_ con cualquier número de palabras y dimensión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-OjxLVrMvWUE"
   },
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=1000,\n",
    "    output_dim=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dKKV1L2Rk7e"
   },
   "source": [
    "Cuando se crea una capa de _Embedding_, sus pesos se inicializan aleatoriamente (como con cualquier otra capa). Durante el entrenamiento se irán ajustando gradualmente mediante retropropagación.\n",
    "\n",
    "Una vez entrenado, es de esperar que los vectores de palabras aprendidos codifiquen _a grandes rasgos_ similitudes entre palabras ya que, después de todo, fueron aprendidas para el problema específico en el que se entrena el modelo.\n",
    "\n",
    "Si se le pasa un entero (esto es, el índice que representa a una palabra) a una capa de _embedding_, esta lo sustituye por su vector de palabra correspondiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "0YUjPgP7w0PO",
    "outputId": "a8fb00c1-aab8-4040-d004-a826618a1ed0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03374875, -0.02414606,  0.00221163,  0.0269236 ,  0.03157217,\n",
       "        -0.01707293,  0.02286866, -0.01786113, -0.03947858, -0.04404721],\n",
       "       [-0.00796921,  0.00779479, -0.04838891,  0.0311293 , -0.01096175,\n",
       "         0.01412726,  0.00867057,  0.03504987,  0.0290009 , -0.01168092],\n",
       "       [ 0.01877454, -0.03021109,  0.00125221,  0.02936791, -0.00773361,\n",
       "        -0.01229348, -0.03614031,  0.01313135, -0.03694637, -0.00403061],\n",
       "       [-0.02397224, -0.03433316,  0.01638235, -0.02745819,  0.03880768,\n",
       "        -0.0497129 , -0.03244399, -0.02057414, -0.02674786, -0.02204483]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([1,2,4,8]))\n",
    "result.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4PC4QzsxTGx"
   },
   "source": [
    "Para los problemas donde tenemos un texto en lugar de una única palabra, podemos pasarle directamente un tensor 2D, en la forma `(num_secuencias, items_por_secuencia)`. El tensor devuelto tendrá la misma forma, pero con una dimensión más que será la de las dimensiones de las palabras.\n",
    "\n",
    "Por ejemplo, en el caso de darle una entrada de dos dimensiones con la forma `(2, 4)` (esto es, 2 secuencias de 4 elementos cada una), el tensor devuelto tendrá un eje más con la dimensión que hemos declarado en el _Embedding_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "vwSYepRjyRGy",
    "outputId": "ccc67727-3756-42b3-bd5e-f35fdc26234d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 4, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([\n",
    "    [0, 1, 2, 3],\n",
    "    [4, 5, 6, 7]\n",
    "]))\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGQp2N92yOyB"
   },
   "source": [
    "Podemos ver que si le damos un _batch_ de secuencias como entrada, una _Embedding_ devuelve un tensor 3D de tipo `float`, de la forma `(num_secuencias, items_por_secuencia, dim_embedding)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zI9_wLIiWO8Z"
   },
   "source": [
    "## 3. Creación y enternamiento del modelo de clasificación\n",
    "\n",
    "Usaremos un modelo secuencial formado por el _Embedding_ y una capa densa. Este modelo entrenará para intentar dar respuesta a qué comentarios son negativos y cuales positivos.\n",
    "\n",
    "Entre todos los pesos que se entrenen, estarán los del _Embedding_. Una vez finalize el entrenamiento, nuestro _Embedding_ tendrá un conocimiento de las relaciones que existen entre nuestras palabras, al menos dentro de nuestro contexto de las películas.\n",
    "\n",
    "El modelo que crearemos será una primera capa de _Embedding_ (con una dimensión de características de 2 para poder mostrarlas en una gráfica) y una capa de salida sigmoidal que determine si es una reseña buena o no. El resto de capas intermedias  como queráis. Luego, compilaremos el modelo de tal manera que se usará la función de pérdida correspondiente para un problema de clasificación (recordad, aquí es binaria), Adam como optimizador y además sacaremos la exactitud durante el entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pHLcFtn5Wsqj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 256, 2)            177176    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                32832     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 210,073\n",
      "Trainable params: 210,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "WORDS_IN_VOCAB = len(word_index)\n",
    "EMBEDDING_DIM  = 2\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=WORDS_IN_VOCAB,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        input_length=256,\n",
    "    ),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.8),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.8),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',  # Mejor para clasificación binaria que categorical_crossentropy\n",
    "    metrics=['binary_accuracy'],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCUgdP69Wzix"
   },
   "source": [
    "Y ahora entrenamos el modelo. Lo haremos durante 250 epochs con un conjunto de validación correspondiente al 10% del conjunto total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "id": "5mQehiQyv8rP",
    "outputId": "2b0a8bac-739f-489c-cdd3-50e4b73770ed"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_split=0.1, epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos la evolución del entrenamiento para comprobar que el modelo va aprendiendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training: {history.history[\"loss\"][-1]:.2f}, validation: {history.history[\"val_loss\"][-1]:.2f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['binary_accuracy'], label='Training')\n",
    "plt.plot(history.history['val_binary_accuracy'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(f'Training: {history.history[\"binary_accuracy\"][-1]:.2f}, validation: {history.history[\"val_binary_accuracy\"][-1]:.2f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wYnVedSPfmX"
   },
   "source": [
    "Con este enfoque, el modelo ha alcanzado una precisión de validación de alrededor del 90%, aunque parece que el modelo está ligeramente sobreajustado porque la precisión en entrenamiento es prácticamente el 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCoA6qwqP836"
   },
   "source": [
    "## 4. Visualización de nuestro _Embedding_\n",
    "\n",
    "Para ver representado el _Embedding_, vamos a crearnos primero una función que nos dará la salida de nuestra capa de _Embedding_ para una entrada concreta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "n-j0db7ipF6k",
    "outputId": "90e34c4e-bc23-4828-e496-1c354c2bf1c7"
   },
   "outputs": [],
   "source": [
    "f_embedding_out = tf.keras.backend.function(\n",
    "    inputs=[model.layers[0].input],\n",
    "    outputs=[model.layers[0].output],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos esta función para obtener la distribución de los vectores de palabras del conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = f_embedding_out(x_test[0])\n",
    "print(f'Output type: {type(output)}, len: {len(output)}, shape (element 0): {output[0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que, dada una secuencia, la salida `output` es una lista de un único tensor con la secuencia de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_words = output[0]\n",
    "plt.scatter(sentence_words[:,0], sentence_words[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos los vectores de los pesos asociados a determinadas palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.layers[0].get_weights()[0][:]\n",
    "df = pd.DataFrame(weights, index=index_word.values())\n",
    "df.loc[['cage','horrible','seagal'],:].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8MiCA77X8B8"
   },
   "source": [
    "Vamos a crear una reseña de película ficticia para ver cómo se distribuyen las palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "GsjempweP9Lq",
    "outputId": "a5ba6d7a-df50-4e02-d321-303c73f85cc5"
   },
   "outputs": [],
   "source": [
    "review = ['cage', 'reeves', 'seagal', 'cavill', 'good', 'fantastic', 'bad', 'crap', 'movie', 'scissors']\n",
    "encoded_review = tf.constant([word_index.get(w, word_index['<UNK>']) for w in review])\n",
    "encoded_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veamos la salida del _Embedding_ si le damos nuestra review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sacamos los vectores de palabra\n",
    "v_words = f_embedding_out(encoded_review)[0]\n",
    "\n",
    "# Los mostramos anotándolos con sus correspondientes palabras\n",
    "plt.scatter(v_words[:,0], v_words[:,1])\n",
    "for i, word in enumerate(review):\n",
    "    plt.annotate(word, (v_words[i, 0], v_words[i, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que en principio nuestro embedding ha localizado espacialmente las palabras que más relacionadas están. Vemos que Henry Cavill o Nicholas Cage son un poco del montón, mientras que Steven Seagal es garantía de una chufa de película. Todo esto, por supuesto, según nuestro embedding, que ha aprendido de las reseñas de los usuarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQyMZWyxYjMr"
   },
   "source": [
    "Hemos creado un _embedding_ a partir de ciertos datos y hemos demostrado cómo éste puede crear una representación en un espacio vectorial de las palabras incluidas en el mismo. Esta representación permite que las palabras que tienen significados similares se agrupen juntas en el espacio vectorial, lo que puede ser útil para tareas de análisis de texto como la clasificación de documentos o la búsqueda de información. También hemos demostrado cómo el embedding puede ser utilizado para realizar tareas como la identificación de palabras desconocidas y la detección de palabras mal escritas.\n",
    "\n",
    "En general, el uso de _embeddings_ para representar datos de texto es una técnica valiosa en el aprendizaje automático y el procesamiento del lenguaje natural. Permiten que las palabras sean representadas de manera más significativa en un espacio vectorial, lo que facilita la identificación de patrones y la realización de tareas de clasificación y predicción. Además, su capacidad de agrupar palabras similares juntas en el espacio vectorial puede mejorar significativamente la precisión y eficacia de los modelos de análisis de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "word_embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
