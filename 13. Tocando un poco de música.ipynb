{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Tocando un poco de música<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Autor: Alberto Díaz Álvarez<br>Última actualización: 2023-03-14</small></i></div>\n",
    "                                                  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducción\n",
    "\n",
    "En este notebook vamos a crear un modelo que aprenderá a tocar \"música\". Lo del entrecomillado es debido a que, para tocar bien música, se necesitan modelos muy complejos y con técnicas que se escapan un poco de un tutorial como este.\n",
    "\n",
    "Sin embargo, en este ejercicio tocaremos las bases de la generación en base a notas y acordes y, junto con los modelos que veremos más adelante en este tema más conceptos como redes bidireccionales y embeddings de la parte de Procesamiento de Lenguaje Natural (NLP) podremos generar música con un poco más de sentido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "\n",
    "Crearemos un modelo de predicción de notas en base a una secuencia de notas previas. Al finalizar habremos aprendido a:\n",
    "\n",
    "- Leer y escribir ficheros midi\n",
    "- Generar secuencias siguiendo una tipología de red recurrente de tipo _one-to-many_\n",
    "- Salvar modelos entrenados a disco para entrenar en diferentes momentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imports y configuración\n",
    "\n",
    "A continuación importaremos las librerías que se usarán a lo largo del notebook. Entre las que ya hemos visto durante el curso, importamos además:\n",
    "\n",
    "- `music21` librería [Music21](https://web.mit.edu/music21/) que permite la manipulación de ficheros midi (de audio en general) de una forma relativamente sencilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import music21\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asímismo, configuramos algunos parámetros para adecuar la presentación gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (20, 6),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga y preparación de datos\n",
    "\n",
    "Comenzaremos cargando todas las notas de los ficheros `.mid` localizados bajo el _path_ relativo `Datasets/Music`. Dichas notas las vamos a almacenar en una lista que llamaremos `notes`. Algunos detalles de implementación:\n",
    "\n",
    "* Para parsear un fichero midi usaremos la función `parse(file)` del módulo `converter` de la librería `music21`\n",
    "* Las notas que queremos obtener se encuentran en el atributo `.flat.notes` del midi parseado. Sin embargo hay que tener en cuenta que contiene dos tipos de datos:\n",
    "  - Notas normales, que son de tipo `note.Note`. Si la nota es `note`, almacenaremos directamente en la lista notes la representación como cadena de `note.pitch`.\n",
    "  - Acordes, que son de tipo `chord.Chord`. Estos son una lista de notas, y lo que almacenaremos será la lista de sus notas (si el acorde es `chord`, la lista será `chord.normalOrder`) como cadena te texto donde cada nota estará separada po un punto (`'.'`)\n",
    "\n",
    "Esto no es por capricho; es una forma de representar las notas que nos hará fácil más tarde convertir las notas generadas en una nueva pista de audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = []  # Almacenará todas las notas y acordes de los ficheros\n",
    "\n",
    "for file in glob.glob('Datasets/warcraft2/*.mid'):\n",
    "    print(f'Parsing {file}')\n",
    "\n",
    "    midi = music21.converter.parse(file)\n",
    "    \n",
    "    for note_or_chord in midi.flat.notes:\n",
    "        if isinstance(note_or_chord, music21.note.Note):\n",
    "            notes.append(str(note_or_chord.pitch))\n",
    "        elif isinstance(note_or_chord, music21.chord.Chord):\n",
    "            notes.append('.'.join(str(n) for n in note_or_chord.normalOrder))\n",
    "notes[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro siguiente paso será el de crear dos diccionarios: `note_to_int` y `int_to_note`. ¿Cuál será su comentido? Pues bien, tenemos la lista de todas las notas de todas las pistas de sonido. Como las redes trabajan con números, no con símbolos, lo que haremos será asignarle un valor único a cada nota diferente. Con estos dos diccionarios sabremos traducir de nota a su número y de un número a su nota para poder traducir hacia dentro y fuera de la red.\n",
    "\n",
    "Por tanto, crearemos estos dos diccionarios con las diferentes notas ordenadas ascendentemente, desde el 0 hasta el número de notas menos uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_to_int = {note: i for i, note in enumerate(sorted(set(notes)))}\n",
    "int_to_note = {i: note for i, note in enumerate(sorted(set(notes)))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguimos con la preparación de los conjuntos de datos. En `notes` tenemos la lista ordenada de notas. Es de esperar que las notas estén determinadas por la secuencia anterior. Por motivos de facilitar la implementación y la teoría, no hemos creado un nuevo token para indicar que una canción ha terminado, por lo que habrá ciertos cortes que no se correspondan con un ritmo real. Si te animas a modificarlo para que soporte esto, ¡adelante!\n",
    "\n",
    "Lo que crearemos ahora será el conjunto de entrenamiento, las variables `X_train` e `Y_train`. `X_train` se compondrá de las secuencias de entrada, que serán de longitud 50 (la variable SEQUENCE_LEN, ya está creada), mientras que `Y_train` tendrá la correspondiente nota que sigue a esa secuencia. Construiremos este conjunto a partir de la lista `notes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LEN = 20\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(SEQUENCE_LEN, len(notes)):\n",
    "    input_notes = notes[i - SEQUENCE_LEN: i]\n",
    "    output_note = notes[i]\n",
    "    x_train.append([note_to_int[note] for note in input_notes])\n",
    "    y_train.append(note_to_int[output_note])\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'... {x_train[i][-10:]} -> {y_train[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Como vamos a alimentar a una red recurrente, las dimensiones de entrada deberían ser $M \\times T \\times C$, siendo:\n",
    "\n",
    "* $M$: El número de ejemplos, es decir, el número total de secuencias\n",
    "* $T$: El tamaño de la secuencia, que lo hemos definido en la constante anterior\n",
    "* $C$: El número de parámetros que tiene cada elemento de la secuencia, que en nuestro caso es $1$ (cada nota es un único entero)\n",
    "\n",
    "El problema es que las dimensiones de nuestro conjunto de entrenamiento no son esas, así que habrá que hacer un reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (len(x_train), SEQUENCE_LEN, 1)) # (M, T, C)\n",
    "\n",
    "print(f'x_train shape: {x_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora normalizaremos los datos de entrada para que caigan dentro del intervalo $[0, 1]$, y convertiremos los datos de salida en formato _one-hot_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / len(note_to_int)\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "\n",
    "print(f'An x_train value: {x_train[0][0]}, Y_train shape: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ya están los conjuntos creados y listos para entrenar. Ahora pasamos a trabajar con el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Implementando y entrenando nuestro modelo\n",
    "\n",
    "Vamos a añadir un elemento nuevo que no habíamos visto hasta ahora pero que es de gran utilidad cuando trabajamos con modelos grandes: un _callback_ para ir guardando checkpoints con los estados del modelo. Los argumentos del inicializador de la clase son bastante descriptivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'tmp/model-{loss:.4f}.h5',\n",
    "    monitor='loss',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ahora crearemos un modelo para que aprenda a predecir la siguiente nota dada una secuencia de notas. La estructura que segurá será la siguiente:\n",
    "\n",
    "`Input`->`SimpleRNN-512`->`Dropout`->`SimpleRNN-256`->`Dropout`->`SimpleRnn-128`->`Dropout`->`SimpleRnn-64`->`Dropout`->`SimpleRnn-32`->`Dropout`->`Output`\n",
    "\n",
    "El modelo lo compilaremos con el loss correspondiente a un problema de clasificación y como optimizador usaremos el algoritmo Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PREVIOUS = True\n",
    "\n",
    "checkpoints = sorted(glob.glob('tmp/*.h5'))\n",
    "if LOAD_PREVIOUS and checkpoints:\n",
    "    print(f'Loading previous model: {checkpoints[0]}')\n",
    "    model = tf.keras.models.load_model(checkpoints[0])\n",
    "else:\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(512, return_sequences=True, input_shape=(*x_train.shape[1:],)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.LSTM(256, return_sequences=True),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.LSTM(32),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(y_train.shape[1], activation='softmax')\n",
    "    ])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora entrenaremos el modelo con nuestro conjunto de datos durante 10 epochs; no son muchos, pero la carga computacional derivada del entrenamiento de este tipo de modelos es bastante pesada. ¡Acuérdate de añadir el _callback_ para ir salvando el modelo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de ver la evolución del error, podríamos modificar la creación del modelo para que cargase el mejor checkpoint en caso de que existiese y de que quisiésemos (mediante una variable, por ejemplo `LOAD_PREVIOUS`).\n",
    "\n",
    "Ahora sí, veamos cómo han evolucionado el error y la precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['categorical_accuracy'], label='Train')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generemos un poco de música\n",
    "\n",
    "Ya tenemos un modelo entrenado para que nos genere música. Procederemos ahora a generar una canción. Para mantenerlo sencillo, generaremos una canción de N notas (por ejemplo 100, y veremos cómo se comporta. Para ello:\n",
    "\n",
    "1. Crearemos una secuencia aleatoria de arranque del tamaño de secuencia esperado. Esa será nuestra primera entrada\n",
    "2. Le pasaremos dicha secuencia al modelo y recogeremos la siguiente nota que prediga.\n",
    "3. Eliminaremos la primera nota de la secuencia y añadiremos la nueva nota al final; esta será la siguiente secuencia\n",
    "4. Seguiremos así hasta que hayamos terminado de generar notas\n",
    "5. El resultado será una lista con la secuencia y todas las notas generadas\n",
    "\n",
    "La lista con las notas generadas se llamará `new_song`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start   = np.random.randint(len(x_train) - 1)\n",
    "pattern = x_train[start]\n",
    "\n",
    "new_song = []\n",
    "for _ in range(100):\n",
    "    X = np.reshape(pattern, (1, SEQUENCE_LEN, 1))\n",
    "\n",
    "    next_note_softmax = model.predict(X)\n",
    "    next_note = np.argmax(next_note_softmax)\n",
    "\n",
    "    new_song.append(int_to_note[next_note])\n",
    "\n",
    "    pattern[:-1] = pattern[1:]\n",
    "    pattern[-1] = next_note / len(note_to_int)\n",
    "    \n",
    "print(f'New song: {new_song}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente fragmento de código transforma la lista de notas en un midi, separando cada nota medio segundo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create the midi given the song\n",
    "offset = 0\n",
    "output_notes = []\n",
    "\n",
    "# Create notes and chords according with the specified song\n",
    "for pattern in new_song:\n",
    "    if ('.' in pattern) or pattern.isdigit():\n",
    "        # Pattern is a chord, so let's split into notes and create it\n",
    "        notes = []\n",
    "        for current_note in pattern.split('.'):\n",
    "            new_note = music21.note.Note(int(current_note))\n",
    "            new_note.storedInstrument = music21.instrument.Violin()\n",
    "            notes.append(new_note)\n",
    "        new_chord = music21.chord.Chord(notes)\n",
    "        new_chord.offset = offset\n",
    "        output_notes.append(new_chord)\n",
    "    else:\n",
    "        # Pattern is a note, so let's create it and that's all\n",
    "        new_note = music21.note.Note(pattern)\n",
    "        new_note.offset = offset\n",
    "        new_note.storedInstrument = music21.instrument.Violin()\n",
    "        output_notes.append(new_note)\n",
    "\n",
    "    # Increase note ofet so no notes stack\n",
    "    offset += 0.5\n",
    "    \n",
    "midi_stream = music21.stream.Stream(output_notes)\n",
    "midi_stream.write('midi', fp='tmp/test_output.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podéis ver, el modelo ha generado una canción. Sí, no respetamos los tiempos, hay secuencias que no tiene sentido (los cortes entre canciones), etcétera, pero nos ha servido como experimento para ver el desarrollo de un proyecto de principio a fin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "Hemos implementado un modelo recurrente que aprende de muchos datos para resolver un problema _one_to_many_: generar música a partir de una semilla inicial.\n",
    "\n",
    "Os animo a que modifiquéis la arquitectura para ver si encontrais alguna que genere canciones que tengan algo de sentido, y que probéis a añadir una entrada aleatoria durante el entrenamiento para que durante la inferencia se pueda añadir dicha entrada para alterar la generación de melodías."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
