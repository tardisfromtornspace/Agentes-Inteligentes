{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Ejercicio de $Q$-learning. _Rock, paper, scissors, lizard, Spock_<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Autor: Alberto Díaz Álvarez<br>Última actualización: 2023-04-17</small></i></div>\n",
    "                                                  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En inteligencia artificial, el **aprendizaje por refuerzo** es una técnica de aprendizaje automático que permite que un agente aprenda a **tomar decisiones a través de la interacción con un entorno**. En este contexto, el algoritmo $Q$-learning es una técnica popular para el aprendizaje por refuerzo que permite que un agente aprenda a maximizar una recompensa a largo plazo al tomar decisiones óptimas en un entorno incierto.\n",
    "\n",
    "En esta práctica, se explorará el uso del algoritmo Q-learning para entrenar a un agente para que juegue al juego _rock, paper, scissors, lizard, spock_ a partir de información extraída de un jugardor con ciertos sesgos, cosa que le sucede generalmente a los humanos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a desarrollar un agente para que aprenda a jugar al juego de manera relativamente competente jugando al juego indicado usando para ello $Q$-learning.\n",
    "\n",
    "Primero desarrollaremos el algoritmo básico para aprender de unos datos extraídos de un jugador. Luego, se pedirá que se altere (si se desea) el agente para quu juegue contra los agentes del resto de los estudiantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imports y configuración\n",
    "\n",
    "A continuación importaremos las librerías que se usarán a lo largo del _notebook_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "import typing\n",
    "import urllib\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este juego es una extensión del clásico juego «piedra, papel o tijera» e incluye dos elementos adicionales, «lagarto» (_lizard_ en inglés) y «Spock» (extraterrestre de la raza Vulcana, conocido por su lógica y falta aparente de emociones). El objetivo del juego es seleccionar una de las cinco opciones posibles y ganar a la opción elegida por el oponente, siguiendo las siguientes reglas:\n",
    "\n",
    "- La piedra aplasta la tijera y aplasta al lagarto\n",
    "- La tijera corta el papel y decapita al lagarto\n",
    "- El papel envuelve la piedra y desautoriza a Spock\n",
    "- El lagarto envenena a Spock y come el papel\n",
    "- Spock rompe las tijeras y vaporiza la piedra\n",
    "\n",
    "Comenzamos definiendo las opciones posibles del juego, que se corresponderán con las acciones posibles que puede realizar un agente en el entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(enum.Enum):\n",
    "    \"\"\"Cada una de las posibles figuras.\"\"\"\n",
    "    ROCK = '🪨'\n",
    "    PAPER = '🧻'\n",
    "    SCISSORS = '✂️'\n",
    "    LIZARD = '🦎'\n",
    "    SPOCK = '🖖'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los movimientos junto con sus recompensas (si no nos hemos equivocado) se representan en el siguiente diccionario. Las recompensas se establecen en 1, 0, -1 dependiendo de si la primera opción gana a, empata con o pierde frente a la segunda, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVES_AND_REWARDS = {\n",
    "    (Action.ROCK, Action.ROCK): 0, (Action.ROCK, Action.PAPER): -1,\n",
    "    (Action.ROCK, Action.SCISSORS): 1, (Action.ROCK, Action.LIZARD): 1,\n",
    "    (Action.ROCK, Action.SPOCK): -1,\n",
    "    (Action.PAPER, Action.ROCK): 1, (Action.PAPER, Action.PAPER): 0,\n",
    "    (Action.PAPER, Action.SCISSORS): -1, (Action.PAPER, Action.LIZARD): -1,\n",
    "    (Action.PAPER, Action.SPOCK): 1,\n",
    "    (Action.SCISSORS, Action.ROCK): -1, (Action.SCISSORS, Action.PAPER): 1,\n",
    "    (Action.SCISSORS, Action.SCISSORS): 0, (Action.SCISSORS, Action.LIZARD): 1,\n",
    "    (Action.SCISSORS, Action.SPOCK): -1,\n",
    "    (Action.LIZARD, Action.ROCK): -1, (Action.LIZARD, Action.PAPER): 1,\n",
    "    (Action.LIZARD, Action.SCISSORS): -1, (Action.LIZARD, Action.LIZARD): 0,\n",
    "    (Action.LIZARD, Action.SPOCK): 1,\n",
    "    (Action.SPOCK, Action.ROCK): 1, (Action.SPOCK, Action.PAPER): -1,\n",
    "    (Action.SPOCK, Action.SCISSORS): 1, (Action.SPOCK, Action.LIZARD): -1,\n",
    "    (Action.SPOCK, Action.SPOCK): 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta información, crearemos el juego para que el agente (humano o máquina) pueda jugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✂️ is beaten by 🦎\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Game:\n",
    "    RENDER_MODE_HUMAN = 'human'\n",
    "    \n",
    "    def __init__(self, render_mode=None):\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "    def play(self, p1_action, p2_action):\n",
    "        result = MOVES_AND_REWARDS[(p1_action, p2_action)]\n",
    "        if self.render_mode == 'human':\n",
    "            self.render(p1_action, p2_action, result)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def render(p1_action, p2_action, result):\n",
    "        if result == 0:\n",
    "            print(f'{p1_action.value} tie!')\n",
    "        elif result == 1:\n",
    "            print(f'{p1_action.value} beats {p2_action.value}')\n",
    "        elif result == -1:\n",
    "            print(f'{p2_action.value} is beaten by {p1_action.value}')\n",
    "        else:\n",
    "            raise ValueError(f'{p1_action}, {p2_action}, {result}')\n",
    "\n",
    "game = Game(render_mode='human')\n",
    "game.play(np.random.choice(list(Action)), np.random.choice(list(Action)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente que aprende mediante $Q$-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear una clase auxiliar denominada `Transition` que va a guardar la información de una transición, definida por el estado origen, el estado destino, la acción por la que ocurrió dicha transición y la recompensa de la misma. De esta manera tendremos en un mismo objeto toda la información relacionada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(typing.NamedTuple):\n",
    "    \"\"\"Representa la transición de un estado al siguiente\"\"\"\n",
    "    prev_state: int              # Estado origen de la transición\n",
    "    next_state: int              # Estado destino de la transición\n",
    "    action: Action               # Acción que provocó esta transición\n",
    "    reward: typing.SupportsFloat # Recompensa obtenida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El estado lo representaremos como un entero ya que, en este problema en concreto, sólo tenemos un estado. Hemos preferido mantener aún así los atributos `prev_state` y `next_state` para que el código se corresponda con las fórmulas explicadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1. Definición del agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiremos el agente que aprenderá a jugar a partir de las jugadas de los demás. Debe implementar los métodos indicados para que cumplan la firma y el comentario.\n",
    "\n",
    "Se permite escribir código sólo entre el espacio delimitado por los comentarios `# START` y `# END`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START\n",
    "# Algigantix\n",
    "import random\n",
    "import collections\n",
    "\n",
    "espacioEstados = [0]\n",
    "# END\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name: str, q_table: typing.Any=None):\n",
    "        \"\"\"Inicializa este objeto.\n",
    "        \n",
    "        :param name: El nombre del agente, para identificarle.\n",
    "        :param q_table: Una tabla q de valores. Es opcional.\n",
    "        \"\"\"\n",
    "        # START\n",
    "        # END\n",
    "        self.name = name\n",
    "        if q_table:\n",
    "            self.q_table = q_table\n",
    "        else:\n",
    "            # START\n",
    "            # preguntar al profe si arriba no tendría que ser self.q_Table = q_Table?\n",
    "            espacioEstados = [0]\n",
    "            self.q_table = {}\n",
    "            for estado in espacioEstados:\n",
    "                self.q_table[estado] = {}\n",
    "                for acción in Action:    \n",
    "                    self.q_table[estado][acción] = 0\n",
    "             # preguntar al profe como hacer esto sin que sea con numpy array\n",
    "            \n",
    "            #np.zeroes(len(MOVES_AND_REWARDS), 1)  # tenemos varias acciones y según el profe solo un estado\n",
    "            # END\n",
    "        # START\n",
    "        self.current_state = None # Estado inicial\n",
    "        # END\n",
    "\n",
    "    def decide(self, state:int, 𝜀: typing.SupportsFloat=0) -> Action:\n",
    "        \"\"\"Decide la acción a ejecutar.\n",
    "        \n",
    "        :param state: El estado en el que nos encontramos.\n",
    "        :param 𝜀: Un valor entre 0 y 1 que representa, según la estrategia\n",
    "            ε-greedy, la probabilidad de que la acción sea elegida de manera\n",
    "            aleatoria de entre todas las opciones posibles. Es opcional, y si\n",
    "            no se especifica su valor es 0 (sin probabilidades de que se elija\n",
    "            una acción aleatoria).\n",
    "        :param returns: La acción a ejecutar.\n",
    "        \"\"\"\n",
    "        # START\n",
    "        self.current_state = state\n",
    "        #self.𝜀 = 𝜀\n",
    "        \n",
    "        if random.random() < 𝜀:\n",
    "            # Selección aleatoria\n",
    "            return np.random.choice(list(Action))\n",
    "        else:\n",
    "            # Selección voraz\n",
    "            accion, valor = self.auxBusquedaMax(state)\n",
    "            return accion\n",
    "        \n",
    "         #max(Actions, key=lambda a: self.action_values[a][state])\n",
    "    def auxBusquedaMax(self, state:int):\n",
    "        valorMasAlto = -np.inf\n",
    "        accionVorazMejor = None\n",
    "        state = 0\n",
    "        \n",
    "        for acción in self.q_table[state]:\n",
    "            if (self.q_table[state][acción] > valorMasAlto):\n",
    "                valorMasAlto = self.q_table[state][acción]\n",
    "                accionVorazMejor = acción\n",
    "\n",
    "        return accionVorazMejor, valorMasAlto\n",
    "\n",
    "        # END\n",
    "\n",
    "    def update(self, t: Transition, 𝛼=0.1, 𝛾=0.95):\n",
    "        \"\"\"Actualiza el estado interno de acuerdo a la experiencia vivida.\n",
    "        \n",
    "        :param transition: Toda la información correspondiente a la transición\n",
    "            de la que queremos aprender.\n",
    "        :param 𝛼: El factor de aprendizaje del cambio en el valor q. Por\n",
    "            defecto es 0.1\n",
    "        :param 𝛾: La influencia de la recompensa a largo plazo en el valor q a\n",
    "            actualizar. Va de 0 (sin influencia) a 1 (misma influencia que el\n",
    "            valor actual). Por defecto es 0.95.\n",
    "        \"\"\"\n",
    "        # START\n",
    "        self.t = t\n",
    "        self.𝛼 = 𝛼\n",
    "        self.𝛾 = 𝛾\n",
    "        \n",
    "        accion, maxaQstmas1ya = self.auxBusquedaMax(t.next_state)\n",
    "        \n",
    "        self.q_table[t.prev_state][t.action] = self.q_table[t.prev_state][t.action] + 𝛼 * (t.reward + 𝛾 * maxaQstmas1ya - self.q_table[t.prev_state][t.action]) \n",
    "        self.state = t.next_state\n",
    "        self.reward = t.reward\n",
    "        # END\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Representación textual de la tabla Q del agente.\n",
    "        \n",
    "        :returns: Una cadena indicando la estructura interna de la tabla Q.\n",
    "        \"\"\"\n",
    "        # START\n",
    "        miString = \"(\"\n",
    "        for state in self.q_table:\n",
    "            miString = miString+\"\\nESTADO \"+str(state)+\"[\"\n",
    "            for action in self.q_table[state]:\n",
    "                miString = miString+\"   \"+str(action)+\"->:\"+str(self.q_table[state][action])\n",
    "            miString = miString+\"]\"\n",
    "        miString = miString+\"\\n)\"\n",
    "        return miString\n",
    "        # END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el entrenamiento usaremos el dataset localizado en <https://blazaid.github.io/Aprendizaje-profundo/Datasets/rock-paper-scissors-lizard-spock.trn>, el cual contiene una secuencia de opciones que ha tomado un jugador en una partida ficticia de este juego. Comenzamos cargando el fichero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'https://blazaid.github.io/Aprendizaje-profundo/Datasets/rock-paper-scissors-lizard-spock.trn'\n",
    "\n",
    "player2_actions = []\n",
    "with urllib.request.urlopen(dataset_url) as f:\n",
    "    for line in f:\n",
    "        move = line.decode('utf-8').strip().upper()\n",
    "        if move:\n",
    "            player2_actions.append(Action[move])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el dataset descargado, ya podemos realizar el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "𝜀 = 1\n",
    "𝛿𝜀 = 𝛆 / len(player2_actions)\n",
    "\n",
    "game = Game()\n",
    "agent = Agent(name='Agent')\n",
    "\n",
    "state = 0  # El entorno (juego) no tiene estado, así que siempre será el mismo\n",
    "for p2_action in player2_actions:\n",
    "    p1_action = agent.decide(state, 𝛆)\n",
    "    reward = game.play(p1_action, p2_action)\n",
    "\n",
    "    # Actualizamos el agente\n",
    "    agent.update(Transition(\n",
    "        prev_state=state,\n",
    "        next_state=state,\n",
    "        action=p1_action,\n",
    "        reward=reward\n",
    "    ))\n",
    "\n",
    "    # Actualizamos 𝜀\n",
    "    𝜀 -= 𝛿𝜀 if 𝜀 > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras el entrenamiento, podemos ver cómo están de repartidos los valores de la tabla $Q$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n",
      "ESTADO 0[   Action.ROCK->:3.675285288011116   Action.PAPER->:3.2872813062235595   Action.SCISSORS->:3.635954232143073   Action.LIZARD->:3.614512996145612   Action.SPOCK->:4.050960896685394]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qué tal se comporta el con un conjunto de datos que nunca ha visto del mismo contrincante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wins': 1115, 'loses': 679, 'ties': 206}\n"
     ]
    }
   ],
   "source": [
    "dataset_url = 'https://blazaid.github.io/Aprendizaje-profundo/Datasets/rock-paper-scissors-lizard-spock.tst'\n",
    "\n",
    "player2_actions = []\n",
    "with urllib.request.urlopen(dataset_url) as f:\n",
    "    for line in f:\n",
    "        move = line.decode('utf-8').strip().upper()\n",
    "        if move:\n",
    "            player2_actions.append(Action[move])\n",
    "\n",
    "stats = collections.defaultdict(int)\n",
    "state = 0\n",
    "for p2_action in player2_actions:\n",
    "    p1_action = agent.decide(state)\n",
    "    reward = game.play(p1_action, p2_action)\n",
    "    if reward == 1:\n",
    "        stats['wins'] += 1\n",
    "    elif reward == -1:\n",
    "        stats['loses'] += 1\n",
    "    else:\n",
    "        stats['ties'] += 1\n",
    "\n",
    "print(dict(stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo visto ha aprendido un poco del sesgo que tenía el contrincante. ¡Bien por nuestro agente! Ya está un poco más cerca de dominar el mundo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2. Nuevo agente para competición"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer las cosas un poco más interesantes, vamos a hacer una pequeña competición. Se trata de desarrollar un agente como el que acabamos de hacer, pero donde los contrincantes serán el resto de agentes de los grupos. Se permite cualquier implementación, siempre y cuando:\n",
    "\n",
    "- Herede de la clase `Agent` suministrada.\n",
    "- El método `__init__` no admita ningún parámetro adicional.\n",
    "\n",
    "En la siguiente celda se ofrece el esqueleto del agente que competirá. Como se puede ver:\n",
    "\n",
    "- Tiene un nombre\n",
    "- A la hora de decidir la acción recibirá un estado, que siempre será 0 y devolverá la opción a realizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class Agent(metaclass=abc.ABCMeta):\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def __init__(self, name: str):\n",
    "        \"\"\"Inicializa el objeto.\n",
    "        \n",
    "        :param name: El nombre del agente.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        # START\n",
    "        # END\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def decide(self, state:int) -> Action:\n",
    "        \"\"\"Decide la acción a llevar a cabo dado el estado actual.\n",
    "        \n",
    "        :param state: El estado en el que se encuentra el agente.\n",
    "        :returns: La acción a llevar a cabo.\n",
    "        \"\"\"\n",
    "        # START\n",
    "        # END\n",
    "    \n",
    "    def update(self, transition: Transition):\n",
    "        \"\"\"Actualiza (si es necesario) el estado interno del agente.\n",
    "        \n",
    "        :param transition: La información de la transición efectuada.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, un par de posibles implementaciones (que no usan nada de $q$-learning, y que no se recomiendan) son las siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Botnifacio(Agent):\n",
    "    def __init__(self):\n",
    "        super().__init__(name='Botnifacio')\n",
    "    \n",
    "    def decide(self, state:int) -> Action:\n",
    "        return np.random.choice(list(Action))\n",
    "\n",
    "\n",
    "class Gustabot(Agent):\n",
    "    def __init__(self):\n",
    "        super().__init__(name='Gustabot')\n",
    "        self.weights = np.random.random(5)\n",
    "        self.weights /= sum(self.weights)\n",
    "    \n",
    "    def decide(self, state:int) -> Action:\n",
    "        return np.random.choice(list(Action), p=self.weights)\n",
    "    \n",
    "    def update(self, transition: Transition):\n",
    "        self.weights = np.random.random(5)\n",
    "        self.weights /= sum(self.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El agente se deberá desarrollar en la siguiente celda, la cual incluirá todo lo necesario para instanciarlo y que funcione. Se recomienda la competición entre grupos antes de la entrega final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementar aquí al agente\n",
    "\n",
    "# Nosotros hemos decidido realizar un agente Q-learning con ciertos pesos ponderados según las guías comunes de\n",
    "# piedra-papel-tijeras, de forma que se desvíe un poco del comportamiento de mantener la jugada ganadora de la \n",
    "# última ronda y cambiar a la forma ganadora de la última ronda cuando se pierde; lo que introduce un comportamiento\n",
    "# ligeramente no determinista.\n",
    "import random\n",
    "import collections\n",
    "\n",
    "class Agente(Agent):\n",
    "    def __init__(self, name: str):\n",
    "        \"\"\"Inicializa este objeto.\n",
    "        \n",
    "        :param name: El nombre del agente, para identificarle.\n",
    "        :param q_table: Una tabla q de valores. Es opcional.\n",
    "        \"\"\"\n",
    "        # START\n",
    "        super().__init__(name)\n",
    "\n",
    "        espacioEstados = [0]\n",
    "        self.q_table = {}\n",
    "        for estado in espacioEstados:\n",
    "            self.q_table[estado] = {}\n",
    "            for acción in Action:    \n",
    "                self.q_table[estado][acción] = 0\n",
    "\n",
    "        self.current_state = None # Estado inicial\n",
    "        self.𝜀 = 1.0\n",
    "        self.𝛿𝜀 = 0.0005\n",
    "        self.𝛼=0.1\n",
    "        self.𝛾=0.95\n",
    "\n",
    "        self.t = [None, None, None]\n",
    "        self.count = 0\n",
    "        \n",
    "        self.q_tableSesgo = {}\n",
    "        for estado in espacioEstados:\n",
    "            self.q_tableSesgo[estado] = {}\n",
    "            for acción in Action:    \n",
    "                self.q_tableSesgo[estado][acción] = 0.0\n",
    "                \n",
    "        # END\n",
    "\n",
    "    def decide(self, state:int) -> Action:\n",
    "        \"\"\"Decide la acción a ejecutar.\n",
    "        \n",
    "        :param state: El estado en el que nos encontramos.\n",
    "        :param 𝜀: Un valor entre 0 y 1 que representa, según la estrategia\n",
    "            ε-greedy, la probabilidad de que la acción sea elegida de manera\n",
    "            aleatoria de entre todas las opciones posibles. Es opcional, y si\n",
    "            no se especifica su valor es 0 (sin probabilidades de que se elija\n",
    "            una acción aleatoria).\n",
    "        :param returns: La acción a ejecutar.\n",
    "        \"\"\"\n",
    "        # START\n",
    "        self.current_state = state\n",
    "        #print(self.𝜀)\n",
    "        if random.random() < self.𝜀:\n",
    "            # Selección aleatoria\n",
    "            return np.random.choice(list(Action))\n",
    "        else:\n",
    "            # Selección voraz\n",
    "            accion, valor = self.auxBusquedaMax(state)\n",
    "            return accion\n",
    "        \n",
    "    def auxBusquedaMax(self, state:int):\n",
    "        valorMasAlto = -np.inf\n",
    "        accionVorazMejor = None\n",
    "        state = 0\n",
    "        \n",
    "        for acción in Action:\n",
    "                self.q_tableSesgo[state][acción] = 0.0\n",
    "                \n",
    "        ganeUltima = 0\n",
    "        if self.t[self.count] != None:\n",
    "            if (self.t[self.count].reward == -1): # si perdimos, sabemos que el ganador tratará de repetir, -asi que hacemos más importantes las config que lo vencen\n",
    "                MOVES_AND_REWARDS[(p1_action, p2_action)]\n",
    "                accionGano1 = None\n",
    "                accionGano2 = None\n",
    "\n",
    "                cuentaGane = 0\n",
    "                for acción in Action:\n",
    "                    if MOVES_AND_REWARDS[(self.t[self.count].action, acción)] == 1:\n",
    "                        if cuentaGane == 0:\n",
    "                            accionGano1 = acción\n",
    "                            cuentaGane += 1\n",
    "                        else:\n",
    "                            accionGano2 = acción\n",
    "            \n",
    "                if MOVES_AND_REWARDS[(accionGano1, accionGano2)] == 1:\n",
    "                    self.q_tableSesgo[state][accionGano2] += 0.2\n",
    "                    self.q_tableSesgo[state][accionGano1] += 0.09\n",
    "                else:\n",
    "                    self.q_tableSesgo[state][accionGano2] += 0.09\n",
    "                    self.q_tableSesgo[state][accionGano1] += 0.2\n",
    "            elif (self.t[self.count].reward == 1): # si ganamos, la gente tiende a quedarse con ese valor, pero nosotros no lo haremos\n",
    "                self.q_tableSesgo[state][self.t[self.count].action] -= 0.26\n",
    "            else: # en situación de empate\n",
    "                self.q_tableSesgo[state][self.t[self.count].action] += 0.06\n",
    "        \n",
    "        if self.t[self.count] != None and self.t[self.count-1] != None:\n",
    "            if (self.t[self.count].action == self.t[self.count-1].action): # si las dos últimas son iguales, es muy probable que se cambie a cualquier otra, pero eso lo sabemos así que nos quedamos acá\n",
    "                self.q_tableSesgo[state][self.t[self.count].action] += 0.26\n",
    "            if self.t[self.count-2] != None:\n",
    "                if ((self.t[self.count].action == self.t[self.count-1].action) and (self.t[self.count].action == self.t[self.count-2].action) and (self.t[self.count].reward == -1) and (self.t[self.count-1].reward == -1) and (self.t[self.count-2].reward == -1)): # cambia\n",
    "                    self.q_tableSesgo[state][self.t[self.count].action] -= 0.36\n",
    "        \n",
    "        for acción in self.q_table[state]:\n",
    "            #print(f'el self más el sesgo: {self.q_table[state][acción]} + {self.q_tableSesgo[state][acción]} = {self.q_table[state][acción] + self.q_tableSesgo[state][acción]}')\n",
    "            if ((self.q_table[state][acción] + self.q_tableSesgo[state][acción]) > valorMasAlto):\n",
    "                valorMasAlto = self.q_table[state][acción] + self.q_tableSesgo[state][acción]\n",
    "                accionVorazMejor = acción\n",
    "                \n",
    "        #for acción in self.q_table[state]:\n",
    "        #    if ((self.q_table[state][acción]) > valorMasAlto):\n",
    "        #        valorMasAlto = self.q_table[state][acción]\n",
    "        #        accionVorazMejor = acción\n",
    "\n",
    "        return accionVorazMejor, valorMasAlto\n",
    "\n",
    "        # END\n",
    "\n",
    "    def update(self, transition: Transition):\n",
    "        \"\"\"Actualiza el estado interno de acuerdo a la experiencia vivida.\n",
    "        \n",
    "        :param transition: Toda la información correspondiente a la transición\n",
    "            de la que queremos aprender.\n",
    "        :param 𝛼: El factor de aprendizaje del cambio en el valor q. Por\n",
    "            defecto es 0.1\n",
    "        :param 𝛾: La influencia de la recompensa a largo plazo en el valor q a\n",
    "            actualizar. Va de 0 (sin influencia) a 1 (misma influencia que el\n",
    "            valor actual). Por defecto es 0.95.\n",
    "        \"\"\"\n",
    "        # START\n",
    "        \n",
    "        accion, maxaQstmas1ya = self.auxBusquedaMax(transition.next_state)\n",
    "        \n",
    "        \n",
    "        self.q_table[transition.prev_state][transition.action] = self.q_table[transition.prev_state][transition.action] + self.𝛼 * (transition.reward + self.𝛾 * maxaQstmas1ya - self.q_table[transition.prev_state][transition.action]) \n",
    "        self.state = transition.next_state\n",
    "        self.reward = transition.reward\n",
    "        \n",
    "        self.𝜀 -= 𝛿𝜀 if 𝜀 > 0 else 0\n",
    "        #self.𝛼 -= 𝛿𝜀 if 𝜀 > 0 else 0\n",
    "        #self.𝛾 -= self.𝛿𝜀 if self.𝜀 > 0 else 0\n",
    "        \n",
    "        self.t[self.count] = transition\n",
    "        self.count = (self.count +1)%3\n",
    "        # END\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Representación textual de la tabla Q del agente.\n",
    "        \n",
    "        :returns: Una cadena indicando la estructura interna de la tabla Q.\n",
    "        \"\"\"\n",
    "        # START\n",
    "        miString = self.name+\" (\"\n",
    "        for state in self.q_table:\n",
    "            miString = miString+\"\\nESTADO \"+str(state)+\"[\"\n",
    "            for action in self.q_table[state]:\n",
    "                miString = miString+\"   \"+str(action)+\"->:\"+str(self.q_table[state][action])\n",
    "            miString = miString+\"]\"\n",
    "        miString = miString+\"\\n)\"\n",
    "        return miString\n",
    "        # END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wins': 1115, 'loses': 679, 'ties': 206}\n"
     ]
    }
   ],
   "source": [
    "# ESTE DE ACÁ ABAJO ES DE NUESTRO TEST\n",
    "\n",
    "dataset_url = 'https://blazaid.github.io/Aprendizaje-profundo/Datasets/rock-paper-scissors-lizard-spock.trn'\n",
    "\n",
    "player2_actions = []\n",
    "with urllib.request.urlopen(dataset_url) as f:\n",
    "    for line in f:\n",
    "        move = line.decode('utf-8').strip().upper()\n",
    "        if move:\n",
    "            player2_actions.append(Action[move])\n",
    "\n",
    "𝜀 = 1\n",
    "𝛿𝜀 = 𝛆 / len(player2_actions)\n",
    "\n",
    "game = Game()\n",
    "agent = Agente(name='Agentillo')\n",
    "\n",
    "state = 0  # El entorno (juego) no tiene estado, así que siempre será el mismo\n",
    "for p2_action in player2_actions:\n",
    "    p1_action = agent.decide(state)\n",
    "    reward = game.play(p1_action, p2_action)\n",
    "\n",
    "    # Actualizamos el agente\n",
    "    agent.update(Transition(\n",
    "        prev_state=state,\n",
    "        next_state=state,\n",
    "        action=p1_action,\n",
    "        reward=reward\n",
    "    ))\n",
    "\n",
    "    # Actualizamos 𝜀\n",
    "    #𝜀 -= 𝛿𝜀 if 𝜀 > 0 else 0\n",
    "\n",
    "dataset_url = 'https://blazaid.github.io/Aprendizaje-profundo/Datasets/rock-paper-scissors-lizard-spock.tst'\n",
    "\n",
    "player2_actions = []\n",
    "with urllib.request.urlopen(dataset_url) as f:\n",
    "    for line in f:\n",
    "        move = line.decode('utf-8').strip().upper()\n",
    "        if move:\n",
    "            player2_actions.append(Action[move])\n",
    "\n",
    "stats = collections.defaultdict(int)\n",
    "state = 0\n",
    "for p2_action in player2_actions:\n",
    "    p1_action = agent.decide(state)\n",
    "    reward = game.play(p1_action, p2_action)\n",
    "    if reward == 1:\n",
    "        stats['wins'] += 1\n",
    "    elif reward == -1:\n",
    "        stats['loses'] += 1\n",
    "    else:\n",
    "        stats['ties'] += 1\n",
    "\n",
    "print(dict(stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La competición será una liga entre los agentes de cada grupo a 100 partidas.\n",
    "\n",
    "Antes de dichas 100 partidas, se hará un previo de 10000 partidas con el fin de que cada uno de los agentes \"aprenda\" a competir contra el contrario. Sólo en esta fase se invocará el método `update`.\n",
    "\n",
    "La competición se realizará como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algigantix (\n",
      "ESTADO 0[   Action.ROCK->:2.8130993301867893   Action.PAPER->:2.5693836850365197   Action.SCISSORS->:2.74173809948628   Action.LIZARD->:2.378499657726694   Action.SPOCK->:2.734158734046112]\n",
      "): -2, Botnifacio: 2\n",
      "Algigantix (\n",
      "ESTADO 0[   Action.ROCK->:3.0340537817737823   Action.PAPER->:1.5207878486047812   Action.SCISSORS->:1.6485676516637606   Action.LIZARD->:1.5471872077200537   Action.SPOCK->:1.5660009957392542]\n",
      "): 5, Gustabot: -5\n",
      "Botnifacio: 0, Gustabot: 0\n",
      "LEADERBOARD\n",
      "3         \tAlgigantix (\n",
      "ESTADO 0[   Action.ROCK->:3.0340537817737823   Action.PAPER->:1.5207878486047812   Action.SCISSORS->:1.6485676516637606   Action.LIZARD->:1.5471872077200537   Action.SPOCK->:1.5660009957392542]\n",
      ")\n",
      "2         \tBotnifacio\n",
      "-5        \tGustabot\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "FRIENDLY_SETS = 10000\n",
    "COMPETITION_SETS = 10\n",
    "\n",
    "nombreNuestro = \"Algigantix\"\n",
    "\n",
    "competitors = [\n",
    "    Gustabot(),\n",
    "    Botnifacio(),\n",
    "    Agente(nombreNuestro),\n",
    "]\n",
    "\n",
    "leaderboard = {c: 0 for c in sorted(competitors, key=lambda x: x.__str__())}\n",
    "game = Game()\n",
    "for p1, p2 in itertools.combinations(leaderboard.keys(), 2):\n",
    "    # Amistoso\n",
    "    s = 0\n",
    "    for i in range(FRIENDLY_SETS):\n",
    "        a1 = p1.decide(s)\n",
    "        a2 = p2.decide(s)\n",
    "        reward = game.play(a1, a2)\n",
    "        p1.update(Transition(prev_state=s, next_state=s, action=a1, reward=reward))\n",
    "        p2.update(Transition(prev_state=s, next_state=s, action=a2, reward=-reward))\n",
    "    \n",
    "    # Competición\n",
    "    s = 0\n",
    "    r1 = r2 = 0\n",
    "    for i in range(COMPETITION_SETS):\n",
    "        a1 = p1.decide(s)\n",
    "        a2 = p2.decide(s)\n",
    "        reward = game.play(a1, a2)\n",
    "        r1 += reward\n",
    "        r2 -= reward\n",
    "\n",
    "    # Actualización de marcadores globales\n",
    "    leaderboard[p1] += r1\n",
    "    leaderboard[p2] += r2\n",
    "    \n",
    "    print(f'{p1}: {r1}, {p2}: {r2}')\n",
    "    \n",
    "\n",
    "print('LEADERBOARD')\n",
    "for c, r in sorted(leaderboard.items(), key=lambda t: t[1], reverse=True):\n",
    "    print(f'{r:<10}\\t{c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La calificación de cada grupo irá en función de la puntuación final que haya conseguido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumiendo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos desarrollado un agente para un juego diferente que aprender a tomar decisiones óptimas en cada situación, a partir de la retroalimentación proporcionada por el entorno del juego. Hemos observado cómo el agente ha ido mejorando gradualmente su desempeño en el juego, a medida que ha acumulado experiencia y ha ajustado sus valores de Q.\n",
    "\n",
    "Además, hemos podido comprobar cómo el agente es capaz de adaptarse a diferentes estrategias de juego de su oponente, lo que demuestra su capacidad para generalizar y tomar decisiones en situaciones nuevas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
