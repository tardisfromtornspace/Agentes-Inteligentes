{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Ejercicio de $Q$-learning. _Rock, paper, scissors, lizard, Spock_<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Autor: Alberto D铆az lvarez<br>ltima actualizaci贸n: 2023-04-17</small></i></div>\n",
    "                                                  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducci贸n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En inteligencia artificial, el **aprendizaje por refuerzo** es una t茅cnica de aprendizaje autom谩tico que permite que un agente aprenda a **tomar decisiones a trav茅s de la interacci贸n con un entorno**. En este contexto, el algoritmo $Q$-learning es una t茅cnica popular para el aprendizaje por refuerzo que permite que un agente aprenda a maximizar una recompensa a largo plazo al tomar decisiones 贸ptimas en un entorno incierto.\n",
    "\n",
    "En esta pr谩ctica, se explorar谩 el uso del algoritmo Q-learning para entrenar a un agente para que juegue al juego _rock, paper, scissors, lizard, spock_ a partir de informaci贸n extra铆da de un jugardor con ciertos sesgos, cosa que le sucede generalmente a los humanos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a desarrollar un agente para que aprenda a jugar al juego de manera relativamente competente jugando al juego indicado usando para ello $Q$-learning.\n",
    "\n",
    "Primero desarrollaremos el algoritmo b谩sico para aprender de unos datos extra铆dos de un jugador. Luego, se pedir谩 que se altere (si se desea) el agente para quu juegue contra los agentes del resto de los estudiantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imports y configuraci贸n\n",
    "\n",
    "A continuaci贸n importaremos las librer铆as que se usar谩n a lo largo del _notebook_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "import typing\n",
    "import urllib\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripci贸n del problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este juego es una extensi贸n del cl谩sico juego 芦piedra, papel o tijera禄 e incluye dos elementos adicionales, 芦lagarto禄 (_lizard_ en ingl茅s) y 芦Spock禄 (extraterrestre de la raza Vulcana, conocido por su l贸gica y falta aparente de emociones). El objetivo del juego es seleccionar una de las cinco opciones posibles y ganar a la opci贸n elegida por el oponente, siguiendo las siguientes reglas:\n",
    "\n",
    "- La piedra aplasta la tijera y aplasta al lagarto\n",
    "- La tijera corta el papel y decapita al lagarto\n",
    "- El papel envuelve la piedra y desautoriza a Spock\n",
    "- El lagarto envenena a Spock y come el papel\n",
    "- Spock rompe las tijeras y vaporiza la piedra\n",
    "\n",
    "Comenzamos definiendo las opciones posibles del juego, que se corresponder谩n con las acciones posibles que puede realizar un agente en el entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(enum.Enum):\n",
    "    \"\"\"Cada una de las posibles figuras.\"\"\"\n",
    "    ROCK = ''\n",
    "    PAPER = 'Щ'\n",
    "    SCISSORS = '锔'\n",
    "    LIZARD = ''\n",
    "    SPOCK = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los movimientos junto con sus recompensas (si no nos hemos equivocado) se representan en el siguiente diccionario. Las recompensas se establecen en 1, 0, -1 dependiendo de si la primera opci贸n gana a, empata con o pierde frente a la segunda, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVES_AND_REWARDS = {\n",
    "    (Action.ROCK, Action.ROCK): 0, (Action.ROCK, Action.PAPER): -1,\n",
    "    (Action.ROCK, Action.SCISSORS): 1, (Action.ROCK, Action.LIZARD): 1,\n",
    "    (Action.ROCK, Action.SPOCK): -1,\n",
    "    (Action.PAPER, Action.ROCK): 1, (Action.PAPER, Action.PAPER): 0,\n",
    "    (Action.PAPER, Action.SCISSORS): -1, (Action.PAPER, Action.LIZARD): -1,\n",
    "    (Action.PAPER, Action.SPOCK): 1,\n",
    "    (Action.SCISSORS, Action.ROCK): -1, (Action.SCISSORS, Action.PAPER): 1,\n",
    "    (Action.SCISSORS, Action.SCISSORS): 0, (Action.SCISSORS, Action.LIZARD): 1,\n",
    "    (Action.SCISSORS, Action.SPOCK): -1,\n",
    "    (Action.LIZARD, Action.ROCK): -1, (Action.LIZARD, Action.PAPER): 1,\n",
    "    (Action.LIZARD, Action.SCISSORS): -1, (Action.LIZARD, Action.LIZARD): 0,\n",
    "    (Action.LIZARD, Action.SPOCK): 1,\n",
    "    (Action.SPOCK, Action.ROCK): 1, (Action.SPOCK, Action.PAPER): -1,\n",
    "    (Action.SPOCK, Action.SCISSORS): 1, (Action.SPOCK, Action.LIZARD): -1,\n",
    "    (Action.SPOCK, Action.SPOCK): 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta informaci贸n, crearemos el juego para que el agente (humano o m谩quina) pueda jugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "锔 is beaten by \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Game:\n",
    "    RENDER_MODE_HUMAN = 'human'\n",
    "    \n",
    "    def __init__(self, render_mode=None):\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "    def play(self, p1_action, p2_action):\n",
    "        result = MOVES_AND_REWARDS[(p1_action, p2_action)]\n",
    "        if self.render_mode == 'human':\n",
    "            self.render(p1_action, p2_action, result)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def render(p1_action, p2_action, result):\n",
    "        if result == 0:\n",
    "            print(f'{p1_action.value} tie!')\n",
    "        elif result == 1:\n",
    "            print(f'{p1_action.value} beats {p2_action.value}')\n",
    "        elif result == -1:\n",
    "            print(f'{p2_action.value} is beaten by {p1_action.value}')\n",
    "        else:\n",
    "            raise ValueError(f'{p1_action}, {p2_action}, {result}')\n",
    "\n",
    "game = Game(render_mode='human')\n",
    "game.play(np.random.choice(list(Action)), np.random.choice(list(Action)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente que aprende mediante $Q$-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear una clase auxiliar denominada `Transition` que va a guardar la informaci贸n de una transici贸n, definida por el estado origen, el estado destino, la acci贸n por la que ocurri贸 dicha transici贸n y la recompensa de la misma. De esta manera tendremos en un mismo objeto toda la informaci贸n relacionada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(typing.NamedTuple):\n",
    "    \"\"\"Representa la transici贸n de un estado al siguiente\"\"\"\n",
    "    prev_state: int              # Estado origen de la transici贸n\n",
    "    next_state: int              # Estado destino de la transici贸n\n",
    "    action: Action               # Acci贸n que provoc贸 esta transici贸n\n",
    "    reward: typing.SupportsFloat # Recompensa obtenida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El estado lo representaremos como un entero ya que, en este problema en concreto, s贸lo tenemos un estado. Hemos preferido mantener a煤n as铆 los atributos `prev_state` y `next_state` para que el c贸digo se corresponda con las f贸rmulas explicadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1. Definici贸n del agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiremos el agente que aprender谩 a jugar a partir de las jugadas de los dem谩s. Debe implementar los m茅todos indicados para que cumplan la firma y el comentario.\n",
    "\n",
    "Se permite escribir c贸digo s贸lo entre el espacio delimitado por los comentarios `# START` y `# END`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START\n",
    "# Algigantix\n",
    "import random\n",
    "import collections\n",
    "\n",
    "espacioEstados = [0]\n",
    "# END\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name: str, q_table: typing.Any=None):\n",
    "        \"\"\"Inicializa este objeto.\n",
    "        \n",
    "        :param name: El nombre del agente, para identificarle.\n",
    "        :param q_table: Una tabla q de valores. Es opcional.\n",
    "        \"\"\"\n",
    "        # START\n",
    "        # END\n",
    "        self.name = name\n",
    "        if q_table:\n",
    "            self.q_table = q_table\n",
    "        else:\n",
    "            # START\n",
    "            # preguntar al profe si arriba no tendr铆a que ser self.q_Table = q_Table?\n",
    "            espacioEstados = [0]\n",
    "            self.q_table = {}\n",
    "            for estado in espacioEstados:\n",
    "                self.q_table[estado] = {}\n",
    "                for acci贸n in Action:    \n",
    "                    self.q_table[estado][acci贸n] = 0\n",
    "             # preguntar al profe como hacer esto sin que sea con numpy array\n",
    "            \n",
    "            #np.zeroes(len(MOVES_AND_REWARDS), 1)  # tenemos varias acciones y seg煤n el profe solo un estado\n",
    "            # END\n",
    "        # START\n",
    "        self.current_state = None # Estado inicial\n",
    "        # END\n",
    "\n",
    "    def decide(self, state:int, : typing.SupportsFloat=0) -> Action:\n",
    "        \"\"\"Decide la acci贸n a ejecutar.\n",
    "        \n",
    "        :param state: El estado en el que nos encontramos.\n",
    "        :param : Un valor entre 0 y 1 que representa, seg煤n la estrategia\n",
    "            蔚-greedy, la probabilidad de que la acci贸n sea elegida de manera\n",
    "            aleatoria de entre todas las opciones posibles. Es opcional, y si\n",
    "            no se especifica su valor es 0 (sin probabilidades de que se elija\n",
    "            una acci贸n aleatoria).\n",
    "        :param returns: La acci贸n a ejecutar.\n",
    "        \"\"\"\n",
    "        # START\n",
    "        self.current_state = state\n",
    "        #self. = \n",
    "        \n",
    "        if random.random() < :\n",
    "            # Selecci贸n aleatoria\n",
    "            return np.random.choice(list(Action))\n",
    "        else:\n",
    "            # Selecci贸n voraz\n",
    "            accion, valor = self.auxBusquedaMax(state)\n",
    "            return accion\n",
    "        \n",
    "         #max(Actions, key=lambda a: self.action_values[a][state])\n",
    "    def auxBusquedaMax(self, state:int):\n",
    "        valorMasAlto = -np.inf\n",
    "        accionVorazMejor = None\n",
    "        state = 0\n",
    "        \n",
    "        for acci贸n in self.q_table[state]:\n",
    "            if (self.q_table[state][acci贸n] > valorMasAlto):\n",
    "                valorMasAlto = self.q_table[state][acci贸n]\n",
    "                accionVorazMejor = acci贸n\n",
    "\n",
    "        return accionVorazMejor, valorMasAlto\n",
    "\n",
    "        # END\n",
    "\n",
    "    def update(self, t: Transition, =0.1, =0.95):\n",
    "        \"\"\"Actualiza el estado interno de acuerdo a la experiencia vivida.\n",
    "        \n",
    "        :param transition: Toda la informaci贸n correspondiente a la transici贸n\n",
    "            de la que queremos aprender.\n",
    "        :param : El factor de aprendizaje del cambio en el valor q. Por\n",
    "            defecto es 0.1\n",
    "        :param : La influencia de la recompensa a largo plazo en el valor q a\n",
    "            actualizar. Va de 0 (sin influencia) a 1 (misma influencia que el\n",
    "            valor actual). Por defecto es 0.95.\n",
    "        \"\"\"\n",
    "        # START\n",
    "        self.t = t\n",
    "        self. = \n",
    "        self. = \n",
    "        \n",
    "        accion, maxaQstmas1ya = self.auxBusquedaMax(t.next_state)\n",
    "        \n",
    "        self.q_table[t.prev_state][t.action] = self.q_table[t.prev_state][t.action] +  * (t.reward +  * maxaQstmas1ya - self.q_table[t.prev_state][t.action]) \n",
    "        self.state = t.next_state\n",
    "        self.reward = t.reward\n",
    "        # END\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Representaci贸n textual de la tabla Q del agente.\n",
    "        \n",
    "        :returns: Una cadena indicando la estructura interna de la tabla Q.\n",
    "        \"\"\"\n",
    "        # START\n",
    "        miString = \"(\"\n",
    "        for state in self.q_table:\n",
    "            miString = miString+\"\\nESTADO \"+str(state)+\"[\"\n",
    "            for action in self.q_table[state]:\n",
    "                miString = miString+\"   \"+str(action)+\"->:\"+str(self.q_table[state][action])\n",
    "            miString = miString+\"]\"\n",
    "        miString = miString+\"\\n)\"\n",
    "        return miString\n",
    "        # END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el entrenamiento usaremos el dataset localizado en <https://blazaid.github.io/Aprendizaje-profundo/Datasets/rock-paper-scissors-lizard-spock.trn>, el cual contiene una secuencia de opciones que ha tomado un jugador en una partida ficticia de este juego. Comenzamos cargando el fichero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'https://blazaid.github.io/Aprendizaje-profundo/Datasets/rock-paper-scissors-lizard-spock.trn'\n",
    "\n",
    "player2_actions = []\n",
    "with urllib.request.urlopen(dataset_url) as f:\n",
    "    for line in f:\n",
    "        move = line.decode('utf-8').strip().upper()\n",
    "        if move:\n",
    "            player2_actions.append(Action[move])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el dataset descargado, ya podemos realizar el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " = 1\n",
    "筐 =  / len(player2_actions)\n",
    "\n",
    "game = Game()\n",
    "agent = Agent(name='Agent')\n",
    "\n",
    "state = 0  # El entorno (juego) no tiene estado, as铆 que siempre ser谩 el mismo\n",
    "for p2_action in player2_actions:\n",
    "    p1_action = agent.decide(state, )\n",
    "    reward = game.play(p1_action, p2_action)\n",
    "\n",
    "    # Actualizamos el agente\n",
    "    agent.update(Transition(\n",
    "        prev_state=state,\n",
    "        next_state=state,\n",
    "        action=p1_action,\n",
    "        reward=reward\n",
    "    ))\n",
    "\n",
    "    # Actualizamos \n",
    "     -= 筐 if  > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras el entrenamiento, podemos ver c贸mo est谩n de repartidos los valores de la tabla $Q$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n",
      "ESTADO 0[   Action.ROCK->:3.675285288011116   Action.PAPER->:3.2872813062235595   Action.SCISSORS->:3.635954232143073   Action.LIZARD->:3.614512996145612   Action.SPOCK->:4.050960896685394]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qu茅 tal se comporta el con un conjunto de datos que nunca ha visto del mismo contrincante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wins': 1115, 'loses': 679, 'ties': 206}\n"
     ]
    }
   ],
   "source": [
    "dataset_url = 'https://blazaid.github.io/Aprendizaje-profundo/Datasets/rock-paper-scissors-lizard-spock.tst'\n",
    "\n",
    "player2_actions = []\n",
    "with urllib.request.urlopen(dataset_url) as f:\n",
    "    for line in f:\n",
    "        move = line.decode('utf-8').strip().upper()\n",
    "        if move:\n",
    "            player2_actions.append(Action[move])\n",
    "\n",
    "stats = collections.defaultdict(int)\n",
    "state = 0\n",
    "for p2_action in player2_actions:\n",
    "    p1_action = agent.decide(state)\n",
    "    reward = game.play(p1_action, p2_action)\n",
    "    if reward == 1:\n",
    "        stats['wins'] += 1\n",
    "    elif reward == -1:\n",
    "        stats['loses'] += 1\n",
    "    else:\n",
    "        stats['ties'] += 1\n",
    "\n",
    "print(dict(stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo visto ha aprendido un poco del sesgo que ten铆a el contrincante. 隆Bien por nuestro agente! Ya est谩 un poco m谩s cerca de dominar el mundo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2. Nuevo agente para competici贸n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer las cosas un poco m谩s interesantes, vamos a hacer una peque帽a competici贸n. Se trata de desarrollar un agente como el que acabamos de hacer, pero donde los contrincantes ser谩n el resto de agentes de los grupos. Se permite cualquier implementaci贸n, siempre y cuando:\n",
    "\n",
    "- Herede de la clase `Agent` suministrada.\n",
    "- El m茅todo `__init__` no admita ning煤n par谩metro adicional.\n",
    "\n",
    "En la siguiente celda se ofrece el esqueleto del agente que competir谩. Como se puede ver:\n",
    "\n",
    "- Tiene un nombre\n",
    "- A la hora de decidir la acci贸n recibir谩 un estado, que siempre ser谩 0 y devolver谩 la opci贸n a realizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class Agent(metaclass=abc.ABCMeta):\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def __init__(self, name: str):\n",
    "        \"\"\"Inicializa el objeto.\n",
    "        \n",
    "        :param name: El nombre del agente.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        # START\n",
    "        # END\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def decide(self, state:int) -> Action:\n",
    "        \"\"\"Decide la acci贸n a llevar a cabo dado el estado actual.\n",
    "        \n",
    "        :param state: El estado en el que se encuentra el agente.\n",
    "        :returns: La acci贸n a llevar a cabo.\n",
    "        \"\"\"\n",
    "        # START\n",
    "        # END\n",
    "    \n",
    "    def update(self, transition: Transition):\n",
    "        \"\"\"Actualiza (si es necesario) el estado interno del agente.\n",
    "        \n",
    "        :param transition: La informaci贸n de la transici贸n efectuada.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, un par de posibles implementaciones (que no usan nada de $q$-learning, y que no se recomiendan) son las siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Botnifacio(Agent):\n",
    "    def __init__(self):\n",
    "        super().__init__(name='Botnifacio')\n",
    "    \n",
    "    def decide(self, state:int) -> Action:\n",
    "        return np.random.choice(list(Action))\n",
    "\n",
    "\n",
    "class Gustabot(Agent):\n",
    "    def __init__(self):\n",
    "        super().__init__(name='Gustabot')\n",
    "        self.weights = np.random.random(5)\n",
    "        self.weights /= sum(self.weights)\n",
    "    \n",
    "    def decide(self, state:int) -> Action:\n",
    "        return np.random.choice(list(Action), p=self.weights)\n",
    "    \n",
    "    def update(self, transition: Transition):\n",
    "        self.weights = np.random.random(5)\n",
    "        self.weights /= sum(self.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El agente se deber谩 desarrollar en la siguiente celda, la cual incluir谩 todo lo necesario para instanciarlo y que funcione. Se recomienda la competici贸n entre grupos antes de la entrega final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementar aqu铆 al agente\n",
    "\n",
    "# Nosotros hemos decidido realizar un agente Q-learning con ciertos pesos ponderados seg煤n las gu铆as comunes de\n",
    "# piedra-papel-tijeras, de forma que se desv铆e un poco del comportamiento de mantener la jugada ganadora de la \n",
    "# 煤ltima ronda y cambiar a la forma ganadora de la 煤ltima ronda cuando se pierde; lo que introduce un comportamiento\n",
    "# ligeramente no determinista.\n",
    "import random\n",
    "import collections\n",
    "\n",
    "class Agente(Agent):\n",
    "    def __init__(self, name: str):\n",
    "        \"\"\"Inicializa este objeto.\n",
    "        \n",
    "        :param name: El nombre del agente, para identificarle.\n",
    "        :param q_table: Una tabla q de valores. Es opcional.\n",
    "        \"\"\"\n",
    "        # START\n",
    "        super().__init__(name)\n",
    "\n",
    "        espacioEstados = [0]\n",
    "        self.q_table = {}\n",
    "        for estado in espacioEstados:\n",
    "            self.q_table[estado] = {}\n",
    "            for acci贸n in Action:    \n",
    "                self.q_table[estado][acci贸n] = 0\n",
    "\n",
    "        self.current_state = None # Estado inicial\n",
    "        self. = 1.0\n",
    "        self.筐 = 0.0005\n",
    "        self.=0.1\n",
    "        self.=0.95\n",
    "\n",
    "        self.t = [None, None, None]\n",
    "        self.count = 0\n",
    "        \n",
    "        self.q_tableSesgo = {}\n",
    "        for estado in espacioEstados:\n",
    "            self.q_tableSesgo[estado] = {}\n",
    "            for acci贸n in Action:    \n",
    "                self.q_tableSesgo[estado][acci贸n] = 0.0\n",
    "                \n",
    "        # END\n",
    "\n",
    "    def decide(self, state:int) -> Action:\n",
    "        \"\"\"Decide la acci贸n a ejecutar.\n",
    "        \n",
    "        :param state: El estado en el que nos encontramos.\n",
    "        :param : Un valor entre 0 y 1 que representa, seg煤n la estrategia\n",
    "            蔚-greedy, la probabilidad de que la acci贸n sea elegida de manera\n",
    "            aleatoria de entre todas las opciones posibles. Es opcional, y si\n",
    "            no se especifica su valor es 0 (sin probabilidades de que se elija\n",
    "            una acci贸n aleatoria).\n",
    "        :param returns: La acci贸n a ejecutar.\n",
    "        \"\"\"\n",
    "        # START\n",
    "        self.current_state = state\n",
    "        #print(self.)\n",
    "        if random.random() < self.:\n",
    "            # Selecci贸n aleatoria\n",
    "            return np.random.choice(list(Action))\n",
    "        else:\n",
    "            # Selecci贸n voraz\n",
    "            accion, valor = self.auxBusquedaMax(state)\n",
    "            return accion\n",
    "        \n",
    "    def auxBusquedaMax(self, state:int):\n",
    "        valorMasAlto = -np.inf\n",
    "        accionVorazMejor = None\n",
    "        state = 0\n",
    "        \n",
    "        for acci贸n in Action:\n",
    "                self.q_tableSesgo[state][acci贸n] = 0.0\n",
    "                \n",
    "        ganeUltima = 0\n",
    "        if self.t[self.count] != None:\n",
    "            if (self.t[self.count].reward == -1): # si perdimos, sabemos que el ganador tratar谩 de repetir, -asi que hacemos m谩s importantes las config que lo vencen\n",
    "                MOVES_AND_REWARDS[(p1_action, p2_action)]\n",
    "                accionGano1 = None\n",
    "                accionGano2 = None\n",
    "\n",
    "                cuentaGane = 0\n",
    "                for acci贸n in Action:\n",
    "                    if MOVES_AND_REWARDS[(self.t[self.count].action, acci贸n)] == 1:\n",
    "                        if cuentaGane == 0:\n",
    "                            accionGano1 = acci贸n\n",
    "                            cuentaGane += 1\n",
    "                        else:\n",
    "                            accionGano2 = acci贸n\n",
    "            \n",
    "                if MOVES_AND_REWARDS[(accionGano1, accionGano2)] == 1:\n",
    "                    self.q_tableSesgo[state][accionGano2] += 0.2\n",
    "                    self.q_tableSesgo[state][accionGano1] += 0.09\n",
    "                else:\n",
    "                    self.q_tableSesgo[state][accionGano2] += 0.09\n",
    "                    self.q_tableSesgo[state][accionGano1] += 0.2\n",
    "            elif (self.t[self.count].reward == 1): # si ganamos, la gente tiende a quedarse con ese valor, pero nosotros no lo haremos\n",
    "                self.q_tableSesgo[state][self.t[self.count].action] -= 0.26\n",
    "            else: # en situaci贸n de empate\n",
    "                self.q_tableSesgo[state][self.t[self.count].action] += 0.06\n",
    "        \n",
    "        if self.t[self.count] != None and self.t[self.count-1] != None:\n",
    "            if (self.t[self.count].action == self.t[self.count-1].action): # si las dos 煤ltimas son iguales, es muy probable que se cambie a cualquier otra, pero eso lo sabemos as铆 que nos quedamos ac谩\n",
    "                self.q_tableSesgo[state][self.t[self.count].action] += 0.26\n",
    "            if self.t[self.count-2] != None:\n",
    "                if ((self.t[self.count].action == self.t[self.count-1].action) and (self.t[self.count].action == self.t[self.count-2].action) and (self.t[self.count].reward == -1) and (self.t[self.count-1].reward == -1) and (self.t[self.count-2].reward == -1)): # cambia\n",
    "                    self.q_tableSesgo[state][self.t[self.count].action] -= 0.36\n",
    "        \n",
    "        for acci贸n in self.q_table[state]:\n",
    "            #print(f'el self m谩s el sesgo: {self.q_table[state][acci贸n]} + {self.q_tableSesgo[state][acci贸n]} = {self.q_table[state][acci贸n] + self.q_tableSesgo[state][acci贸n]}')\n",
    "            if ((self.q_table[state][acci贸n] + self.q_tableSesgo[state][acci贸n]) > valorMasAlto):\n",
    "                valorMasAlto = self.q_table[state][acci贸n] + self.q_tableSesgo[state][acci贸n]\n",
    "                accionVorazMejor = acci贸n\n",
    "                \n",
    "        #for acci贸n in self.q_table[state]:\n",
    "        #    if ((self.q_table[state][acci贸n]) > valorMasAlto):\n",
    "        #        valorMasAlto = self.q_table[state][acci贸n]\n",
    "        #        accionVorazMejor = acci贸n\n",
    "\n",
    "        return accionVorazMejor, valorMasAlto\n",
    "\n",
    "        # END\n",
    "\n",
    "    def update(self, transition: Transition):\n",
    "        \"\"\"Actualiza el estado interno de acuerdo a la experiencia vivida.\n",
    "        \n",
    "        :param transition: Toda la informaci贸n correspondiente a la transici贸n\n",
    "            de la que queremos aprender.\n",
    "        :param : El factor de aprendizaje del cambio en el valor q. Por\n",
    "            defecto es 0.1\n",
    "        :param : La influencia de la recompensa a largo plazo en el valor q a\n",
    "            actualizar. Va de 0 (sin influencia) a 1 (misma influencia que el\n",
    "            valor actual). Por defecto es 0.95.\n",
    "        \"\"\"\n",
    "        # START\n",
    "        \n",
    "        accion, maxaQstmas1ya = self.auxBusquedaMax(transition.next_state)\n",
    "        \n",
    "        \n",
    "        self.q_table[transition.prev_state][transition.action] = self.q_table[transition.prev_state][transition.action] + self. * (transition.reward + self. * maxaQstmas1ya - self.q_table[transition.prev_state][transition.action]) \n",
    "        self.state = transition.next_state\n",
    "        self.reward = transition.reward\n",
    "        \n",
    "        self. -= 筐 if  > 0 else 0\n",
    "        #self. -= 筐 if  > 0 else 0\n",
    "        #self. -= self.筐 if self. > 0 else 0\n",
    "        \n",
    "        self.t[self.count] = transition\n",
    "        self.count = (self.count +1)%3\n",
    "        # END\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Representaci贸n textual de la tabla Q del agente.\n",
    "        \n",
    "        :returns: Una cadena indicando la estructura interna de la tabla Q.\n",
    "        \"\"\"\n",
    "        # START\n",
    "        miString = self.name+\" (\"\n",
    "        for state in self.q_table:\n",
    "            miString = miString+\"\\nESTADO \"+str(state)+\"[\"\n",
    "            for action in self.q_table[state]:\n",
    "                miString = miString+\"   \"+str(action)+\"->:\"+str(self.q_table[state][action])\n",
    "            miString = miString+\"]\"\n",
    "        miString = miString+\"\\n)\"\n",
    "        return miString\n",
    "        # END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wins': 1115, 'loses': 679, 'ties': 206}\n"
     ]
    }
   ],
   "source": [
    "# ESTE DE AC ABAJO ES DE NUESTRO TEST\n",
    "\n",
    "dataset_url = 'https://blazaid.github.io/Aprendizaje-profundo/Datasets/rock-paper-scissors-lizard-spock.trn'\n",
    "\n",
    "player2_actions = []\n",
    "with urllib.request.urlopen(dataset_url) as f:\n",
    "    for line in f:\n",
    "        move = line.decode('utf-8').strip().upper()\n",
    "        if move:\n",
    "            player2_actions.append(Action[move])\n",
    "\n",
    " = 1\n",
    "筐 =  / len(player2_actions)\n",
    "\n",
    "game = Game()\n",
    "agent = Agente(name='Agentillo')\n",
    "\n",
    "state = 0  # El entorno (juego) no tiene estado, as铆 que siempre ser谩 el mismo\n",
    "for p2_action in player2_actions:\n",
    "    p1_action = agent.decide(state)\n",
    "    reward = game.play(p1_action, p2_action)\n",
    "\n",
    "    # Actualizamos el agente\n",
    "    agent.update(Transition(\n",
    "        prev_state=state,\n",
    "        next_state=state,\n",
    "        action=p1_action,\n",
    "        reward=reward\n",
    "    ))\n",
    "\n",
    "    # Actualizamos \n",
    "    # -= 筐 if  > 0 else 0\n",
    "\n",
    "dataset_url = 'https://blazaid.github.io/Aprendizaje-profundo/Datasets/rock-paper-scissors-lizard-spock.tst'\n",
    "\n",
    "player2_actions = []\n",
    "with urllib.request.urlopen(dataset_url) as f:\n",
    "    for line in f:\n",
    "        move = line.decode('utf-8').strip().upper()\n",
    "        if move:\n",
    "            player2_actions.append(Action[move])\n",
    "\n",
    "stats = collections.defaultdict(int)\n",
    "state = 0\n",
    "for p2_action in player2_actions:\n",
    "    p1_action = agent.decide(state)\n",
    "    reward = game.play(p1_action, p2_action)\n",
    "    if reward == 1:\n",
    "        stats['wins'] += 1\n",
    "    elif reward == -1:\n",
    "        stats['loses'] += 1\n",
    "    else:\n",
    "        stats['ties'] += 1\n",
    "\n",
    "print(dict(stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La competici贸n ser谩 una liga entre los agentes de cada grupo a 100 partidas.\n",
    "\n",
    "Antes de dichas 100 partidas, se har谩 un previo de 10000 partidas con el fin de que cada uno de los agentes \"aprenda\" a competir contra el contrario. S贸lo en esta fase se invocar谩 el m茅todo `update`.\n",
    "\n",
    "La competici贸n se realizar谩 como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algigantix (\n",
      "ESTADO 0[   Action.ROCK->:2.8130993301867893   Action.PAPER->:2.5693836850365197   Action.SCISSORS->:2.74173809948628   Action.LIZARD->:2.378499657726694   Action.SPOCK->:2.734158734046112]\n",
      "): -2, Botnifacio: 2\n",
      "Algigantix (\n",
      "ESTADO 0[   Action.ROCK->:3.0340537817737823   Action.PAPER->:1.5207878486047812   Action.SCISSORS->:1.6485676516637606   Action.LIZARD->:1.5471872077200537   Action.SPOCK->:1.5660009957392542]\n",
      "): 5, Gustabot: -5\n",
      "Botnifacio: 0, Gustabot: 0\n",
      "LEADERBOARD\n",
      "3         \tAlgigantix (\n",
      "ESTADO 0[   Action.ROCK->:3.0340537817737823   Action.PAPER->:1.5207878486047812   Action.SCISSORS->:1.6485676516637606   Action.LIZARD->:1.5471872077200537   Action.SPOCK->:1.5660009957392542]\n",
      ")\n",
      "2         \tBotnifacio\n",
      "-5        \tGustabot\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "FRIENDLY_SETS = 10000\n",
    "COMPETITION_SETS = 10\n",
    "\n",
    "nombreNuestro = \"Algigantix\"\n",
    "\n",
    "competitors = [\n",
    "    Gustabot(),\n",
    "    Botnifacio(),\n",
    "    Agente(nombreNuestro),\n",
    "]\n",
    "\n",
    "leaderboard = {c: 0 for c in sorted(competitors, key=lambda x: x.__str__())}\n",
    "game = Game()\n",
    "for p1, p2 in itertools.combinations(leaderboard.keys(), 2):\n",
    "    # Amistoso\n",
    "    s = 0\n",
    "    for i in range(FRIENDLY_SETS):\n",
    "        a1 = p1.decide(s)\n",
    "        a2 = p2.decide(s)\n",
    "        reward = game.play(a1, a2)\n",
    "        p1.update(Transition(prev_state=s, next_state=s, action=a1, reward=reward))\n",
    "        p2.update(Transition(prev_state=s, next_state=s, action=a2, reward=-reward))\n",
    "    \n",
    "    # Competici贸n\n",
    "    s = 0\n",
    "    r1 = r2 = 0\n",
    "    for i in range(COMPETITION_SETS):\n",
    "        a1 = p1.decide(s)\n",
    "        a2 = p2.decide(s)\n",
    "        reward = game.play(a1, a2)\n",
    "        r1 += reward\n",
    "        r2 -= reward\n",
    "\n",
    "    # Actualizaci贸n de marcadores globales\n",
    "    leaderboard[p1] += r1\n",
    "    leaderboard[p2] += r2\n",
    "    \n",
    "    print(f'{p1}: {r1}, {p2}: {r2}')\n",
    "    \n",
    "\n",
    "print('LEADERBOARD')\n",
    "for c, r in sorted(leaderboard.items(), key=lambda t: t[1], reverse=True):\n",
    "    print(f'{r:<10}\\t{c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La calificaci贸n de cada grupo ir谩 en funci贸n de la puntuaci贸n final que haya conseguido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumiendo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos desarrollado un agente para un juego diferente que aprender a tomar decisiones 贸ptimas en cada situaci贸n, a partir de la retroalimentaci贸n proporcionada por el entorno del juego. Hemos observado c贸mo el agente ha ido mejorando gradualmente su desempe帽o en el juego, a medida que ha acumulado experiencia y ha ajustado sus valores de Q.\n",
    "\n",
    "Adem谩s, hemos podido comprobar c贸mo el agente es capaz de adaptarse a diferentes estrategias de juego de su oponente, lo que demuestra su capacidad para generalizar y tomar decisiones en situaciones nuevas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
